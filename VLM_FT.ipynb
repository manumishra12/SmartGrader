{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb909b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 317, done.\u001b[K\n",
      "remote: Counting objects: 100% (317/317), done.\u001b[K\n",
      "remote: Compressing objects: 100% (263/263), done.\u001b[K\n",
      "remote: Total 317 (delta 81), reused 184 (delta 41), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (317/317), 8.97 MiB | 7.18 MiB/s, done.\n",
      "Resolving deltas: 100% (81/81), done.\n",
      "/workspace/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///workspace/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<=4.46.1,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (4.45.2)\n",
      "Requirement already satisfied: datasets<=3.1.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.21.0)\n",
      "Requirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.34.2)\n",
      "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.12.0)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.9.6)\n",
      "Requirement already satisfied: gradio<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (4.44.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.12.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.8.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (4.24.4)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.32.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.5.3)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.115.3)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.1.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.8.2)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.7.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.24.4)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (13.1.0)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.5.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.9.1)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.0.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (5.9.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (0.26.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.9.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (4.6.2.post1)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.27.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (6.4.5)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.1.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.10.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (10.4.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.0.12)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.7.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (4.9.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.2.3)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (12.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.9.1.dev0) (0.41.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (2.14.6)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.6.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.1.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.1.dev0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.1.dev0) (0.20.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.8.13)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.1.dev0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.1.dev0) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.1.dev0) (2.5.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.9.1.dev0) (1.3.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->llamafactory==0.9.1.dev0) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.0.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.3.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (13.7.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (1.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.1.dev0-0.editable-py3-none-any.whl size=23001 sha256=7edaea1d8fec6cb6167c75a8b8fd4ea369e0e10ebc70b61579a444e9b611770f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-m4j2ddxu/wheels/6b/fb/2e/068b37b77399a386b5777cf8b247cb07a69197b4baef3732a2\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.9.1.dev0\n",
      "    Uninstalling llamafactory-0.9.1.dev0:\n",
      "      Successfully uninstalled llamafactory-0.9.1.dev0\n",
      "Successfully installed llamafactory-0.9.1.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "!pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a996d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_TOKEN=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a4a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://1bbf2f5dbcde65f789.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "[INFO|2024-11-11 22:28:18] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:29764\n",
      "W1111 22:28:19.966000 4449 torch/distributed/run.py:793] \n",
      "W1111 22:28:19.966000 4449 torch/distributed/run.py:793] *****************************************\n",
      "W1111 22:28:19.966000 4449 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1111 22:28:19.966000 4449 torch/distributed/run.py:793] *****************************************\n",
      "[INFO|2024-11-11 22:28:23] llamafactory.hparams.parser:355 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[WARNING|2024-11-11 22:28:23] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2024-11-11 22:28:23] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "[rank1]:     response.raise_for_status()\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank1]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank1]: requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/google/paligemma-3b-pt-896/resolve/main/config.json\n",
      "\n",
      "[rank1]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "[rank1]:     resolved_file = hf_hub_download(\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n",
      "[rank1]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n",
      "[rank1]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1484, in _raise_on_head_call_error\n",
      "[rank1]:     raise head_call_error\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1376, in _get_metadata_or_catch_error\n",
      "[rank1]:     metadata = get_hf_file_metadata(\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n",
      "[rank1]:     r = _request_wrapper(\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 277, in _request_wrapper\n",
      "[rank1]:     response = _request_wrapper(\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 301, in _request_wrapper\n",
      "[rank1]:     hf_raise_for_status(response)\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "[rank1]:     raise _format(GatedRepoError, message, response) from e\n",
      "[rank1]: huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-67328508-70cab1515686f7f14e8c73eb;09060880-137a-4cef-b308-0b0c4088ba6c)\n",
      "\n",
      "[rank1]: Cannot access gated repo for url https://huggingface.co/google/paligemma-3b-pt-896/resolve/main/config.json.\n",
      "[rank1]: Access to model google/paligemma-3b-pt-896 is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "\n",
      "[rank1]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank1]:     launch()\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank1]:     run_exp()\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 44, in run_sft\n",
      "[rank1]:     tokenizer_module = load_tokenizer(model_args)\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/model/loader.py\", line 69, in load_tokenizer\n",
      "[rank1]:     config = load_config(model_args)\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/model/loader.py\", line 119, in load_config\n",
      "[rank1]:     return AutoConfig.from_pretrained(model_args.model_name_or_path, **init_kwargs)\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n",
      "[rank1]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 570, in get_config_dict\n",
      "[rank1]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 629, in _get_config_dict\n",
      "[rank1]:     resolved_config_file = cached_file(\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "[rank1]:     raise EnvironmentError(\n",
      "[rank1]: OSError: You are trying to access a gated repo.\n",
      "[rank1]: Make sure to have access to it at https://huggingface.co/google/paligemma-3b-pt-896.\n",
      "[rank1]: 401 Client Error. (Request ID: Root=1-67328508-70cab1515686f7f14e8c73eb;09060880-137a-4cef-b308-0b0c4088ba6c)\n",
      "\n",
      "[rank1]: Cannot access gated repo for url https://huggingface.co/google/paligemma-3b-pt-896/resolve/main/config.json.\n",
      "[rank1]: Access to model google/paligemma-3b-pt-896 is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "[rank0]:     response.raise_for_status()\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank0]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank0]: requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/google/paligemma-3b-pt-896/resolve/main/config.json\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "[rank0]:     resolved_file = hf_hub_download(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n",
      "[rank0]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n",
      "[rank0]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1484, in _raise_on_head_call_error\n",
      "[rank0]:     raise head_call_error\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1376, in _get_metadata_or_catch_error\n",
      "[rank0]:     metadata = get_hf_file_metadata(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n",
      "[rank0]:     r = _request_wrapper(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 277, in _request_wrapper\n",
      "[rank0]:     response = _request_wrapper(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 301, in _request_wrapper\n",
      "[rank0]:     hf_raise_for_status(response)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "[rank0]:     raise _format(GatedRepoError, message, response) from e\n",
      "[rank0]: huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-67328508-4b21c41e702c43251314708b;e36108a6-aaf0-4639-b415-60d5995f5fc0)\n",
      "\n",
      "[rank0]: Cannot access gated repo for url https://huggingface.co/google/paligemma-3b-pt-896/resolve/main/config.json.\n",
      "[rank0]: Access to model google/paligemma-3b-pt-896 is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 44, in run_sft\n",
      "[rank0]:     tokenizer_module = load_tokenizer(model_args)\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/model/loader.py\", line 69, in load_tokenizer\n",
      "[rank0]:     config = load_config(model_args)\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/model/loader.py\", line 119, in load_config\n",
      "[rank0]:     return AutoConfig.from_pretrained(model_args.model_name_or_path, **init_kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n",
      "[rank0]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 570, in get_config_dict\n",
      "[rank0]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 629, in _get_config_dict\n",
      "[rank0]:     resolved_config_file = cached_file(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "[rank0]:     raise EnvironmentError(\n",
      "[rank0]: OSError: You are trying to access a gated repo.\n",
      "[rank0]: Make sure to have access to it at https://huggingface.co/google/paligemma-3b-pt-896.\n",
      "[rank0]: 401 Client Error. (Request ID: Root=1-67328508-4b21c41e702c43251314708b;e36108a6-aaf0-4639-b415-60d5995f5fc0)\n",
      "\n",
      "[rank0]: Cannot access gated repo for url https://huggingface.co/google/paligemma-3b-pt-896/resolve/main/config.json.\n",
      "[rank0]: Access to model google/paligemma-3b-pt-896 is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "[rank0]:[W1111 22:28:25.311416847 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "W1111 22:28:25.092000 4449 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 4520 closing signal SIGTERM\n",
      "E1111 22:28:25.157000 4449 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 4521) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/workspace/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-11-11_22:28:25\n",
      "  host      : 25abc780b243\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 4521)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "[INFO|2024-11-11 22:31:44] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:27979\n",
      "W1111 22:31:45.884000 4676 torch/distributed/run.py:793] \n",
      "W1111 22:31:45.884000 4676 torch/distributed/run.py:793] *****************************************\n",
      "W1111 22:31:45.884000 4676 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1111 22:31:45.884000 4676 torch/distributed/run.py:793] *****************************************\n",
      "[WARNING|2024-11-11 22:31:49] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2024-11-11 22:31:49] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2024-11-11 22:31:49] llamafactory.hparams.parser:355 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "config.json: 100%|█████████████████████████████| 967/967 [00:00<00:00, 2.49MB/s]\n",
      "[INFO|configuration_utils.py:675] 2024-11-11 22:31:51,125 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "configuration_phi3.py: 100%|███████████████| 11.2k/11.2k [00:00<00:00, 20.9MB/s]\n",
      "[INFO|configuration_utils.py:675] 2024-11-11 22:31:52,675 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-11-11 22:31:52,679 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████| 3.44k/3.44k [00:00<00:00, 10.5MB/s]\n",
      "tokenizer.model: 100%|███████████████████████| 500k/500k [00:00<00:00, 5.77MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.94M/1.94M [00:01<00:00, 1.81MB/s]\n",
      "added_tokens.json: 100%|████████████████████████| 306/306 [00:00<00:00, 958kB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 599/599 [00:00<00:00, 1.85MB/s]\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:31:56,844 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:31:56,844 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:31:56,845 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:31:56,845 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:31:56,845 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2470] 2024-11-11 22:31:57,026 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:675] 2024-11-11 22:31:58,395 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "[INFO|configuration_utils.py:675] 2024-11-11 22:31:58,886 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-11-11 22:31:58,889 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[rank1]:[W1111 22:31:58.276100935 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:31:59,144 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:31:59,144 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:31:59,144 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:31:59,145 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:31:59,145 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2470] 2024-11-11 22:31:59,288 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2024-11-11 22:31:59] llamafactory.data.template:157 >> Replace eos token: <|end|>\n",
      "[WARNING|2024-11-11 22:31:59] llamafactory.data.template:162 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[INFO|2024-11-11 22:31:59] llamafactory.data.loader:157 >> Loading dataset os_chat.json...\n",
      "Generating train split: 270 examples [00:00, 2501.34 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 270/270 [00:00<00:00, 717.98\n",
      "[rank0]:[W1111 22:32:01.184162411 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Running tokenizer on dataset (num_proc=16):   0%| | 0/270 [00:00<?, ? examples/s\n",
      "[rank0]: multiprocess.pool.RemoteTraceback: \n",
      "[rank0]: \"\"\"\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 125, in worker\n",
      "[rank0]:     result = (True, func(*args, **kwds))\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n",
      "[rank0]:     for i, result in enumerate(func(**kwargs)):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3558, in _map_single\n",
      "[rank0]:     batch = apply_function_on_filtered_inputs(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3427, in apply_function_on_filtered_inputs\n",
      "[rank0]:     processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/processors/supervised.py\", line 107, in preprocess_supervised_dataset\n",
      "[rank0]:     input_ids, labels = _encode_supervised_example(\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/processors/supervised.py\", line 48, in _encode_supervised_example\n",
      "[rank0]:     messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/mm_plugin.py\", line 202, in process_messages\n",
      "[rank0]:     self._validate_input(images, videos)\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/mm_plugin.py\", line 68, in _validate_input\n",
      "[rank0]:     raise ValueError(\"This model does not support image input.\")\n",
      "[rank0]: ValueError: This model does not support image input.\n",
      "[rank0]: \"\"\"\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "[rank0]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/loader.py\", line 265, in get_dataset\n",
      "[rank0]:     dataset = _get_preprocessed_dataset(\n",
      "[rank0]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/loader.py\", line 204, in _get_preprocessed_dataset\n",
      "[rank0]:     dataset = dataset.map(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 602, in wrapper\n",
      "[rank0]:     out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 567, in wrapper\n",
      "[rank0]:     out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3259, in map\n",
      "[rank0]:     for rank, done, content in iflatmap_unordered(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 718, in iflatmap_unordered\n",
      "[rank0]:     [async_result.get(timeout=0.05) for async_result in async_results]\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 718, in <listcomp>\n",
      "[rank0]:     [async_result.get(timeout=0.05) for async_result in async_results]\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 774, in get\n",
      "[rank0]:     raise self._value\n",
      "[rank0]: ValueError: This model does not support image input.\n",
      "Running tokenizer on dataset (num_proc=16):   0%| | 0/270 [00:00<?, ? examples/s[rank0]:[W1111 22:32:05.305221521 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "Running tokenizer on dataset (num_proc=16):   0%| | 0/270 [00:00<?, ? examples/s\n",
      "[rank1]: multiprocess.pool.RemoteTraceback: \n",
      "[rank1]: \"\"\"\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 125, in worker\n",
      "[rank1]:     result = (True, func(*args, **kwds))\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n",
      "[rank1]:     for i, result in enumerate(func(**kwargs)):\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3558, in _map_single\n",
      "[rank1]:     batch = apply_function_on_filtered_inputs(\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3427, in apply_function_on_filtered_inputs\n",
      "[rank1]:     processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/processors/supervised.py\", line 107, in preprocess_supervised_dataset\n",
      "[rank1]:     input_ids, labels = _encode_supervised_example(\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/processors/supervised.py\", line 48, in _encode_supervised_example\n",
      "[rank1]:     messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/mm_plugin.py\", line 202, in process_messages\n",
      "[rank1]:     self._validate_input(images, videos)\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/mm_plugin.py\", line 68, in _validate_input\n",
      "[rank1]:     raise ValueError(\"This model does not support image input.\")\n",
      "[rank1]: ValueError: This model does not support image input.\n",
      "[rank1]: \"\"\"\n",
      "\n",
      "[rank1]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank1]:     launch()\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank1]:     run_exp()\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "[rank1]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/loader.py\", line 265, in get_dataset\n",
      "[rank1]:     dataset = _get_preprocessed_dataset(\n",
      "[rank1]:   File \"/workspace/LLaMA-Factory/src/llamafactory/data/loader.py\", line 204, in _get_preprocessed_dataset\n",
      "[rank1]:     dataset = dataset.map(\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 602, in wrapper\n",
      "[rank1]:     out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 567, in wrapper\n",
      "[rank1]:     out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3259, in map\n",
      "[rank1]:     for rank, done, content in iflatmap_unordered(\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 718, in iflatmap_unordered\n",
      "[rank1]:     [async_result.get(timeout=0.05) for async_result in async_results]\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 718, in <listcomp>\n",
      "[rank1]:     [async_result.get(timeout=0.05) for async_result in async_results]\n",
      "[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 774, in get\n",
      "[rank1]:     raise self._value\n",
      "[rank1]: ValueError: This model does not support image input.\n",
      "W1111 22:32:07.362000 4676 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 4748 closing signal SIGTERM\n",
      "E1111 22:32:07.577000 4676 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 4747) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/workspace/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-11-11_22:32:07\n",
      "  host      : 25abc780b243\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 4747)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "[INFO|2024-11-11 22:35:42] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:22770\n",
      "W1111 22:35:43.869000 5151 torch/distributed/run.py:793] \n",
      "W1111 22:35:43.869000 5151 torch/distributed/run.py:793] *****************************************\n",
      "W1111 22:35:43.869000 5151 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1111 22:35:43.869000 5151 torch/distributed/run.py:793] *****************************************\n",
      "[INFO|2024-11-11 22:35:47] llamafactory.hparams.parser:355 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[WARNING|2024-11-11 22:35:47] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2024-11-11 22:35:47] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|configuration_utils.py:675] 2024-11-11 22:35:48,678 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-11-11 22:35:48,706 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:49,099 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:49,099 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:49,100 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:49,100 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:49,100 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:49,100 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2470] 2024-11-11 22:35:49,506 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|image_processing_base.py:375] 2024-11-11 22:35:51,030 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:375] 2024-11-11 22:35:51,285 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:429] 2024-11-11 22:35:51,286 >> Image processor Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:51,519 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:51,519 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:51,519 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:51,520 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:51,520 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-11-11 22:35:51,520 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2470] 2024-11-11 22:35:51,857 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|processing_utils.py:744] 2024-11-11 22:35:52,940 >> Processor Qwen2VLProcessor:\n",
      "- image_processor: Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"Qwen2VLProcessor\"\n",
      "}\n",
      "\n",
      "[INFO|2024-11-11 22:35:52] llamafactory.data.template:157 >> Replace eos token: <|im_end|>\n",
      "[INFO|2024-11-11 22:35:52] llamafactory.data.loader:157 >> Loading dataset os_chat.json...\n",
      "[rank1]:[W1111 22:35:53.625938098 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Converting format of dataset (num_proc=16): 100%|█| 270/270 [00:00<00:00, 820.28\n",
      "[rank0]:[W1111 22:35:55.653589628 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 270/270 [00:03<00:00, 72.47 \n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 74785, 419, 2168, 369, 752, 30, 151645, 198, 151644, 77091, 198, 1986, 2168, 45380, 279, 28922, 315, 3162, 13617, 304, 458, 10292, 9420, 3126, 320, 12778, 38868, 35133, 739, 8, 4573, 13, 576, 13617, 525, 16645, 504, 279, 1909, 320, 872, 63306, 8357, 8, 311, 279, 5622, 320, 68215, 24099, 568, 5692, 374, 264, 11682, 16148, 315, 1817, 6193, 1447, 16, 13, 3070, 4988, 6687, 334, 510, 256, 481, 1096, 374, 279, 1909, 3562, 6193, 11, 14064, 279, 1196, 8357, 476, 3162, 7468, 429, 525, 1598, 553, 279, 1196, 13, 4220, 8357, 525, 6188, 311, 2736, 3151, 9079, 11, 1741, 438, 3409, 8692, 11, 42553, 20306, 11, 476, 3868, 382, 17, 13, 3070, 1061, 1713, 739, 6687, 334, 510, 256, 481, 1096, 6193, 23011, 3685, 279, 3766, 2025, 323, 5646, 279, 6200, 6813, 315, 279, 10350, 1849, 429, 525, 6661, 1119, 4938, 2337, 279, 10459, 1882, 13, 4220, 6813, 3410, 279, 6770, 14944, 2567, 369, 279, 1849, 311, 14476, 11, 1741, 438, 1034, 6240, 11, 4938, 23757, 11, 323, 1849, 3516, 382, 18, 13, 3070, 4826, 9420, 3126, 13903, 45737, 334, 510, 256, 481, 1096, 6193, 374, 26366, 304, 6303, 11, 18860, 1181, 12650, 304, 279, 17646, 13, 10292, 9420, 3126, 3671, 11788, 525, 3162, 13454, 429, 2138, 279, 10350, 1849, 311, 16282, 448, 11773, 7611, 13, 4220, 11788, 525, 8480, 369, 66271, 279, 1550, 11591, 11293, 504, 279, 18967, 1849, 2025, 1119, 3347, 11591, 11221, 429, 279, 11773, 646, 3535, 382, 19, 13, 3070, 3361, 52863, 13903, 45737, 334, 510, 256, 481, 1096, 374, 279, 5622, 6193, 11, 14064, 279, 14625, 5571, 14, 5097, 739, 320, 8598, 3126, 8, 9768, 304, 279, 4457, 12, 7308, 13850, 320, 3361, 8, 315, 279, 6366, 13, 576, 52863, 5707, 279, 1429, 6770, 2188, 315, 11773, 16230, 11, 2670, 2355, 6240, 11, 10459, 287, 11, 323, 3347, 11591, 1946, 47016, 7525, 382, 785, 36957, 304, 279, 2168, 13216, 279, 6396, 315, 2524, 323, 821, 1948, 1493, 13617, 13, 7323, 7468, 16282, 448, 279, 18967, 1849, 2025, 11, 892, 304, 2484, 83161, 448, 279, 10292, 9420, 3126, 3671, 11788, 13, 4220, 11788, 646, 2987, 5961, 16282, 448, 279, 11773, 476, 990, 279, 3516, 3897, 553, 279, 30686, 52863, 3671, 11788, 382, 641, 12126, 11, 419, 2168, 4933, 279, 63141, 5944, 315, 458, 10292, 9420, 3126, 1849, 11, 1380, 8357, 518, 279, 1909, 17188, 389, 1849, 7468, 11, 892, 304, 2484, 17188, 389, 3671, 11788, 311, 19032, 448, 279, 11773, 13, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>Describe this image for me?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "This image illustrates the hierarchy of software layers in an MS-DOS (Microsoft Disk Operating System) environment. The layers are organized from the top (user-facing applications) to the bottom (hardware interfaces). Here is a detailed explanation of each layer:\n",
      "\n",
      "1. **Application Program**:\n",
      "   - This is the topmost layer, representing the user applications or software programs that are run by the user. These applications are designed to perform specific tasks, such as word processing, spreadsheets, or games.\n",
      "\n",
      "2. **Resident System Program**:\n",
      "   - This layer sits below the application program and includes the core components of the operating system that are loaded into memory during the boot process. These components provide the basic functionality required for the system to operate, such as file management, memory allocation, and system services.\n",
      "\n",
      "3. **MS-DOS Device Drivers**:\n",
      "   - This layer is highlighted in blue, indicating its importance in the architecture. MS-DOS device drivers are software modules that allow the operating system to interact with hardware devices. These drivers are responsible for translating the high-level commands from the resident system program into low-level instructions that the hardware can understand.\n",
      "\n",
      "4. **ROM BIOS Device Drivers**:\n",
      "   - This is the bottom layer, representing the Basic Input/Output System (BIOS) stored in the Read-Only Memory (ROM) of the computer. The BIOS provides the most basic level of hardware interaction, including power management, booting, and low-level input/output operations.\n",
      "\n",
      "The arrows in the image indicate the flow of control and data between these layers. Application programs interact with the resident system program, which in turn interacts with the MS-DOS device drivers. These drivers can either directly interact with the hardware or use the services provided by the ROM BIOS device drivers.\n",
      "\n",
      "In summary, this image shows the layered structure of an MS-DOS system, where applications at the top rely on system programs, which in turn rely on device drivers to communicate with the hardware.<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1986, 2168, 45380, 279, 28922, 315, 3162, 13617, 304, 458, 10292, 9420, 3126, 320, 12778, 38868, 35133, 739, 8, 4573, 13, 576, 13617, 525, 16645, 504, 279, 1909, 320, 872, 63306, 8357, 8, 311, 279, 5622, 320, 68215, 24099, 568, 5692, 374, 264, 11682, 16148, 315, 1817, 6193, 1447, 16, 13, 3070, 4988, 6687, 334, 510, 256, 481, 1096, 374, 279, 1909, 3562, 6193, 11, 14064, 279, 1196, 8357, 476, 3162, 7468, 429, 525, 1598, 553, 279, 1196, 13, 4220, 8357, 525, 6188, 311, 2736, 3151, 9079, 11, 1741, 438, 3409, 8692, 11, 42553, 20306, 11, 476, 3868, 382, 17, 13, 3070, 1061, 1713, 739, 6687, 334, 510, 256, 481, 1096, 6193, 23011, 3685, 279, 3766, 2025, 323, 5646, 279, 6200, 6813, 315, 279, 10350, 1849, 429, 525, 6661, 1119, 4938, 2337, 279, 10459, 1882, 13, 4220, 6813, 3410, 279, 6770, 14944, 2567, 369, 279, 1849, 311, 14476, 11, 1741, 438, 1034, 6240, 11, 4938, 23757, 11, 323, 1849, 3516, 382, 18, 13, 3070, 4826, 9420, 3126, 13903, 45737, 334, 510, 256, 481, 1096, 6193, 374, 26366, 304, 6303, 11, 18860, 1181, 12650, 304, 279, 17646, 13, 10292, 9420, 3126, 3671, 11788, 525, 3162, 13454, 429, 2138, 279, 10350, 1849, 311, 16282, 448, 11773, 7611, 13, 4220, 11788, 525, 8480, 369, 66271, 279, 1550, 11591, 11293, 504, 279, 18967, 1849, 2025, 1119, 3347, 11591, 11221, 429, 279, 11773, 646, 3535, 382, 19, 13, 3070, 3361, 52863, 13903, 45737, 334, 510, 256, 481, 1096, 374, 279, 5622, 6193, 11, 14064, 279, 14625, 5571, 14, 5097, 739, 320, 8598, 3126, 8, 9768, 304, 279, 4457, 12, 7308, 13850, 320, 3361, 8, 315, 279, 6366, 13, 576, 52863, 5707, 279, 1429, 6770, 2188, 315, 11773, 16230, 11, 2670, 2355, 6240, 11, 10459, 287, 11, 323, 3347, 11591, 1946, 47016, 7525, 382, 785, 36957, 304, 279, 2168, 13216, 279, 6396, 315, 2524, 323, 821, 1948, 1493, 13617, 13, 7323, 7468, 16282, 448, 279, 18967, 1849, 2025, 11, 892, 304, 2484, 83161, 448, 279, 10292, 9420, 3126, 3671, 11788, 13, 4220, 11788, 646, 2987, 5961, 16282, 448, 279, 11773, 476, 990, 279, 3516, 3897, 553, 279, 30686, 52863, 3671, 11788, 382, 641, 12126, 11, 419, 2168, 4933, 279, 63141, 5944, 315, 458, 10292, 9420, 3126, 1849, 11, 1380, 8357, 518, 279, 1909, 17188, 389, 1849, 7468, 11, 892, 304, 2484, 17188, 389, 3671, 11788, 311, 19032, 448, 279, 11773, 13, 151645]\n",
      "labels:\n",
      "This image illustrates the hierarchy of software layers in an MS-DOS (Microsoft Disk Operating System) environment. The layers are organized from the top (user-facing applications) to the bottom (hardware interfaces). Here is a detailed explanation of each layer:\n",
      "\n",
      "1. **Application Program**:\n",
      "   - This is the topmost layer, representing the user applications or software programs that are run by the user. These applications are designed to perform specific tasks, such as word processing, spreadsheets, or games.\n",
      "\n",
      "2. **Resident System Program**:\n",
      "   - This layer sits below the application program and includes the core components of the operating system that are loaded into memory during the boot process. These components provide the basic functionality required for the system to operate, such as file management, memory allocation, and system services.\n",
      "\n",
      "3. **MS-DOS Device Drivers**:\n",
      "   - This layer is highlighted in blue, indicating its importance in the architecture. MS-DOS device drivers are software modules that allow the operating system to interact with hardware devices. These drivers are responsible for translating the high-level commands from the resident system program into low-level instructions that the hardware can understand.\n",
      "\n",
      "4. **ROM BIOS Device Drivers**:\n",
      "   - This is the bottom layer, representing the Basic Input/Output System (BIOS) stored in the Read-Only Memory (ROM) of the computer. The BIOS provides the most basic level of hardware interaction, including power management, booting, and low-level input/output operations.\n",
      "\n",
      "The arrows in the image indicate the flow of control and data between these layers. Application programs interact with the resident system program, which in turn interacts with the MS-DOS device drivers. These drivers can either directly interact with the hardware or use the services provided by the ROM BIOS device drivers.\n",
      "\n",
      "In summary, this image shows the layered structure of an MS-DOS system, where applications at the top rely on system programs, which in turn rely on device drivers to communicate with the hardware.<|im_end|>\n",
      "[INFO|configuration_utils.py:675] 2024-11-11 22:36:01,628 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-11-11 22:36:01,632 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3732] 2024-11-11 22:36:01,905 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1622] 2024-11-11 22:36:01,956 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1099] 2024-11-11 22:36:01,962 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[WARNING|logging.py:168] 2024-11-11 22:36:02,097 >> `Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [01:21<00:00, 40.99s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [01:22<00:00, 41.03s/it]\n",
      "[INFO|modeling_utils.py:4574] 2024-11-11 22:37:24,373 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4582] 2024-11-11 22:37:24,373 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1054] 2024-11-11 22:37:24,610 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/generation_config.json\n",
      "[INFO|configuration_utils.py:1099] 2024-11-11 22:37:24,610 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.01,\n",
      "  \"top_k\": 1,\n",
      "  \"top_p\": 0.001\n",
      "}\n",
      "\n",
      "[INFO|2024-11-11 22:37:24] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2024-11-11 22:37:24] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2024-11-11 22:37:24] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
      "[INFO|2024-11-11 22:37:24] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2024-11-11 22:37:24] llamafactory.model.model_utils.misc:157 >> Found linear modules: k_proj,down_proj,up_proj,q_proj,v_proj,o_proj,gate_proj\n",
      "[INFO|2024-11-11 22:37:25] llamafactory.model.loader:157 >> trainable params: 18,464,768 || all params: 2,227,450,368 || trainable%: 0.8290\n",
      "[INFO|trainer.py:667] 2024-11-11 22:37:25,653 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2243] 2024-11-11 22:37:26,119 >> ***** Running training *****\n",
      "[INFO|trainer.py:2244] 2024-11-11 22:37:26,119 >>   Num examples = 270\n",
      "[INFO|trainer.py:2245] 2024-11-11 22:37:26,119 >>   Num Epochs = 25\n",
      "[INFO|trainer.py:2246] 2024-11-11 22:37:26,119 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2249] 2024-11-11 22:37:26,119 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2250] 2024-11-11 22:37:26,119 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2251] 2024-11-11 22:37:26,119 >>   Total optimization steps = 200\n",
      "[INFO|trainer.py:2252] 2024-11-11 22:37:26,125 >>   Number of trainable parameters = 18,464,768\n",
      "  2%|█                                          | 5/200 [00:16<08:56,  2.75s/it][INFO|2024-11-11 22:37:42] llamafactory.train.callbacks:157 >> {'loss': 0.8317, 'learning_rate': 4.9923e-05, 'epoch': 0.59, 'throughput': 7452.77}\n",
      "{'loss': 0.8317, 'grad_norm': 0.2091410905122757, 'learning_rate': 4.99229333433282e-05, 'epoch': 0.59, 'num_input_tokens_seen': 121824}\n",
      "  5%|██                                        | 10/200 [00:27<06:59,  2.21s/it][INFO|2024-11-11 22:37:53] llamafactory.train.callbacks:157 >> {'loss': 0.7892, 'learning_rate': 4.9692e-05, 'epoch': 1.18, 'throughput': 8804.16}\n",
      "{'loss': 0.7892, 'grad_norm': 0.2185114026069641, 'learning_rate': 4.9692208514878444e-05, 'epoch': 1.18, 'num_input_tokens_seen': 239760}\n",
      "  8%|███▏                                      | 15/200 [00:38<06:58,  2.26s/it][INFO|2024-11-11 22:38:04] llamafactory.train.callbacks:157 >> {'loss': 0.7677, 'learning_rate': 4.9309e-05, 'epoch': 1.76, 'throughput': 9438.48}\n",
      "{'loss': 0.7677, 'grad_norm': 0.20165987312793732, 'learning_rate': 4.9309248009941914e-05, 'epoch': 1.76, 'num_input_tokens_seen': 364912}\n",
      " 10%|████▏                                     | 20/200 [00:49<06:45,  2.25s/it][INFO|2024-11-11 22:38:15] llamafactory.train.callbacks:157 >> {'loss': 0.7281, 'learning_rate': 4.8776e-05, 'epoch': 2.35, 'throughput': 9787.12}\n",
      "{'loss': 0.7281, 'grad_norm': 0.2064184546470642, 'learning_rate': 4.877641290737884e-05, 'epoch': 2.35, 'num_input_tokens_seen': 487440}\n",
      " 12%|█████▎                                    | 25/200 [01:00<06:28,  2.22s/it][INFO|2024-11-11 22:38:27] llamafactory.train.callbacks:157 >> {'loss': 0.7369, 'learning_rate': 4.8097e-05, 'epoch': 2.94, 'throughput': 9986.08}\n",
      "{'loss': 0.7369, 'grad_norm': 0.20706391334533691, 'learning_rate': 4.8096988312782174e-05, 'epoch': 2.94, 'num_input_tokens_seen': 607888}\n",
      " 15%|██████▎                                   | 30/200 [01:12<06:20,  2.24s/it][INFO|2024-11-11 22:38:38] llamafactory.train.callbacks:157 >> {'loss': 0.7225, 'learning_rate': 4.7275e-05, 'epoch': 3.53, 'throughput': 10095.83}\n",
      "{'loss': 0.7225, 'grad_norm': 0.19629089534282684, 'learning_rate': 4.72751631047092e-05, 'epoch': 3.53, 'num_input_tokens_seen': 728656}\n",
      " 18%|███████▎                                  | 35/200 [01:23<06:10,  2.25s/it][INFO|2024-11-11 22:38:49] llamafactory.train.callbacks:157 >> {'loss': 0.6913, 'learning_rate': 4.6316e-05, 'epoch': 4.12, 'throughput': 10189.24}\n",
      "{'loss': 0.6913, 'grad_norm': 0.2219933569431305, 'learning_rate': 4.6316004108852305e-05, 'epoch': 4.12, 'num_input_tokens_seen': 849216}\n",
      " 20%|████████▍                                 | 40/200 [01:34<05:53,  2.21s/it][INFO|2024-11-11 22:39:00] llamafactory.train.callbacks:157 >> {'loss': 0.6781, 'learning_rate': 4.5225e-05, 'epoch': 4.71, 'throughput': 10301.96}\n",
      "{'loss': 0.6781, 'grad_norm': 0.21249355375766754, 'learning_rate': 4.522542485937369e-05, 'epoch': 4.71, 'num_input_tokens_seen': 971440}\n",
      " 22%|█████████▍                                | 45/200 [01:45<05:52,  2.28s/it][INFO|2024-11-11 22:39:11] llamafactory.train.callbacks:157 >> {'loss': 0.6633, 'learning_rate': 4.4010e-05, 'epoch': 5.29, 'throughput': 10325.25}\n",
      "{'loss': 0.6633, 'grad_norm': 0.22328196465969086, 'learning_rate': 4.401014914000078e-05, 'epoch': 5.29, 'num_input_tokens_seen': 1091264}\n",
      " 25%|██████████▌                               | 50/200 [01:56<05:25,  2.17s/it][INFO|2024-11-11 22:39:22] llamafactory.train.callbacks:157 >> {'loss': 0.6484, 'learning_rate': 4.2678e-05, 'epoch': 5.88, 'throughput': 10395.22}\n",
      "{'loss': 0.6484, 'grad_norm': 0.25499823689460754, 'learning_rate': 4.267766952966369e-05, 'epoch': 5.88, 'num_input_tokens_seen': 1211184}\n",
      " 28%|███████████▌                              | 55/200 [02:07<05:21,  2.22s/it][INFO|2024-11-11 22:39:33] llamafactory.train.callbacks:157 >> {'loss': 0.6248, 'learning_rate': 4.1236e-05, 'epoch': 6.47, 'throughput': 10446.15}\n",
      "{'loss': 0.6248, 'grad_norm': 0.2620154023170471, 'learning_rate': 4.123620120825459e-05, 'epoch': 6.47, 'num_input_tokens_seen': 1334112}\n",
      " 30%|████████████▌                             | 60/200 [02:18<05:06,  2.19s/it][INFO|2024-11-11 22:39:44] llamafactory.train.callbacks:157 >> {'loss': 0.6228, 'learning_rate': 3.9695e-05, 'epoch': 7.06, 'throughput': 10475.49}\n",
      "{'loss': 0.6228, 'grad_norm': 0.31241604685783386, 'learning_rate': 3.969463130731183e-05, 'epoch': 7.06, 'num_input_tokens_seen': 1454512}\n",
      " 32%|█████████████▋                            | 65/200 [02:29<05:00,  2.23s/it][INFO|2024-11-11 22:39:55] llamafactory.train.callbacks:157 >> {'loss': 0.5956, 'learning_rate': 3.8062e-05, 'epoch': 7.65, 'throughput': 10520.59}\n",
      "{'loss': 0.5956, 'grad_norm': 0.28532159328460693, 'learning_rate': 3.8062464117898724e-05, 'epoch': 7.65, 'num_input_tokens_seen': 1576640}\n",
      " 35%|██████████████▋                           | 70/200 [02:41<04:55,  2.27s/it][INFO|2024-11-11 22:40:07] llamafactory.train.callbacks:157 >> {'loss': 0.5789, 'learning_rate': 3.6350e-05, 'epoch': 8.24, 'throughput': 10531.96}\n",
      "{'loss': 0.5789, 'grad_norm': 0.3202768564224243, 'learning_rate': 3.634976249348867e-05, 'epoch': 8.24, 'num_input_tokens_seen': 1698624}\n",
      " 38%|███████████████▊                          | 75/200 [02:52<04:33,  2.19s/it][INFO|2024-11-11 22:40:18] llamafactory.train.callbacks:157 >> {'loss': 0.5755, 'learning_rate': 3.4567e-05, 'epoch': 8.82, 'throughput': 10557.32}\n",
      "{'loss': 0.5755, 'grad_norm': 0.35822543501853943, 'learning_rate': 3.456708580912725e-05, 'epoch': 8.82, 'num_input_tokens_seen': 1818688}\n",
      " 40%|████████████████▊                         | 80/200 [03:03<04:28,  2.24s/it][INFO|2024-11-11 22:40:29] llamafactory.train.callbacks:157 >> {'loss': 0.5406, 'learning_rate': 3.2725e-05, 'epoch': 9.41, 'throughput': 10579.35}\n",
      "{'loss': 0.5406, 'grad_norm': 0.3345039188861847, 'learning_rate': 3.272542485937369e-05, 'epoch': 9.41, 'num_input_tokens_seen': 1942112}\n",
      " 42%|█████████████████▊                        | 85/200 [03:14<04:14,  2.21s/it][INFO|2024-11-11 22:40:40] llamafactory.train.callbacks:157 >> {'loss': 0.5462, 'learning_rate': 3.0836e-05, 'epoch': 10.00, 'throughput': 10593.27}\n",
      "{'loss': 0.5462, 'grad_norm': 0.3895969092845917, 'learning_rate': 3.083613409639764e-05, 'epoch': 10.0, 'num_input_tokens_seen': 2061552}\n",
      " 45%|██████████████████▉                       | 90/200 [03:25<04:04,  2.22s/it][INFO|2024-11-11 22:40:51] llamafactory.train.callbacks:157 >> {'loss': 0.5048, 'learning_rate': 2.8911e-05, 'epoch': 10.59, 'throughput': 10620.56}\n",
      "{'loss': 0.5048, 'grad_norm': 0.3603554368019104, 'learning_rate': 2.8910861626005776e-05, 'epoch': 10.59, 'num_input_tokens_seen': 2185344}\n",
      " 48%|███████████████████▉                      | 95/200 [03:37<03:59,  2.29s/it][INFO|2024-11-11 22:41:03] llamafactory.train.callbacks:157 >> {'loss': 0.5067, 'learning_rate': 2.6961e-05, 'epoch': 11.18, 'throughput': 10621.47}\n",
      "{'loss': 0.5067, 'grad_norm': 0.38494327664375305, 'learning_rate': 2.6961477393196126e-05, 'epoch': 11.18, 'num_input_tokens_seen': 2306256}\n",
      " 50%|████████████████████▌                    | 100/200 [03:48<03:39,  2.20s/it][INFO|2024-11-11 22:41:14] llamafactory.train.callbacks:157 >> {'loss': 0.4892, 'learning_rate': 2.5000e-05, 'epoch': 11.76, 'throughput': 10637.35}\n",
      "{'loss': 0.4892, 'grad_norm': 0.43097996711730957, 'learning_rate': 2.5e-05, 'epoch': 11.76, 'num_input_tokens_seen': 2426592}\n",
      " 50%|████████████████████▌                    | 100/200 [03:48<03:39,  2.20s/it][INFO|trainer.py:3705] 2024-11-11 22:41:14,253 >> Saving model checkpoint to saves/Qwen2-VL-2B-Instruct/lora/train_2024-11-11-22-27-14/checkpoint-100\n",
      "[INFO|configuration_utils.py:675] 2024-11-11 22:41:15,174 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-11-11 22:41:15,177 >> Model config Qwen2VLConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2641] 2024-11-11 22:41:15,292 >> tokenizer config file saved in saves/Qwen2-VL-2B-Instruct/lora/train_2024-11-11-22-27-14/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2650] 2024-11-11 22:41:15,292 >> Special tokens file saved in saves/Qwen2-VL-2B-Instruct/lora/train_2024-11-11-22-27-14/checkpoint-100/special_tokens_map.json\n",
      "[INFO|image_processing_base.py:258] 2024-11-11 22:41:15,577 >> Image processor saved in saves/Qwen2-VL-2B-Instruct/lora/train_2024-11-11-22-27-14/checkpoint-100/preprocessor_config.json\n",
      "[INFO|tokenization_utils_base.py:2641] 2024-11-11 22:41:15,578 >> tokenizer config file saved in saves/Qwen2-VL-2B-Instruct/lora/train_2024-11-11-22-27-14/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2650] 2024-11-11 22:41:15,578 >> Special tokens file saved in saves/Qwen2-VL-2B-Instruct/lora/train_2024-11-11-22-27-14/checkpoint-100/special_tokens_map.json\n",
      "[INFO|processing_utils.py:530] 2024-11-11 22:41:16,062 >> chat template saved in saves/Qwen2-VL-2B-Instruct/lora/train_2024-11-11-22-27-14/checkpoint-100/chat_template.json\n",
      " 52%|█████████████████████▌                   | 105/200 [04:01<03:47,  2.39s/it][INFO|2024-11-11 22:41:27] llamafactory.train.callbacks:157 >> {'loss': 0.4710, 'learning_rate': 2.3039e-05, 'epoch': 12.35, 'throughput': 10561.31}\n",
      "{'loss': 0.471, 'grad_norm': 0.47492387890815735, 'learning_rate': 2.303852260680388e-05, 'epoch': 12.35, 'num_input_tokens_seen': 2548672}\n",
      " 55%|██████████████████████▌                  | 110/200 [04:12<03:22,  2.25s/it][INFO|2024-11-11 22:41:38] llamafactory.train.callbacks:157 >> {'loss': 0.4617, 'learning_rate': 2.1089e-05, 'epoch': 12.94, 'throughput': 10570.75}\n",
      "{'loss': 0.4617, 'grad_norm': 0.4394392967224121, 'learning_rate': 2.1089138373994223e-05, 'epoch': 12.94, 'num_input_tokens_seen': 2667984}\n",
      " 57%|███████████████████████▌                 | 115/200 [04:23<03:18,  2.33s/it][INFO|2024-11-11 22:41:49] llamafactory.train.callbacks:157 >> {'loss': 0.4388, 'learning_rate': 1.9164e-05, 'epoch': 13.53, 'throughput': 10584.25}\n",
      "{'loss': 0.4388, 'grad_norm': 0.5069657564163208, 'learning_rate': 1.9163865903602374e-05, 'epoch': 13.53, 'num_input_tokens_seen': 2791904}\n",
      " 60%|████████████████████████▌                | 120/200 [04:34<03:01,  2.27s/it][INFO|2024-11-11 22:42:00] llamafactory.train.callbacks:157 >> {'loss': 0.4393, 'learning_rate': 1.7275e-05, 'epoch': 14.12, 'throughput': 10591.30}\n",
      "{'loss': 0.4393, 'grad_norm': 0.4666094183921814, 'learning_rate': 1.7274575140626318e-05, 'epoch': 14.12, 'num_input_tokens_seen': 2910560}\n",
      " 62%|█████████████████████████▋               | 125/200 [04:45<02:48,  2.25s/it][INFO|2024-11-11 22:42:12] llamafactory.train.callbacks:157 >> {'loss': 0.4120, 'learning_rate': 1.5433e-05, 'epoch': 14.71, 'throughput': 10610.89}\n",
      "{'loss': 0.412, 'grad_norm': 0.48239991068840027, 'learning_rate': 1.5432914190872757e-05, 'epoch': 14.71, 'num_input_tokens_seen': 3034368}\n",
      " 65%|██████████████████████████▋              | 130/200 [04:57<02:37,  2.25s/it][INFO|2024-11-11 22:42:23] llamafactory.train.callbacks:157 >> {'loss': 0.4209, 'learning_rate': 1.3650e-05, 'epoch': 15.29, 'throughput': 10611.48}\n",
      "{'loss': 0.4209, 'grad_norm': 0.516402006149292, 'learning_rate': 1.3650237506511331e-05, 'epoch': 15.29, 'num_input_tokens_seen': 3153984}\n",
      " 68%|███████████████████████████▋             | 135/200 [05:08<02:25,  2.24s/it][INFO|2024-11-11 22:42:34] llamafactory.train.callbacks:157 >> {'loss': 0.3875, 'learning_rate': 1.1938e-05, 'epoch': 15.88, 'throughput': 10626.41}\n",
      "{'loss': 0.3875, 'grad_norm': 0.5024691224098206, 'learning_rate': 1.1937535882101281e-05, 'epoch': 15.88, 'num_input_tokens_seen': 3278336}\n",
      " 70%|████████████████████████████▋            | 140/200 [05:19<02:13,  2.22s/it][INFO|2024-11-11 22:42:45] llamafactory.train.callbacks:157 >> {'loss': 0.3979, 'learning_rate': 1.0305e-05, 'epoch': 16.47, 'throughput': 10639.73}\n",
      "{'loss': 0.3979, 'grad_norm': 0.497351735830307, 'learning_rate': 1.0305368692688174e-05, 'epoch': 16.47, 'num_input_tokens_seen': 3398224}\n",
      " 72%|█████████████████████████████▋           | 145/200 [05:30<02:00,  2.18s/it][INFO|2024-11-11 22:42:56] llamafactory.train.callbacks:157 >> {'loss': 0.3782, 'learning_rate': 8.7638e-06, 'epoch': 17.06, 'throughput': 10650.15}\n",
      "{'loss': 0.3782, 'grad_norm': 0.5652949810028076, 'learning_rate': 8.763798791745411e-06, 'epoch': 17.06, 'num_input_tokens_seen': 3519168}\n",
      " 75%|██████████████████████████████▊          | 150/200 [05:41<01:51,  2.22s/it][INFO|2024-11-11 22:43:07] llamafactory.train.callbacks:157 >> {'loss': 0.3776, 'learning_rate': 7.3223e-06, 'epoch': 17.65, 'throughput': 10658.48}\n",
      "{'loss': 0.3776, 'grad_norm': 0.5505023002624512, 'learning_rate': 7.3223304703363135e-06, 'epoch': 17.65, 'num_input_tokens_seen': 3641392}\n",
      " 78%|███████████████████████████████▊         | 155/200 [05:52<01:41,  2.27s/it][INFO|2024-11-11 22:43:18] llamafactory.train.callbacks:157 >> {'loss': 0.3649, 'learning_rate': 5.9899e-06, 'epoch': 18.24, 'throughput': 10667.92}\n",
      "{'loss': 0.3649, 'grad_norm': 0.4925500750541687, 'learning_rate': 5.989850859999227e-06, 'epoch': 18.24, 'num_input_tokens_seen': 3764240}\n",
      " 80%|████████████████████████████████▊        | 160/200 [06:03<01:29,  2.23s/it][INFO|2024-11-11 22:43:29] llamafactory.train.callbacks:157 >> {'loss': 0.3730, 'learning_rate': 4.7746e-06, 'epoch': 18.82, 'throughput': 10675.94}\n",
      "{'loss': 0.373, 'grad_norm': 0.5581334233283997, 'learning_rate': 4.7745751406263165e-06, 'epoch': 18.82, 'num_input_tokens_seen': 3882896}\n",
      " 82%|█████████████████████████████████▊       | 165/200 [06:14<01:18,  2.23s/it][INFO|2024-11-11 22:43:40] llamafactory.train.callbacks:157 >> {'loss': 0.3606, 'learning_rate': 3.6840e-06, 'epoch': 19.41, 'throughput': 10683.70}\n",
      "{'loss': 0.3606, 'grad_norm': 0.5196709632873535, 'learning_rate': 3.6839958911476957e-06, 'epoch': 19.41, 'num_input_tokens_seen': 4004560}\n",
      " 85%|██████████████████████████████████▊      | 170/200 [06:26<01:07,  2.24s/it][INFO|2024-11-11 22:43:52] llamafactory.train.callbacks:157 >> {'loss': 0.3642, 'learning_rate': 2.7248e-06, 'epoch': 20.00, 'throughput': 10688.45}\n",
      "{'loss': 0.3642, 'grad_norm': 0.5688323378562927, 'learning_rate': 2.7248368952908053e-06, 'epoch': 20.0, 'num_input_tokens_seen': 4127360}\n",
      " 88%|███████████████████████████████████▉     | 175/200 [06:37<00:56,  2.25s/it][INFO|2024-11-11 22:44:03] llamafactory.train.callbacks:157 >> {'loss': 0.3549, 'learning_rate': 1.9030e-06, 'epoch': 20.59, 'throughput': 10691.85}\n",
      "{'loss': 0.3549, 'grad_norm': 0.5372576117515564, 'learning_rate': 1.9030116872178316e-06, 'epoch': 20.59, 'num_input_tokens_seen': 4248640}\n",
      " 90%|████████████████████████████████████▉    | 180/200 [06:48<00:45,  2.28s/it][INFO|2024-11-11 22:44:14] llamafactory.train.callbacks:157 >> {'loss': 0.3541, 'learning_rate': 1.2236e-06, 'epoch': 21.18, 'throughput': 10694.45}\n",
      "{'loss': 0.3541, 'grad_norm': 0.5008477568626404, 'learning_rate': 1.2235870926211619e-06, 'epoch': 21.18, 'num_input_tokens_seen': 4369872}\n",
      " 92%|█████████████████████████████████████▉   | 185/200 [06:59<00:33,  2.23s/it][INFO|2024-11-11 22:44:25] llamafactory.train.callbacks:157 >> {'loss': 0.3554, 'learning_rate': 6.9075e-07, 'epoch': 21.76, 'throughput': 10696.56}\n",
      "{'loss': 0.3554, 'grad_norm': 0.5176458358764648, 'learning_rate': 6.907519900580861e-07, 'epoch': 21.76, 'num_input_tokens_seen': 4490688}\n",
      " 95%|██████████████████████████████████████▉  | 190/200 [07:11<00:22,  2.25s/it][INFO|2024-11-11 22:44:37] llamafactory.train.callbacks:157 >> {'loss': 0.3621, 'learning_rate': 3.0779e-07, 'epoch': 22.35, 'throughput': 10698.49}\n",
      "{'loss': 0.3621, 'grad_norm': 0.5638202428817749, 'learning_rate': 3.077914851215585e-07, 'epoch': 22.35, 'num_input_tokens_seen': 4612144}\n",
      " 98%|███████████████████████████████████████▉ | 195/200 [07:22<00:11,  2.23s/it][INFO|2024-11-11 22:44:48] llamafactory.train.callbacks:157 >> {'loss': 0.3455, 'learning_rate': 7.7067e-08, 'epoch': 22.94, 'throughput': 10704.37}\n",
      "{'loss': 0.3455, 'grad_norm': 0.5257574319839478, 'learning_rate': 7.706665667180091e-08, 'epoch': 22.94, 'num_input_tokens_seen': 4733632}\n",
      " 99%|████████████████████████████████████████▌| 198/200 [07:28<00:04,  2.24s/it]"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7640dfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
