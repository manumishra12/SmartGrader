{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "edebb3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 22217, done.\u001b[K\n",
      "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
      "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
      "remote: Total 22217 (delta 9), reused 2 (delta 2), pack-reused 22193 (from 3)\u001b[K\n",
      "Receiving objects: 100% (22217/22217), 42.93 MiB | 5.43 MiB/s, done.\n",
      "Resolving deltas: 100% (16192/16192), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b654fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "87c52cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff  Makefile      \u001b[0m\u001b[01;34massets\u001b[0m/  \u001b[01;34mevaluation\u001b[0m/     requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
      "LICENSE       README.md     \u001b[01;34mdata\u001b[0m/    \u001b[01;34mexamples\u001b[0m/       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
      "MANIFEST.in   README_zh.md  \u001b[01;34mdocker\u001b[0m/  pyproject.toml  setup.py\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04e8854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (4.49.0)\n",
      "Requirement already satisfied: datasets<=3.3.2,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (3.3.2)\n",
      "Requirement already satisfied: accelerate<=1.4.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (1.0.1)\n",
      "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.12.0)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.9.6)\n",
      "Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.21.0)\n",
      "Requirement already satisfied: gradio<=5.21.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (4.44.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (1.12.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (3.20.3)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.34.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (2.5.3)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.115.11)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (2.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (3.8.2)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.7.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (1.24.4)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (14.2.0)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.10.1)\n",
      "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.8.14)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (2.6.0+cu118)\n",
      "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2) (0.45.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.4.0,>=0.34.0->llamafactory==0.9.2) (5.9.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.4.0,>=0.34.0->llamafactory==0.9.2) (0.29.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.4.0,>=0.34.0->llamafactory==0.9.2) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (3.9.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (4.8.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (0.28.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (6.5.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (2.1.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (3.10.15)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (10.2.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (0.9.10)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (2.3.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (12.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.9.2) (0.46.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2) (2.14.6)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (2.6.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.2) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->llamafactory==0.9.2) (2023.12.25)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2) (1.7.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.2) (2.5.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2) (0.57.1+1.g1ff679645)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2) (0.3.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2) (1.0.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (4.0.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.2) (0.40.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->llamafactory==0.9.2) (4.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.2) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.3.2,>=2.16.0->llamafactory==0.9.2) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2) (2.17.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.2) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.2) (1.16.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.2) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.2) (2.21)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2) (0.1.2)\n",
      "Building wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.2-0.editable-py3-none-any.whl size=25905 sha256=14f6383653f260a6696316d753557b01c8844bb23a10b8fad3ffd0ec8893f737\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vas4_bjt/wheels/43/ae/95/b78a7eb35733a74e644b1f49a50dda44d4222f563f1010994e\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.9.2\n",
      "    Uninstalling llamafactory-0.9.2:\n",
      "      Successfully uninstalled llamafactory-0.9.2\n",
      "Successfully installed llamafactory-0.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e .[torch,bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a475067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Ignoring transformers: markers 'python_version < \"3.10\"' don't match your environment\n",
      "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.49.0)\n",
      "Requirement already satisfied: datasets<=3.3.2,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: accelerate<=1.4.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.12.0)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.9.6)\n",
      "Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.21.0)\n",
      "Requirement already satisfied: gradio<=5.21.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.44.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.12.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.20.3)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.34.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (2.5.3)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (0.115.11)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (2.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.8.2)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (0.7.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (1.24.4)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (14.2.0)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (0.10.1)\n",
      "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (0.8.14)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (0.29.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (3.9.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (2.6.0+cu118)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (4.8.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (0.28.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (6.5.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (2.1.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (3.10.15)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (10.2.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (0.9.10)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (2.3.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 9)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 9)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 9)) (2025.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->-r requirements.txt (line 15)) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->-r requirements.txt (line 15)) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->-r requirements.txt (line 16)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic->-r requirements.txt (line 16)) (2.14.6)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->-r requirements.txt (line 17)) (0.46.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (3.1.1)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 20)) (2.5.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (0.57.1+1.g1ff679645)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (0.3.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (1.0.7)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->-r requirements.txt (line 26)) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->-r requirements.txt (line 26)) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->-r requirements.txt (line 26)) (1.7.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.3.2,>=2.16.0->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 25)) (0.40.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->-r requirements.txt (line 25)) (4.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26)) (2.17.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->-r requirements.txt (line 25)) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 25)) (1.16.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (2.6.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate<=1.4.0,>=0.34.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 25)) (2.21)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26)) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f9d90d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.6.0+cu118)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add869b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de18419",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_TOKEN=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56045b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://0f6df2bbd4b458d5ce.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[INFO|2025-03-08 16:55:15] llamafactory.cli:157 >> Initializing 2 distributed tasks at: 127.0.0.1:21214\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "W0308 16:55:16.604000 2932 torch/distributed/run.py:792] \n",
      "W0308 16:55:16.604000 2932 torch/distributed/run.py:792] *****************************************\n",
      "W0308 16:55:16.604000 2932 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0308 16:55:16.604000 2932 torch/distributed/run.py:792] *****************************************\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[INFO|2025-03-08 16:55:20] llamafactory.hparams.parser:384 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "config.json: 100%|| 826/826 [00:00<00:00, 2.16MB/s]\n",
      "[WARNING|2025-03-08 16:55:21] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[WARNING|2025-03-08 16:55:21] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-03-08 16:55:21] llamafactory.hparams.parser:384 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "tokenizer_config.json: 100%|| 3.07k/3.07k [00:00<00:00, 10.1MB/s]\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 16:55:21,728 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 16:55:21,732 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer.json: 100%|| 9.08M/9.08M [00:01<00:00, 6.26MB/s]\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:24,515 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:24,516 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:24,516 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:24,516 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:24,516 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:24,516 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 16:55:25,015 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 16:55:25,926 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 16:55:25,928 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:26,146 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:26,146 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:26,146 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:26,147 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:26,147 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 16:55:26,147 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 16:55:26,641 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-03-08 16:55:26] llamafactory.data.loader:157 >> Loading dataset manumishra/dsa_qa_pairs...\n",
      "[rank1]:[W308 16:55:26.954556436 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "README.md: 100%|| 28.0/28.0 [00:00<00:00, 165kB/s]\n",
      "Combined.json: 100%|| 204k/204k [00:00<00:00, 1.01MB/s]\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 100%|| 1/1 [00:00<00:00, 86.47 examples/s]\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Converting format of dataset: 100%|| 1/1 [00:00<00:00, 52.49 examples/s]\n",
      "[rank0]:[W308 16:55:32.862544781 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Running tokenizer on dataset:   0%|                | 0/1 [00:00<?, ? examples/s][WARNING|2025-03-08 16:58:36] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset: 100%|| 1/1 [00:00<00:00, 42.55 examples/s]\n",
      "training example:\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 269, in _get_preprocessed_dataset\n",
      "[rank0]:     dataset_processor.print_data_example(next(iter(dataset)))\n",
      "[rank0]: StopIteration\n",
      "\n",
      "[rank0]: During handling of the above exception, another exception occurred:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 103, in run_exp\n",
      "[rank0]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 68, in _training_function\n",
      "[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n",
      "[rank0]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 314, in get_dataset\n",
      "[rank0]:     dataset = _get_preprocessed_dataset(\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 274, in _get_preprocessed_dataset\n",
      "[rank0]:     raise RuntimeError(\"Cannot find valid samples, check `data/README.md` for the data format.\")\n",
      "[rank0]: RuntimeError: Cannot find valid samples, check `data/README.md` for the data format.\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "[rank0]:[W308 16:58:42.064188775 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "model.safetensors.index.json: 100%|| 24.2k/24.2k [00:00<00:00, 49.4MB/s]\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]W0308 16:58:43.584000 2932 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2998 closing signal SIGTERM\n",
      "E0308 16:58:43.849000 2932 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 2997) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 918, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-03-08_16:58:43\n",
      "  host      : 7ae64d46f1c7\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 2997)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[INFO|2025-03-08 17:01:38] llamafactory.cli:157 >> Initializing 2 distributed tasks at: 127.0.0.1:20218\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "W0308 17:01:39.927000 3178 torch/distributed/run.py:792] \n",
      "W0308 17:01:39.927000 3178 torch/distributed/run.py:792] *****************************************\n",
      "W0308 17:01:39.927000 3178 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0308 17:01:39.927000 3178 torch/distributed/run.py:792] *****************************************\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[INFO|2025-03-08 17:01:43] llamafactory.hparams.parser:384 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.float16\n",
      "[WARNING|2025-03-08 17:01:44] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[WARNING|2025-03-08 17:01:44] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-03-08 17:01:44] llamafactory.hparams.parser:384 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.float16\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 17:01:44,630 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 17:01:44,633 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:45,020 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:45,021 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:45,021 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:45,021 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:45,021 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:45,022 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 17:01:45,464 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 17:01:46,573 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 17:01:46,575 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[rank1]:[W308 17:01:46.763588860 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:46,794 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:46,794 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:46,794 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:46,794 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:46,794 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:01:46,795 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 17:01:47,211 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-03-08 17:01:47] llamafactory.data.loader:157 >> Loading dataset manumishra/dsa_qa_pairs...\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "[rank0]:[W308 17:01:51.000078228 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "training example:\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 269, in _get_preprocessed_dataset\n",
      "[rank0]:     dataset_processor.print_data_example(next(iter(dataset)))\n",
      "[rank0]: StopIteration\n",
      "\n",
      "[rank0]: During handling of the above exception, another exception occurred:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 103, in run_exp\n",
      "[rank0]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 68, in _training_function\n",
      "[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n",
      "[rank0]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 314, in get_dataset\n",
      "[rank0]:     dataset = _get_preprocessed_dataset(\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 274, in _get_preprocessed_dataset\n",
      "[rank0]:     raise RuntimeError(\"Cannot find valid samples, check `data/README.md` for the data format.\")\n",
      "[rank0]: RuntimeError: Cannot find valid samples, check `data/README.md` for the data format.\n",
      "[rank0]:[W308 17:05:00.674174514 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-000002.safetensors:   0%|            | 0.00/8.67G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   0%|   | 10.5M/8.67G [00:00<02:34, 56.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   0%|   | 21.0M/8.67G [00:00<02:43, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   0%|   | 31.5M/8.67G [00:00<02:43, 52.8MB/s]\u001b[AW0308 17:05:01.846000 3178 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3244 closing signal SIGTERM\n",
      "E0308 17:05:02.111000 3178 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 3243) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 918, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-03-08_17:05:01\n",
      "  host      : 7ae64d46f1c7\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 3243)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7860 <> https://0f6df2bbd4b458d5ce.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40bd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4bac79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690478ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5b0b931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found at: /workspace/llm_finetune/LLaMA-Factory/llm_finetune/Combined.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the file exists\n",
    "file_path = \"llm_finetune/Combined.json\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found at: {os.path.abspath(file_path)}\")  # Print absolute path <button class=\"citation-flag\" data-index=\"5\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1335c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f3afb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://44468424c90def3ea9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[INFO|2025-03-08 17:31:19] llamafactory.cli:157 >> Initializing 2 distributed tasks at: 127.0.0.1:21600\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "W0308 17:31:21.007000 3629 torch/distributed/run.py:792] \n",
      "W0308 17:31:21.007000 3629 torch/distributed/run.py:792] *****************************************\n",
      "W0308 17:31:21.007000 3629 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0308 17:31:21.007000 3629 torch/distributed/run.py:792] *****************************************\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[WARNING|2025-03-08 17:31:24] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-03-08 17:31:24] llamafactory.hparams.parser:384 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 17:31:25,231 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 17:31:25,234 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:25,467 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:25,467 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:25,468 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:25,468 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:25,468 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:25,468 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|2025-03-08 17:31:25] llamafactory.hparams.parser:384 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 17:31:25,929 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 17:31:27,260 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 17:31:27,261 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:27,480 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:27,481 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:27,481 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:27,481 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:27,481 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 17:31:27,481 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 17:31:27,938 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-03-08 17:31:27] llamafactory.data.loader:157 >> Loading dataset manumishra/dsa_llm...\n",
      "README.md: 100%|| 28.0/28.0 [00:00<00:00, 144kB/s]\n",
      "[rank1]:[W308 17:31:29.413665434 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "dataset_updated.json: 100%|| 256k/256k [00:00<00:00, 641kB/s]\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 100%|| 1/1 [00:00<00:00, 92.23 examples/s]\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Converting format of dataset: 100%|| 1/1 [00:00<00:00, 53.93 examples/s]\n",
      "[rank0]:[W308 17:31:34.122707357 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Running tokenizer on dataset:   0%|                | 0/1 [00:00<?, ? examples/s][WARNING|2025-03-08 17:34:38] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset: 100%|| 1/1 [00:00<00:00, 33.77 examples/s]\n",
      "training example:\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 269, in _get_preprocessed_dataset\n",
      "[rank0]:     dataset_processor.print_data_example(next(iter(dataset)))\n",
      "[rank0]: StopIteration\n",
      "\n",
      "[rank0]: During handling of the above exception, another exception occurred:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 103, in run_exp\n",
      "[rank0]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 68, in _training_function\n",
      "[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n",
      "[rank0]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 314, in get_dataset\n",
      "[rank0]:     dataset = _get_preprocessed_dataset(\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 274, in _get_preprocessed_dataset\n",
      "[rank0]:     raise RuntimeError(\"Cannot find valid samples, check `data/README.md` for the data format.\")\n",
      "[rank0]: RuntimeError: Cannot find valid samples, check `data/README.md` for the data format.\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "[rank0]:[W308 17:34:43.420521132 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-000002.safetensors:   0%|           | 31.5M/8.67G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   0%|   | 41.9M/8.67G [00:00<02:18, 62.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   1%|   | 52.4M/8.67G [00:00<02:38, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   1%|   | 62.9M/8.67G [00:00<02:33, 56.0MB/s]\u001b[AW0308 17:34:44.574000 3629 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3695 closing signal SIGTERM\n",
      "E0308 17:34:44.840000 3629 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 3694) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 918, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-03-08_17:34:44\n",
      "  host      : 7ae64d46f1c7\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 3694)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7860 <> https://44468424c90def3ea9.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7c17c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  8 17:35:54 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 PCIe               Off |   00000000:2A:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             89W /  350W |   20402MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 PCIe               Off |   00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   43C    P0             82W /  350W |    6499MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d932cc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113676e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d02a3ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=0 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "539d381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://1ada422af3e812e4bf.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7860 <> https://1ada422af3e812e4bf.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd5fdf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://71b22ebb84036175a6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[INFO|2025-03-08 18:09:35] llamafactory.cli:157 >> Initializing 2 distributed tasks at: 127.0.0.1:26075\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "W0308 18:09:37.137000 4337 torch/distributed/run.py:792] \n",
      "W0308 18:09:37.137000 4337 torch/distributed/run.py:792] *****************************************\n",
      "W0308 18:09:37.137000 4337 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0308 18:09:37.137000 4337 torch/distributed/run.py:792] *****************************************\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[WARNING|2025-03-08 18:09:41] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[WARNING|2025-03-08 18:09:41] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-03-08 18:09:41] llamafactory.hparams.parser:384 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-03-08 18:09:41] llamafactory.hparams.parser:384 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 18:09:41,341 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 18:09:41,345 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:41,583 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:41,584 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:41,584 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:41,585 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:41,585 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:41,585 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 18:09:42,030 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 18:09:42,956 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 18:09:42,958 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:43,187 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:43,187 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:43,188 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:43,188 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:43,189 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 18:09:43,189 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 18:09:43,624 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-03-08 18:09:43] llamafactory.data.loader:157 >> Loading dataset manumishra/dsa_llm_new...\n",
      "README.md: 100%|| 384/384 [00:00<00:00, 1.85MB/s]\n",
      "[rank1]:[W308 18:09:44.626502358 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "train-00000-of-00001.parquet: 100%|| 60.9k/60.9k [00:00<00:00, 53.7MB/s]\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 100%|| 679/679 [00:00<00:00, 71727.30 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|| 679/679 [00:00<00:00, 2121.0\n",
      "[rank0]:[W308 18:09:50.866774451 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "Running tokenizer on dataset (num_proc=16):   0%| | 0/679 [00:00<?, ? examples/s[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):   6%| | 43/679 [00:01<00:19, 33.14 e[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  13%|| 86/679 [00:01<00:09, 62.16 e[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:57] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  19%|| 129/679 [00:01<00:06, 87.21 [WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  32%|| 215/679 [00:02<00:03, 121.45[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  38%|| 258/679 [00:02<00:03, 131.41[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:58] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  51%|| 343/679 [00:02<00:01, 181.76[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  57%|| 385/679 [00:03<00:02, 143.09[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  69%|| 469/679 [00:03<00:01, 183.41[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:12:59] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  75%|| 511/679 [00:03<00:00, 175.75[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  81%|| 553/679 [00:04<00:00, 178.18[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  88%|| 595/679 [00:04<00:00, 171.13[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16):  94%|| 637/679 [00:04<00:00, 179.97[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "[WARNING|2025-03-08 18:13:00] llamafactory.data.processor.supervised:162 >> Dropped invalid example: [{'content': '', 'role': 'user'}]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|| 679/679 [00:04<00:00, 138.42\n",
      "training example:\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 269, in _get_preprocessed_dataset\n",
      "[rank0]:     dataset_processor.print_data_example(next(iter(dataset)))\n",
      "[rank0]: StopIteration\n",
      "\n",
      "[rank0]: During handling of the above exception, another exception occurred:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 103, in run_exp\n",
      "[rank0]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 68, in _training_function\n",
      "[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n",
      "[rank0]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 314, in get_dataset\n",
      "[rank0]:     dataset = _get_preprocessed_dataset(\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/data/loader.py\", line 274, in _get_preprocessed_dataset\n",
      "[rank0]:     raise RuntimeError(\"Cannot find valid samples, check `data/README.md` for the data format.\")\n",
      "[rank0]: RuntimeError: Cannot find valid samples, check `data/README.md` for the data format.\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s][rank0]:[W308 18:13:01.778389898 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "\n",
      "model-00001-of-000002.safetensors:   1%|           | 62.9M/8.67G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   1%|   | 73.4M/8.67G [00:00<02:43, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   1%|   | 83.9M/8.67G [00:00<02:48, 51.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   1%|   | 94.4M/8.67G [00:00<02:51, 49.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   1%|    | 105M/8.67G [00:00<02:51, 49.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   1%|    | 115M/8.67G [00:01<02:49, 50.5MB/s]\u001b[AW0308 18:13:03.199000 4337 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 4403 closing signal SIGTERM\n",
      "E0308 18:13:03.465000 4337 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 4402) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 918, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/workspace/llm_finetune/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-03-08_18:13:03\n",
      "  host      : 7ae64d46f1c7\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 4402)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7860 <> https://71b22ebb84036175a6.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53ea45c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://dc7490390d9c1cfc39.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[INFO|2025-03-08 20:22:36] llamafactory.cli:157 >> Initializing 2 distributed tasks at: 127.0.0.1:21775\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "W0308 20:22:37.451000 4875 torch/distributed/run.py:792] \n",
      "W0308 20:22:37.451000 4875 torch/distributed/run.py:792] *****************************************\n",
      "W0308 20:22:37.451000 4875 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0308 20:22:37.451000 4875 torch/distributed/run.py:792] *****************************************\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[WARNING|2025-03-08 20:22:41] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[WARNING|2025-03-08 20:22:41] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-03-08 20:22:41] llamafactory.hparams.parser:384 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:22:42,049 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:22:42,052 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-03-08 20:22:42] llamafactory.hparams.parser:384 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:42,281 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:42,281 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:42,281 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:42,281 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:42,281 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:42,282 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 20:22:42,741 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:22:44,025 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:22:44,027 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:44,248 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:44,248 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:44,248 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:44,248 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:44,249 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:22:44,249 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 20:22:44,764 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-03-08 20:22:44] llamafactory.data.loader:157 >> Loading dataset manumishra/dsa_llm_new...\n",
      "[rank1]:[W308 20:22:44.983542777 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "Converting format of dataset (num_proc=16): 100%|| 679/679 [00:00<00:00, 2274.1\n",
      "[rank0]:[W308 20:22:50.720169674 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "Running tokenizer on dataset (num_proc=16): 100%|| 679/679 [00:04<00:00, 161.32\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128011, 50, 4035, 420, 2956, 71150, 612, 86859, 3575, 198, 3923, 374, 279, 892, 23965, 315, 279, 17737, 90054, 15583, 320, 87514, 8, 12384, 30, 128012, 791, 17737, 90054, 15583, 320, 87514, 8, 12384, 706, 264, 892, 23965, 315, 507, 1471, 1515, 308, 705, 1405, 308, 374, 279, 1396, 315, 3585, 304, 279, 1988, 13, 1115, 15374, 3727, 433, 13882, 1511, 304, 8450, 8863, 11, 48411, 47544, 11, 323, 1023, 8522, 23537, 11900, 8106, 6492, 13, 128001]\n",
      "inputs:\n",
      "<beginofsentence><User>Solve this Data Structures & Algorithms problem\n",
      "What is the time complexity of the Fast Fourier Transform (FFT) algorithm?<Assistant>The Fast Fourier Transform (FFT) algorithm has a time complexity of O(n log n), where n is the number of points in the input. This efficiency makes it widely used in signal processing, polynomial multiplication, and other applications requiring frequency domain analysis.<endofsentence>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 791, 17737, 90054, 15583, 320, 87514, 8, 12384, 706, 264, 892, 23965, 315, 507, 1471, 1515, 308, 705, 1405, 308, 374, 279, 1396, 315, 3585, 304, 279, 1988, 13, 1115, 15374, 3727, 433, 13882, 1511, 304, 8450, 8863, 11, 48411, 47544, 11, 323, 1023, 8522, 23537, 11900, 8106, 6492, 13, 128001]\n",
      "labels:\n",
      "The Fast Fourier Transform (FFT) algorithm has a time complexity of O(n log n), where n is the number of points in the input. This efficiency makes it widely used in signal processing, polynomial multiplication, and other applications requiring frequency domain analysis.<endofsentence>\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:25:58,704 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:25:58,706 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-03-08 20:25:58] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|modeling_utils.py:3982] 2025-03-08 20:25:58,820 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/model.safetensors.index.json\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-000002.safetensors:   1%|           | 115M/8.67G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   1%|    | 126M/8.67G [00:00<02:31, 56.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   2%|    | 136M/8.67G [00:00<02:38, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   2%|    | 147M/8.67G [00:00<02:37, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   2%|    | 157M/8.67G [00:00<02:37, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   2%|    | 168M/8.67G [00:00<02:37, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   2%|    | 178M/8.67G [00:01<02:38, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   2%|    | 189M/8.67G [00:01<02:36, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   2%|    | 199M/8.67G [00:01<02:36, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   2%|    | 210M/8.67G [00:01<02:33, 55.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   3%|    | 220M/8.67G [00:01<02:34, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   3%|    | 231M/8.67G [00:02<02:35, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   3%|    | 241M/8.67G [00:02<02:35, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   3%|    | 252M/8.67G [00:02<02:34, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   3%|    | 262M/8.67G [00:02<02:33, 54.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   3%|   | 273M/8.67G [00:02<02:32, 54.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   3%|   | 283M/8.67G [00:03<02:33, 54.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   3%|   | 294M/8.67G [00:03<02:33, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   4%|   | 304M/8.67G [00:03<02:32, 54.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   4%|   | 315M/8.67G [00:03<02:32, 54.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   4%|   | 325M/8.67G [00:03<02:31, 55.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   4%|   | 336M/8.67G [00:04<02:32, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   4%|   | 346M/8.67G [00:04<02:34, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   4%|   | 357M/8.67G [00:04<02:36, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   4%|   | 367M/8.67G [00:04<02:35, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   4%|   | 377M/8.67G [00:04<02:34, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   4%|   | 388M/8.67G [00:05<02:33, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   5%|   | 398M/8.67G [00:05<02:35, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   5%|   | 409M/8.67G [00:05<02:33, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   5%|   | 419M/8.67G [00:05<02:33, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   5%|   | 430M/8.67G [00:05<02:32, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   5%|   | 440M/8.67G [00:06<02:33, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   5%|   | 451M/8.67G [00:06<02:30, 54.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   5%|   | 461M/8.67G [00:06<02:29, 54.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   5%|   | 472M/8.67G [00:06<02:28, 55.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   6%|   | 482M/8.67G [00:06<02:28, 55.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   6%|   | 493M/8.67G [00:06<02:27, 55.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   6%|   | 503M/8.67G [00:07<02:27, 55.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   6%|   | 514M/8.67G [00:07<02:27, 55.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   6%|   | 524M/8.67G [00:07<02:28, 54.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   6%|   | 535M/8.67G [00:07<02:27, 55.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   6%|   | 545M/8.67G [00:07<02:29, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   6%|   | 556M/8.67G [00:08<02:32, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   7%|   | 566M/8.67G [00:08<02:31, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   7%|   | 577M/8.67G [00:08<02:30, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   7%|   | 587M/8.67G [00:08<02:29, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   7%|   | 598M/8.67G [00:08<02:28, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   7%|   | 608M/8.67G [00:09<02:27, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   7%|   | 619M/8.67G [00:09<02:28, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   7%|   | 629M/8.67G [00:09<02:27, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   7%|   | 640M/8.67G [00:09<02:27, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   8%|   | 650M/8.67G [00:09<02:27, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   8%|   | 661M/8.67G [00:10<02:26, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   8%|   | 671M/8.67G [00:10<02:25, 54.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   8%|   | 682M/8.67G [00:10<02:28, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   8%|   | 692M/8.67G [00:10<02:27, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   8%|   | 703M/8.67G [00:10<02:26, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   8%|   | 713M/8.67G [00:11<02:28, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   8%|   | 724M/8.67G [00:11<02:27, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   8%|   | 734M/8.67G [00:11<02:27, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   9%|   | 744M/8.67G [00:11<02:27, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   9%|   | 755M/8.67G [00:11<02:27, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   9%|   | 765M/8.67G [00:11<02:26, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   9%|   | 776M/8.67G [00:12<02:26, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   9%|   | 786M/8.67G [00:12<02:29, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   9%|   | 797M/8.67G [00:12<02:26, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   9%|   | 807M/8.67G [00:12<02:26, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:   9%|   | 818M/8.67G [00:12<02:24, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  10%|   | 828M/8.67G [00:13<02:23, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  10%|   | 839M/8.67G [00:13<02:26, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  10%|   | 849M/8.67G [00:13<02:26, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  10%|   | 860M/8.67G [00:13<02:25, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  10%|   | 870M/8.67G [00:13<02:24, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  10%|   | 881M/8.67G [00:14<02:26, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  10%|   | 891M/8.67G [00:14<02:24, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  10%|   | 902M/8.67G [00:14<02:23, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  11%|   | 912M/8.67G [00:14<02:24, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  11%|   | 923M/8.67G [00:14<02:22, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  11%|   | 933M/8.67G [00:15<02:21, 54.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  11%|   | 944M/8.67G [00:15<02:22, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  11%|   | 954M/8.67G [00:15<02:23, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  11%|   | 965M/8.67G [00:15<02:22, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  11%|   | 975M/8.67G [00:15<02:21, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  11%|   | 986M/8.67G [00:16<02:22, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  11%|   | 996M/8.67G [00:16<02:20, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  12%|  | 1.01G/8.67G [00:16<02:20, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  12%|  | 1.02G/8.67G [00:16<02:20, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  12%|  | 1.03G/8.67G [00:16<02:20, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  12%|  | 1.04G/8.67G [00:17<02:20, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  12%|  | 1.05G/8.67G [00:17<03:04, 41.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  12%|  | 1.06G/8.67G [00:17<02:52, 44.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  12%|  | 1.07G/8.67G [00:17<02:42, 46.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  12%|  | 1.08G/8.67G [00:18<02:35, 48.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  13%|  | 1.09G/8.67G [00:18<02:32, 49.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  13%|  | 1.10G/8.67G [00:18<02:27, 51.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  13%|  | 1.11G/8.67G [00:18<02:25, 52.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  13%|  | 1.12G/8.67G [00:18<02:24, 52.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  13%|  | 1.13G/8.67G [00:18<02:21, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  13%|  | 1.14G/8.67G [00:19<02:20, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  13%|  | 1.15G/8.67G [00:19<02:20, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  13%|  | 1.16G/8.67G [00:19<02:21, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  14%|  | 1.17G/8.67G [00:19<02:20, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  14%|  | 1.18G/8.67G [00:19<02:19, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  14%|  | 1.20G/8.67G [00:20<02:19, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  14%|  | 1.21G/8.67G [00:20<02:19, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  14%|  | 1.22G/8.67G [00:20<02:22, 52.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  14%|  | 1.23G/8.67G [00:20<02:21, 52.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  14%|  | 1.24G/8.67G [00:20<02:19, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  14%|  | 1.25G/8.67G [00:21<02:17, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  15%|  | 1.26G/8.67G [00:21<02:18, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  15%|  | 1.27G/8.67G [00:21<02:16, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  15%|  | 1.28G/8.67G [00:21<02:14, 54.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  15%|  | 1.29G/8.67G [00:21<02:16, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  15%|  | 1.30G/8.67G [00:22<02:15, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  15%|  | 1.31G/8.67G [00:22<02:14, 54.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  15%|  | 1.32G/8.67G [00:22<02:15, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  15%|  | 1.33G/8.67G [00:22<02:14, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  15%|  | 1.34G/8.67G [00:22<02:15, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  16%|  | 1.35G/8.67G [00:23<02:16, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  16%|  | 1.36G/8.67G [00:23<02:15, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  16%|  | 1.37G/8.67G [00:23<02:14, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  16%|  | 1.38G/8.67G [00:23<02:15, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  16%|  | 1.39G/8.67G [00:23<02:15, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  16%|  | 1.41G/8.67G [00:24<02:14, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  16%|  | 1.42G/8.67G [00:24<02:13, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  16%|  | 1.43G/8.67G [00:24<02:13, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  17%|  | 1.44G/8.67G [00:24<02:14, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  17%|  | 1.45G/8.67G [00:24<02:16, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  17%|  | 1.46G/8.67G [00:25<02:15, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  17%|  | 1.47G/8.67G [00:25<02:14, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  17%|  | 1.48G/8.67G [00:25<02:13, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  17%|  | 1.49G/8.67G [00:25<02:15, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  17%|  | 1.50G/8.67G [00:25<02:13, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  17%|  | 1.51G/8.67G [00:26<02:13, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  18%|  | 1.52G/8.67G [00:26<02:13, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  18%|  | 1.53G/8.67G [00:26<02:12, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  18%|  | 1.54G/8.67G [00:26<02:11, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  18%|  | 1.55G/8.67G [00:26<02:12, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  18%|  | 1.56G/8.67G [00:26<02:11, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  18%|  | 1.57G/8.67G [00:27<02:09, 54.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  18%|  | 1.58G/8.67G [00:27<02:08, 55.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  18%|  | 1.59G/8.67G [00:27<02:11, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  19%|  | 1.60G/8.67G [00:27<02:11, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  19%|  | 1.61G/8.67G [00:27<02:10, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  19%|  | 1.63G/8.67G [00:28<02:10, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  19%|  | 1.64G/8.67G [00:28<02:10, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  19%|  | 1.65G/8.67G [00:28<02:09, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  19%|  | 1.66G/8.67G [00:28<02:11, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  19%|  | 1.67G/8.67G [00:28<02:12, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  19%|  | 1.68G/8.67G [00:29<02:10, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  19%|  | 1.69G/8.67G [00:29<02:09, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  20%|  | 1.70G/8.67G [00:29<02:09, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  20%|  | 1.71G/8.67G [00:29<02:08, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  20%|  | 1.72G/8.67G [00:29<02:10, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  20%|  | 1.73G/8.67G [00:30<02:10, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  20%|  | 1.74G/8.67G [00:30<02:09, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  20%|  | 1.75G/8.67G [00:30<02:08, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  20%|  | 1.76G/8.67G [00:30<02:09, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  20%|  | 1.77G/8.67G [00:30<02:08, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  21%|  | 1.78G/8.67G [00:31<02:08, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  21%|  | 1.79G/8.67G [00:31<02:08, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  21%|  | 1.80G/8.67G [00:31<02:07, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  21%|  | 1.81G/8.67G [00:31<02:06, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  21%|  | 1.82G/8.67G [00:31<02:05, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  21%|  | 1.84G/8.67G [00:32<02:05, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  21%|  | 1.85G/8.67G [00:32<02:06, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  21%|  | 1.86G/8.67G [00:32<02:08, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  22%|  | 1.87G/8.67G [00:32<02:06, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  22%|  | 1.88G/8.67G [00:32<02:05, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  22%|  | 1.89G/8.67G [00:33<02:04, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  22%|  | 1.90G/8.67G [00:33<02:04, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  22%|  | 1.91G/8.67G [00:33<02:05, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  22%|  | 1.92G/8.67G [00:33<02:05, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  22%|  | 1.93G/8.67G [00:33<02:04, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  22%|  | 1.94G/8.67G [00:33<02:03, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  23%|  | 1.95G/8.67G [00:34<02:04, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  23%|  | 1.96G/8.67G [00:34<02:04, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  23%|  | 1.97G/8.67G [00:34<02:03, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  23%|  | 1.98G/8.67G [00:34<02:04, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  23%|  | 1.99G/8.67G [00:34<02:02, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  23%|  | 2.00G/8.67G [00:35<02:02, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  23%|  | 2.01G/8.67G [00:35<02:03, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  23%|  | 2.02G/8.67G [00:35<02:04, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  23%|  | 2.03G/8.67G [00:35<02:03, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  24%|  | 2.04G/8.67G [00:35<02:01, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  24%|  | 2.06G/8.67G [00:36<02:02, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  24%|  | 2.07G/8.67G [00:36<02:01, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  24%|  | 2.08G/8.67G [00:36<02:01, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  24%|  | 2.09G/8.67G [00:36<02:02, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  24%|  | 2.10G/8.67G [00:36<02:01, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  24%|  | 2.11G/8.67G [00:37<02:01, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  24%|  | 2.12G/8.67G [00:37<02:00, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  25%|  | 2.13G/8.67G [00:37<02:00, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  25%|  | 2.14G/8.67G [00:37<02:01, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  25%|  | 2.15G/8.67G [00:38<02:38, 41.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  25%|  | 2.16G/8.67G [00:38<02:26, 44.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  25%|  | 2.17G/8.67G [00:38<02:17, 47.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  25%|  | 2.18G/8.67G [00:38<02:14, 48.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  25%|  | 2.19G/8.67G [00:38<02:10, 49.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  25%|  | 2.20G/8.67G [00:39<02:07, 50.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  26%|  | 2.21G/8.67G [00:39<02:04, 51.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  26%|  | 2.22G/8.67G [00:39<02:02, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  26%|  | 2.23G/8.67G [00:39<02:00, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  26%|  | 2.24G/8.67G [00:39<02:02, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  26%|  | 2.25G/8.67G [00:40<02:02, 52.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  26%|  | 2.26G/8.67G [00:40<01:59, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  26%|  | 2.28G/8.67G [00:40<01:59, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  26%|  | 2.29G/8.67G [00:40<01:59, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  26%|  | 2.30G/8.67G [00:40<01:58, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  27%|  | 2.31G/8.67G [00:41<01:57, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  27%|  | 2.32G/8.67G [00:41<01:59, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  27%|  | 2.33G/8.67G [00:41<01:58, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  27%|  | 2.34G/8.67G [00:41<01:57, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  27%|  | 2.35G/8.67G [00:41<02:00, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  27%|  | 2.36G/8.67G [00:41<01:59, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  27%|  | 2.37G/8.67G [00:42<01:58, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  27%|  | 2.38G/8.67G [00:42<01:57, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  28%|  | 2.39G/8.67G [00:42<01:57, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  28%|  | 2.40G/8.67G [00:42<01:57, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  28%|  | 2.41G/8.67G [00:42<01:56, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  28%|  | 2.42G/8.67G [00:43<01:56, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  28%|  | 2.43G/8.67G [00:43<02:00, 51.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  28%|  | 2.44G/8.67G [00:43<01:59, 52.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  28%|  | 2.45G/8.67G [00:43<01:57, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  28%|  | 2.46G/8.67G [00:43<01:56, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  29%|  | 2.47G/8.67G [00:44<02:00, 51.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  29%|  | 2.49G/8.67G [00:44<02:01, 50.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  29%|  | 2.50G/8.67G [00:44<01:57, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  29%|  | 2.51G/8.67G [00:44<01:55, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  29%|  | 2.52G/8.67G [00:44<01:54, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  29%|  | 2.53G/8.67G [00:45<01:54, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  29%|  | 2.54G/8.67G [00:45<01:53, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  29%|  | 2.55G/8.67G [00:45<01:53, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  30%|  | 2.56G/8.67G [00:45<01:52, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  30%|  | 2.57G/8.67G [00:45<01:51, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  30%|  | 2.58G/8.67G [00:46<01:53, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  30%|  | 2.59G/8.67G [00:46<01:52, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  30%|  | 2.60G/8.67G [00:46<01:52, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  30%|  | 2.61G/8.67G [00:46<01:53, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  30%|  | 2.62G/8.67G [00:46<01:52, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  30%|  | 2.63G/8.67G [00:47<01:52, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  30%|  | 2.64G/8.67G [00:47<01:52, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  31%|  | 2.65G/8.67G [00:47<01:53, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  31%|  | 2.66G/8.67G [00:47<01:52, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  31%|  | 2.67G/8.67G [00:47<01:51, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  31%|  | 2.68G/8.67G [00:48<01:51, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  31%|  | 2.69G/8.67G [00:48<01:50, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  31%|  | 2.71G/8.67G [00:48<01:50, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  31%|  | 2.72G/8.67G [00:48<01:51, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  31%|  | 2.73G/8.67G [00:48<01:50, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  32%|  | 2.74G/8.67G [00:49<01:52, 52.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  32%|  | 2.75G/8.67G [00:49<01:53, 52.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  32%|  | 2.76G/8.67G [00:49<01:53, 52.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  32%|  | 2.77G/8.67G [00:49<01:51, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  32%|  | 2.78G/8.67G [00:49<01:51, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  32%|  | 2.79G/8.67G [00:50<01:49, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  32%|  | 2.80G/8.67G [00:50<01:48, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  32%|  | 2.81G/8.67G [00:50<01:49, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  33%|  | 2.82G/8.67G [00:50<01:49, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  33%|  | 2.83G/8.67G [00:50<01:49, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  33%|  | 2.84G/8.67G [00:51<01:48, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  33%|  | 2.85G/8.67G [00:51<01:48, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  33%|  | 2.86G/8.67G [00:51<01:47, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  33%|  | 2.87G/8.67G [00:51<01:49, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  33%|  | 2.88G/8.67G [00:51<01:49, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  33%|  | 2.89G/8.67G [00:52<01:48, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  34%|  | 2.90G/8.67G [00:52<01:48, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  34%|  | 2.92G/8.67G [00:52<01:49, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  34%|  | 2.93G/8.67G [00:52<01:47, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  34%|  | 2.94G/8.67G [00:52<01:46, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  34%|  | 2.95G/8.67G [00:53<01:47, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  34%|  | 2.96G/8.67G [00:53<01:46, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  34%|  | 2.97G/8.67G [00:53<01:46, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  34%|  | 2.98G/8.67G [00:53<01:46, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  34%|  | 2.99G/8.67G [00:53<01:46, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  35%|  | 3.00G/8.67G [00:53<01:45, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  35%|  | 3.01G/8.67G [00:54<01:45, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  35%|  | 3.02G/8.67G [00:54<01:45, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  35%|  | 3.03G/8.67G [00:54<01:44, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  35%|  | 3.04G/8.67G [00:54<01:44, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  35%|  | 3.05G/8.67G [00:54<01:44, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  35%|  | 3.06G/8.67G [00:55<01:45, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  35%|  | 3.07G/8.67G [00:55<01:44, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  36%|  | 3.08G/8.67G [00:55<01:45, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  36%|  | 3.09G/8.67G [00:55<01:45, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  36%|  | 3.10G/8.67G [00:56<02:15, 41.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  36%|  | 3.11G/8.67G [00:56<02:05, 44.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  36%|  | 3.12G/8.67G [00:56<01:59, 46.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  36%|  | 3.14G/8.67G [00:56<01:53, 48.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  36%|  | 3.15G/8.67G [00:56<01:50, 50.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  36%|  | 3.16G/8.67G [00:57<01:47, 51.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  37%|  | 3.17G/8.67G [00:57<01:45, 52.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  37%|  | 3.18G/8.67G [00:57<01:43, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  37%|  | 3.19G/8.67G [00:57<01:43, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  37%|  | 3.20G/8.67G [00:57<01:43, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  37%|  | 3.21G/8.67G [00:58<01:42, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  37%|  | 3.22G/8.67G [00:58<01:41, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  37%|  | 3.23G/8.67G [00:58<01:41, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  37%|  | 3.24G/8.67G [00:58<01:41, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  38%| | 3.25G/8.67G [00:58<01:41, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  38%| | 3.26G/8.67G [00:59<01:41, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  38%| | 3.27G/8.67G [00:59<01:40, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  38%| | 3.28G/8.67G [00:59<01:40, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  38%| | 3.29G/8.67G [00:59<01:41, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  38%| | 3.30G/8.67G [00:59<01:41, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  38%| | 3.31G/8.67G [01:00<01:39, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  38%| | 3.32G/8.67G [01:00<01:38, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  38%| | 3.33G/8.67G [01:00<01:38, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  39%| | 3.34G/8.67G [01:00<01:38, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  39%| | 3.36G/8.67G [01:00<01:39, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  39%| | 3.37G/8.67G [01:01<01:39, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  39%| | 3.38G/8.67G [01:01<01:37, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  39%| | 3.39G/8.67G [01:01<01:36, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  39%| | 3.40G/8.67G [01:01<01:37, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  39%| | 3.41G/8.67G [01:01<01:37, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  39%| | 3.42G/8.67G [01:01<01:36, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  40%| | 3.43G/8.67G [01:02<01:37, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  40%| | 3.44G/8.67G [01:02<01:36, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  40%| | 3.45G/8.67G [01:02<01:36, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  40%| | 3.46G/8.67G [01:02<01:36, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  40%| | 3.47G/8.67G [01:02<01:37, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  40%| | 3.48G/8.67G [01:03<01:36, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  40%| | 3.49G/8.67G [01:03<01:36, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  40%| | 3.50G/8.67G [01:03<01:36, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  41%| | 3.51G/8.67G [01:03<01:35, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  41%| | 3.52G/8.67G [01:03<01:35, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  41%| | 3.53G/8.67G [01:04<01:37, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  41%| | 3.54G/8.67G [01:04<01:35, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  41%| | 3.55G/8.67G [01:04<01:34, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  41%| | 3.57G/8.67G [01:04<01:33, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  41%| | 3.58G/8.67G [01:04<01:33, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  41%| | 3.59G/8.67G [01:05<01:34, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  41%| | 3.60G/8.67G [01:05<01:34, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  42%| | 3.61G/8.67G [01:05<01:33, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  42%| | 3.62G/8.67G [01:05<01:33, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  42%| | 3.63G/8.67G [01:05<01:34, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  42%| | 3.64G/8.67G [01:06<01:33, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  42%| | 3.65G/8.67G [01:06<01:33, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  42%| | 3.66G/8.67G [01:06<01:33, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  42%| | 3.67G/8.67G [01:06<01:32, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  42%| | 3.68G/8.67G [01:06<01:32, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  43%| | 3.69G/8.67G [01:07<01:33, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  43%| | 3.70G/8.67G [01:07<01:33, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  43%| | 3.71G/8.67G [01:07<01:32, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  43%| | 3.72G/8.67G [01:07<01:32, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  43%| | 3.73G/8.67G [01:07<01:32, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  43%| | 3.74G/8.67G [01:08<01:30, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  43%| | 3.75G/8.67G [01:08<01:32, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  43%| | 3.76G/8.67G [01:08<01:32, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  44%| | 3.77G/8.67G [01:08<01:30, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  44%| | 3.79G/8.67G [01:08<01:30, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  44%| | 3.80G/8.67G [01:09<01:31, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  44%| | 3.81G/8.67G [01:09<01:30, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  44%| | 3.82G/8.67G [01:09<01:29, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  44%| | 3.83G/8.67G [01:09<01:30, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  44%| | 3.84G/8.67G [01:09<01:29, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  44%| | 3.85G/8.67G [01:09<01:28, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  45%| | 3.86G/8.67G [01:10<01:29, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  45%| | 3.87G/8.67G [01:10<01:28, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  45%| | 3.88G/8.67G [01:10<01:28, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  45%| | 3.89G/8.67G [01:10<01:28, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  45%| | 3.90G/8.67G [01:10<01:28, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  45%| | 3.91G/8.67G [01:11<01:27, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  45%| | 3.92G/8.67G [01:11<01:27, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  45%| | 3.93G/8.67G [01:11<01:29, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  45%| | 3.94G/8.67G [01:11<01:28, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  46%| | 3.95G/8.67G [01:11<01:27, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  46%| | 3.96G/8.67G [01:12<01:27, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  46%| | 3.97G/8.67G [01:12<01:26, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  46%| | 3.98G/8.67G [01:12<01:27, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  46%| | 4.00G/8.67G [01:12<01:26, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  46%| | 4.01G/8.67G [01:12<01:26, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  46%| | 4.02G/8.67G [01:13<01:27, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  46%| | 4.03G/8.67G [01:13<01:26, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  47%| | 4.04G/8.67G [01:13<01:53, 40.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  47%| | 4.05G/8.67G [01:13<01:45, 43.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  47%| | 4.06G/8.67G [01:14<01:40, 46.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  47%| | 4.07G/8.67G [01:14<01:34, 48.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  47%| | 4.08G/8.67G [01:14<01:31, 50.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  47%| | 4.09G/8.67G [01:14<01:29, 51.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  47%| | 4.10G/8.67G [01:14<01:27, 52.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  47%| | 4.11G/8.67G [01:15<01:26, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  48%| | 4.12G/8.67G [01:15<01:26, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  48%| | 4.13G/8.67G [01:15<01:25, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  48%| | 4.14G/8.67G [01:15<01:24, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  48%| | 4.15G/8.67G [01:15<01:24, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  48%| | 4.16G/8.67G [01:16<01:25, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  48%| | 4.17G/8.67G [01:16<01:24, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  48%| | 4.18G/8.67G [01:16<01:24, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  48%| | 4.19G/8.67G [01:16<01:24, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  49%| | 4.20G/8.67G [01:16<01:23, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  49%| | 4.22G/8.67G [01:17<01:22, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  49%| | 4.23G/8.67G [01:17<01:22, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  49%| | 4.24G/8.67G [01:17<01:21, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  49%| | 4.25G/8.67G [01:17<01:21, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  49%| | 4.26G/8.67G [01:17<01:21, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  49%| | 4.27G/8.67G [01:17<01:21, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  49%| | 4.28G/8.67G [01:18<01:20, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  49%| | 4.29G/8.67G [01:18<01:20, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  50%| | 4.30G/8.67G [01:18<01:20, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  50%| | 4.31G/8.67G [01:18<01:20, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  50%| | 4.32G/8.67G [01:18<01:22, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  50%| | 4.33G/8.67G [01:19<01:21, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  50%| | 4.34G/8.67G [01:19<01:20, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  50%| | 4.35G/8.67G [01:19<01:19, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  50%| | 4.36G/8.67G [01:19<01:20, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  50%| | 4.37G/8.67G [01:19<01:19, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  51%| | 4.38G/8.67G [01:20<01:19, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  51%| | 4.39G/8.67G [01:20<01:19, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  51%| | 4.40G/8.67G [01:20<01:19, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  51%| | 4.41G/8.67G [01:20<01:19, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  51%| | 4.42G/8.67G [01:20<01:20, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  51%| | 4.44G/8.67G [01:21<01:22, 51.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  51%| | 4.45G/8.67G [01:21<01:19, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  51%| | 4.46G/8.67G [01:21<01:19, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  52%| | 4.47G/8.67G [01:21<01:18, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  52%| | 4.48G/8.67G [01:21<01:18, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  52%| | 4.49G/8.67G [01:22<01:17, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  52%| | 4.50G/8.67G [01:22<01:19, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  52%| | 4.51G/8.67G [01:22<01:18, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  52%| | 4.52G/8.67G [01:22<01:18, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  52%| | 4.53G/8.67G [01:22<01:18, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  52%| | 4.54G/8.67G [01:23<01:19, 51.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  53%| | 4.55G/8.67G [01:23<01:17, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  53%| | 4.56G/8.67G [01:23<01:16, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  53%| | 4.57G/8.67G [01:23<01:17, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  53%| | 4.58G/8.67G [01:23<01:15, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  53%| | 4.59G/8.67G [01:24<01:14, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  53%| | 4.60G/8.67G [01:24<01:15, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  53%| | 4.61G/8.67G [01:24<01:15, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  53%| | 4.62G/8.67G [01:24<01:14, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  53%| | 4.63G/8.67G [01:24<01:14, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  54%| | 4.65G/8.67G [01:25<01:14, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  54%| | 4.66G/8.67G [01:25<01:14, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  54%| | 4.67G/8.67G [01:25<01:15, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  54%| | 4.68G/8.67G [01:25<01:14, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  54%| | 4.69G/8.67G [01:25<01:14, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  54%| | 4.70G/8.67G [01:26<01:13, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  54%| | 4.71G/8.67G [01:26<01:13, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  54%| | 4.72G/8.67G [01:26<01:13, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  55%| | 4.73G/8.67G [01:26<01:14, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  55%| | 4.74G/8.67G [01:26<01:12, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  55%| | 4.75G/8.67G [01:27<01:12, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  55%| | 4.76G/8.67G [01:27<01:13, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  55%| | 4.77G/8.67G [01:27<01:11, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  55%| | 4.78G/8.67G [01:27<01:11, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  55%| | 4.79G/8.67G [01:27<01:11, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  55%| | 4.80G/8.67G [01:27<01:11, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  56%| | 4.81G/8.67G [01:28<01:10, 54.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  56%| | 4.82G/8.67G [01:28<01:10, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  56%| | 4.83G/8.67G [01:28<01:11, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  56%| | 4.84G/8.67G [01:28<01:10, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  56%| | 4.85G/8.67G [01:28<01:10, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  56%| | 4.87G/8.67G [01:29<01:10, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  56%| | 4.88G/8.67G [01:29<01:09, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  56%| | 4.89G/8.67G [01:29<01:10, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  56%| | 4.90G/8.67G [01:29<01:10, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  57%| | 4.91G/8.67G [01:29<01:09, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  57%| | 4.92G/8.67G [01:30<01:10, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  57%| | 4.93G/8.67G [01:30<01:09, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  57%| | 4.94G/8.67G [01:30<01:09, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  57%| | 4.95G/8.67G [01:30<01:10, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  57%| | 4.96G/8.67G [01:30<01:09, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  57%| | 4.97G/8.67G [01:31<01:08, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  57%| | 4.98G/8.67G [01:31<01:08, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  58%| | 4.99G/8.67G [01:31<01:08, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  58%| | 5.00G/8.67G [01:31<01:07, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  58%| | 5.01G/8.67G [01:31<01:07, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  58%| | 5.02G/8.67G [01:32<01:08, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  58%| | 5.03G/8.67G [01:32<01:07, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  58%| | 5.04G/8.67G [01:32<01:27, 41.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  58%| | 5.05G/8.67G [01:32<01:21, 44.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  58%| | 5.06G/8.67G [01:33<01:17, 46.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  59%| | 5.08G/8.67G [01:33<01:13, 49.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  59%| | 5.09G/8.67G [01:33<01:11, 50.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  59%| | 5.10G/8.67G [01:33<01:09, 51.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  59%| | 5.11G/8.67G [01:33<01:08, 52.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  59%| | 5.12G/8.67G [01:34<01:07, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  59%| | 5.13G/8.67G [01:34<01:07, 52.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  59%| | 5.14G/8.67G [01:34<01:06, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  59%| | 5.15G/8.67G [01:34<01:05, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  60%| | 5.16G/8.67G [01:34<01:06, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  60%| | 5.17G/8.67G [01:34<01:04, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  60%| | 5.18G/8.67G [01:35<01:05, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  60%| | 5.19G/8.67G [01:35<01:05, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  60%| | 5.20G/8.67G [01:35<01:04, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  60%| | 5.21G/8.67G [01:35<01:04, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  60%| | 5.22G/8.67G [01:35<01:04, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  60%| | 5.23G/8.67G [01:36<01:04, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  60%| | 5.24G/8.67G [01:36<01:03, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  61%| | 5.25G/8.67G [01:36<01:03, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  61%| | 5.26G/8.67G [01:36<01:02, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  61%| | 5.27G/8.67G [01:36<01:02, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  61%| | 5.28G/8.67G [01:37<01:04, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  61%| | 5.30G/8.67G [01:37<01:04, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  61%| | 5.31G/8.67G [01:37<01:03, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  61%| | 5.32G/8.67G [01:37<01:02, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  61%| | 5.33G/8.67G [01:37<01:02, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  62%| | 5.34G/8.67G [01:38<01:01, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  62%| | 5.35G/8.67G [01:38<01:01, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  62%| | 5.36G/8.67G [01:38<01:01, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  62%| | 5.37G/8.67G [01:38<01:01, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  62%| | 5.38G/8.67G [01:38<01:00, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  62%| | 5.39G/8.67G [01:39<01:00, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  62%| | 5.40G/8.67G [01:39<01:00, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  62%| | 5.41G/8.67G [01:39<01:00, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  63%| | 5.42G/8.67G [01:39<01:00, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  63%| | 5.43G/8.67G [01:39<01:00, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  63%| | 5.44G/8.67G [01:40<00:59, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  63%| | 5.45G/8.67G [01:40<01:00, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  63%| | 5.46G/8.67G [01:40<00:59, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  63%| | 5.47G/8.67G [01:40<00:59, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  63%| | 5.48G/8.67G [01:40<00:59, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  63%| | 5.49G/8.67G [01:41<00:59, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  64%| | 5.51G/8.67G [01:41<00:58, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  64%| | 5.52G/8.67G [01:41<00:58, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  64%| | 5.53G/8.67G [01:41<00:58, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  64%| | 5.54G/8.67G [01:41<00:57, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  64%| | 5.55G/8.67G [01:42<00:57, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  64%| | 5.56G/8.67G [01:42<00:57, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  64%| | 5.57G/8.67G [01:42<00:56, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  64%| | 5.58G/8.67G [01:42<00:57, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  64%| | 5.59G/8.67G [01:42<00:57, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  65%| | 5.60G/8.67G [01:42<00:56, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  65%| | 5.61G/8.67G [01:43<00:56, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  65%| | 5.62G/8.67G [01:43<00:56, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  65%| | 5.63G/8.67G [01:43<00:56, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  65%| | 5.64G/8.67G [01:43<00:56, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  65%| | 5.65G/8.67G [01:43<00:56, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  65%| | 5.66G/8.67G [01:44<00:56, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  65%| | 5.67G/8.67G [01:44<00:54, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  66%| | 5.68G/8.67G [01:44<00:55, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  66%| | 5.69G/8.67G [01:44<00:55, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  66%| | 5.70G/8.67G [01:44<00:55, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  66%| | 5.71G/8.67G [01:45<00:54, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  66%| | 5.73G/8.67G [01:45<00:54, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  66%| | 5.74G/8.67G [01:45<00:59, 49.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  66%| | 5.75G/8.67G [01:45<00:57, 50.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  66%| | 5.76G/8.67G [01:45<00:56, 51.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  67%| | 5.77G/8.67G [01:46<00:55, 52.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  67%| | 5.78G/8.67G [01:46<00:54, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  67%| | 5.79G/8.67G [01:46<00:54, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  67%| | 5.80G/8.67G [01:46<00:53, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  67%| | 5.81G/8.67G [01:46<00:53, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  67%| | 5.82G/8.67G [01:47<00:53, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  67%| | 5.83G/8.67G [01:47<00:52, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  67%| | 5.84G/8.67G [01:47<00:52, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  68%| | 5.85G/8.67G [01:47<01:08, 41.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  68%| | 5.86G/8.67G [01:48<01:03, 44.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  68%| | 5.87G/8.67G [01:48<00:59, 46.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  68%| | 5.88G/8.67G [01:48<00:57, 48.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  68%| | 5.89G/8.67G [01:48<00:55, 49.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  68%| | 5.90G/8.67G [01:48<00:56, 49.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  68%| | 5.91G/8.67G [01:49<00:54, 50.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  68%| | 5.92G/8.67G [01:49<00:53, 51.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  68%| | 5.93G/8.67G [01:49<00:52, 52.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  69%| | 5.95G/8.67G [01:49<00:51, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  69%| | 5.96G/8.67G [01:49<00:51, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  69%| | 5.97G/8.67G [01:50<00:51, 52.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  69%| | 5.98G/8.67G [01:50<00:51, 52.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  69%| | 5.99G/8.67G [01:50<00:50, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  69%| | 6.00G/8.67G [01:50<00:50, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  69%| | 6.01G/8.67G [01:50<00:49, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  69%| | 6.02G/8.67G [01:51<00:49, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  70%| | 6.03G/8.67G [01:51<00:49, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  70%| | 6.04G/8.67G [01:51<00:48, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  70%| | 6.05G/8.67G [01:51<00:48, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  70%| | 6.06G/8.67G [01:51<00:48, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  70%| | 6.07G/8.67G [01:52<00:49, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  70%| | 6.08G/8.67G [01:52<00:48, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  70%| | 6.09G/8.67G [01:52<00:47, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  70%| | 6.10G/8.67G [01:52<00:47, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  71%| | 6.11G/8.67G [01:52<00:47, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  71%| | 6.12G/8.67G [01:53<00:47, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  71%| | 6.13G/8.67G [01:53<00:46, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  71%|| 6.14G/8.67G [01:53<00:46, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  71%|| 6.16G/8.67G [01:53<00:46, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  71%|| 6.17G/8.67G [01:53<00:47, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  71%|| 6.18G/8.67G [01:54<00:48, 51.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  71%|| 6.19G/8.67G [01:54<00:47, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  71%|| 6.20G/8.67G [01:54<00:46, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  72%|| 6.21G/8.67G [01:54<00:46, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  72%|| 6.22G/8.67G [01:54<00:46, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  72%|| 6.23G/8.67G [01:55<00:46, 52.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  72%|| 6.24G/8.67G [01:55<00:46, 52.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  72%|| 6.25G/8.67G [01:55<00:45, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  72%|| 6.26G/8.67G [01:55<00:49, 48.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  72%|| 6.27G/8.67G [01:55<00:48, 49.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  72%|| 6.28G/8.67G [01:56<00:46, 51.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  73%|| 6.29G/8.67G [01:56<00:45, 52.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  73%|| 6.30G/8.67G [01:56<00:46, 51.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  73%|| 6.31G/8.67G [01:56<00:45, 51.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  73%|| 6.32G/8.67G [01:56<00:44, 52.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  73%|| 6.33G/8.67G [01:57<00:44, 52.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  73%|| 6.34G/8.67G [01:57<00:44, 52.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  73%|| 6.35G/8.67G [01:57<00:43, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  73%|| 6.36G/8.67G [01:57<00:43, 52.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  74%|| 6.38G/8.67G [01:57<00:43, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  74%|| 6.39G/8.67G [01:58<00:42, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  74%|| 6.40G/8.67G [01:58<00:42, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  74%|| 6.41G/8.67G [01:58<00:43, 51.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  74%|| 6.42G/8.67G [01:58<00:42, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  74%|| 6.43G/8.67G [01:58<00:42, 52.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  74%|| 6.44G/8.67G [01:59<00:42, 52.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  74%|| 6.45G/8.67G [01:59<00:43, 51.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  75%|| 6.46G/8.67G [01:59<00:41, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  75%|| 6.47G/8.67G [01:59<00:41, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  75%|| 6.48G/8.67G [01:59<00:40, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  75%|| 6.49G/8.67G [02:00<00:40, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  75%|| 6.50G/8.67G [02:00<00:40, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  75%|| 6.51G/8.67G [02:00<00:40, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  75%|| 6.52G/8.67G [02:00<00:40, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  75%|| 6.53G/8.67G [02:00<00:39, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  75%|| 6.54G/8.67G [02:01<00:39, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  76%|| 6.55G/8.67G [02:01<00:39, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  76%|| 6.56G/8.67G [02:01<00:39, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  76%|| 6.57G/8.67G [02:01<00:38, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  76%|| 6.59G/8.67G [02:01<00:38, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  76%|| 6.60G/8.67G [02:01<00:37, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  76%|| 6.61G/8.67G [02:02<00:37, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  76%|| 6.62G/8.67G [02:02<00:37, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  76%|| 6.63G/8.67G [02:02<00:38, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  77%|| 6.64G/8.67G [02:02<00:38, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  77%|| 6.65G/8.67G [02:02<00:37, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  77%|| 6.66G/8.67G [02:03<00:37, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  77%|| 6.67G/8.67G [02:03<00:36, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  77%|| 6.68G/8.67G [02:03<00:36, 55.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  77%|| 6.69G/8.67G [02:03<00:36, 54.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  77%|| 6.70G/8.67G [02:03<00:36, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  77%|| 6.71G/8.67G [02:04<00:36, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  78%|| 6.72G/8.67G [02:04<00:36, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  78%|| 6.73G/8.67G [02:04<00:36, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  78%|| 6.74G/8.67G [02:04<00:36, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  78%|| 6.75G/8.67G [02:04<00:35, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  78%|| 6.76G/8.67G [02:05<00:35, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  78%|| 6.77G/8.67G [02:05<00:35, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  78%|| 6.78G/8.67G [02:05<00:34, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  78%|| 6.79G/8.67G [02:05<00:34, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  79%|| 6.81G/8.67G [02:05<00:34, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  79%|| 6.82G/8.67G [02:06<00:34, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  79%|| 6.83G/8.67G [02:06<00:34, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  79%|| 6.84G/8.67G [02:06<00:34, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  79%|| 6.85G/8.67G [02:06<00:44, 40.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  79%|| 6.86G/8.67G [02:07<00:40, 44.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  79%|| 6.87G/8.67G [02:07<00:38, 46.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  79%|| 6.88G/8.67G [02:07<00:37, 48.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  79%|| 6.89G/8.67G [02:07<00:35, 49.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  80%|| 6.90G/8.67G [02:07<00:34, 51.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  80%|| 6.91G/8.67G [02:08<00:34, 51.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  80%|| 6.92G/8.67G [02:08<00:33, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  80%|| 6.93G/8.67G [02:08<00:32, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  80%|| 6.94G/8.67G [02:08<00:32, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  80%|| 6.95G/8.67G [02:08<00:32, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  80%|| 6.96G/8.67G [02:08<00:31, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  80%|| 6.97G/8.67G [02:09<00:31, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  81%|| 6.98G/8.67G [02:09<00:30, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  81%|| 6.99G/8.67G [02:09<00:30, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  81%|| 7.00G/8.67G [02:09<00:30, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  81%|| 7.01G/8.67G [02:09<00:30, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  81%|| 7.03G/8.67G [02:10<00:30, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  81%|| 7.04G/8.67G [02:10<00:30, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  81%|| 7.05G/8.67G [02:10<00:30, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  81%|| 7.06G/8.67G [02:10<00:29, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  82%|| 7.07G/8.67G [02:10<00:29, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  82%|| 7.08G/8.67G [02:11<00:29, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  82%|| 7.09G/8.67G [02:11<00:29, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  82%|| 7.10G/8.67G [02:11<00:29, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  82%|| 7.11G/8.67G [02:11<00:28, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  82%|| 7.12G/8.67G [02:11<00:28, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  82%|| 7.13G/8.67G [02:12<00:28, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  82%|| 7.14G/8.67G [02:12<00:28, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  83%|| 7.15G/8.67G [02:12<00:27, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  83%|| 7.16G/8.67G [02:12<00:27, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  83%|| 7.17G/8.67G [02:12<00:27, 54.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  83%|| 7.18G/8.67G [02:13<00:27, 54.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  83%|| 7.19G/8.67G [02:13<00:27, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  83%|| 7.20G/8.67G [02:13<00:27, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  83%|| 7.21G/8.67G [02:13<00:26, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  83%|| 7.22G/8.67G [02:13<00:26, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  83%|| 7.24G/8.67G [02:14<00:26, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  84%|| 7.25G/8.67G [02:14<00:26, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  84%|| 7.26G/8.67G [02:14<00:26, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  84%|| 7.27G/8.67G [02:14<00:26, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  84%|| 7.28G/8.67G [02:14<00:26, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  84%|| 7.29G/8.67G [02:15<00:25, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  84%|| 7.30G/8.67G [02:15<00:25, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  84%|| 7.31G/8.67G [02:15<00:25, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  84%|| 7.32G/8.67G [02:15<00:25, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  85%|| 7.33G/8.67G [02:15<00:24, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  85%|| 7.34G/8.67G [02:15<00:24, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  85%|| 7.35G/8.67G [02:16<00:24, 54.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  85%|| 7.36G/8.67G [02:16<00:24, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  85%|| 7.37G/8.67G [02:16<00:24, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  85%|| 7.38G/8.67G [02:16<00:23, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  85%|| 7.39G/8.67G [02:16<00:23, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  85%|| 7.40G/8.67G [02:17<00:23, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  86%|| 7.41G/8.67G [02:17<00:23, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  86%|| 7.42G/8.67G [02:17<00:23, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  86%|| 7.43G/8.67G [02:17<00:22, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  86%|| 7.44G/8.67G [02:17<00:22, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  86%|| 7.46G/8.67G [02:18<00:22, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  86%|| 7.47G/8.67G [02:18<00:22, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  86%|| 7.48G/8.67G [02:18<00:22, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  86%|| 7.49G/8.67G [02:18<00:21, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  86%|| 7.50G/8.67G [02:18<00:21, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  87%|| 7.51G/8.67G [02:19<00:21, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  87%|| 7.52G/8.67G [02:19<00:21, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  87%|| 7.53G/8.67G [02:19<00:20, 54.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  87%|| 7.54G/8.67G [02:19<00:20, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  87%|| 7.55G/8.67G [02:19<00:20, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  87%|| 7.56G/8.67G [02:20<00:26, 41.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  87%|| 7.57G/8.67G [02:20<00:24, 44.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  87%|| 7.58G/8.67G [02:20<00:23, 46.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  88%|| 7.59G/8.67G [02:20<00:22, 48.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  88%|| 7.60G/8.67G [02:21<00:21, 50.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  88%|| 7.61G/8.67G [02:21<00:20, 51.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  88%|| 7.62G/8.67G [02:21<00:19, 52.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  88%|| 7.63G/8.67G [02:21<00:19, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  88%|| 7.64G/8.67G [02:21<00:19, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  88%|| 7.65G/8.67G [02:22<00:18, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  88%|| 7.67G/8.67G [02:22<00:18, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  89%|| 7.68G/8.67G [02:22<00:18, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  89%|| 7.69G/8.67G [02:22<00:18, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  89%|| 7.70G/8.67G [02:22<00:18, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  89%|| 7.71G/8.67G [02:23<00:17, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  89%|| 7.72G/8.67G [02:23<00:17, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  89%|| 7.73G/8.67G [02:23<00:17, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  89%|| 7.74G/8.67G [02:23<00:17, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  89%|| 7.75G/8.67G [02:23<00:17, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  90%|| 7.76G/8.67G [02:23<00:16, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  90%|| 7.77G/8.67G [02:24<00:16, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  90%|| 7.78G/8.67G [02:24<00:16, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  90%|| 7.79G/8.67G [02:24<00:16, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  90%|| 7.80G/8.67G [02:24<00:16, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  90%|| 7.81G/8.67G [02:24<00:15, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  90%|| 7.82G/8.67G [02:25<00:15, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  90%|| 7.83G/8.67G [02:25<00:15, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  90%|| 7.84G/8.67G [02:25<00:15, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  91%|| 7.85G/8.67G [02:25<00:15, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  91%|| 7.86G/8.67G [02:25<00:15, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  91%|| 7.87G/8.67G [02:26<00:14, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  91%|| 7.89G/8.67G [02:26<00:14, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  91%|| 7.90G/8.67G [02:26<00:14, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  91%|| 7.91G/8.67G [02:26<00:14, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  91%|| 7.92G/8.67G [02:26<00:13, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  91%|| 7.93G/8.67G [02:27<00:13, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  92%|| 7.94G/8.67G [02:27<00:13, 54.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  92%|| 7.95G/8.67G [02:27<00:13, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  92%|| 7.96G/8.67G [02:27<00:13, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  92%|| 7.97G/8.67G [02:27<00:13, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  92%|| 7.98G/8.67G [02:28<00:12, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  92%|| 7.99G/8.67G [02:28<00:12, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  92%|| 8.00G/8.67G [02:28<00:12, 54.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  92%|| 8.01G/8.67G [02:28<00:12, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  93%|| 8.02G/8.67G [02:28<00:12, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  93%|| 8.03G/8.67G [02:29<00:11, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  93%|| 8.04G/8.67G [02:29<00:11, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  93%|| 8.05G/8.67G [02:29<00:11, 54.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  93%|| 8.06G/8.67G [02:29<00:11, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  93%|| 8.07G/8.67G [02:29<00:11, 53.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  93%|| 8.08G/8.67G [02:30<00:10, 54.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  93%|| 8.10G/8.67G [02:30<00:10, 54.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  94%|| 8.11G/8.67G [02:30<00:10, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  94%|| 8.12G/8.67G [02:30<00:10, 54.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  94%|| 8.13G/8.67G [02:30<00:09, 54.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  94%|| 8.14G/8.67G [02:30<00:09, 54.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  94%|| 8.15G/8.67G [02:31<00:09, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  94%|| 8.16G/8.67G [02:31<00:09, 54.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  94%|| 8.17G/8.67G [02:31<00:09, 54.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  94%|| 8.18G/8.67G [02:31<00:09, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  94%|| 8.19G/8.67G [02:31<00:09, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  95%|| 8.20G/8.67G [02:32<00:08, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  95%|| 8.21G/8.67G [02:32<00:08, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  95%|| 8.22G/8.67G [02:32<00:10, 40.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  95%|| 8.23G/8.67G [02:32<00:09, 43.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  95%|| 8.24G/8.67G [02:33<00:09, 45.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  95%|| 8.25G/8.67G [02:33<00:08, 47.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  95%|| 8.26G/8.67G [02:33<00:08, 49.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  95%|| 8.27G/8.67G [02:33<00:07, 50.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  96%|| 8.28G/8.67G [02:33<00:07, 50.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  96%|| 8.29G/8.67G [02:34<00:07, 50.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  96%|| 8.30G/8.67G [02:34<00:07, 51.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  96%|| 8.32G/8.67G [02:34<00:06, 51.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  96%|| 8.33G/8.67G [02:34<00:06, 52.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  96%|| 8.34G/8.67G [02:34<00:06, 52.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  96%|| 8.35G/8.67G [02:35<00:06, 51.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  96%|| 8.36G/8.67G [02:35<00:06, 51.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  97%|| 8.37G/8.67G [02:35<00:05, 52.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  97%|| 8.38G/8.67G [02:35<00:05, 53.2MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  97%|| 8.39G/8.67G [02:36<00:05, 49.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  97%|| 8.40G/8.67G [02:36<00:05, 49.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  97%|| 8.41G/8.67G [02:36<00:05, 51.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  97%|| 8.42G/8.67G [02:36<00:04, 52.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  97%|| 8.43G/8.67G [02:36<00:04, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  97%|| 8.44G/8.67G [02:36<00:04, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  98%|| 8.45G/8.67G [02:37<00:04, 52.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  98%|| 8.46G/8.67G [02:37<00:03, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  98%|| 8.47G/8.67G [02:37<00:03, 52.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  98%|| 8.48G/8.67G [02:37<00:03, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  98%|| 8.49G/8.67G [02:37<00:03, 52.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  98%|| 8.50G/8.67G [02:38<00:03, 51.8MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  98%|| 8.51G/8.67G [02:38<00:02, 52.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  98%|| 8.52G/8.67G [02:38<00:02, 53.0MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  98%|| 8.54G/8.67G [02:38<00:02, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  99%|| 8.55G/8.67G [02:38<00:02, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  99%|| 8.56G/8.67G [02:39<00:02, 53.6MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  99%|| 8.57G/8.67G [02:39<00:01, 52.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  99%|| 8.58G/8.67G [02:39<00:01, 53.3MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  99%|| 8.59G/8.67G [02:39<00:01, 53.5MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  99%|| 8.60G/8.67G [02:39<00:01, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  99%|| 8.61G/8.67G [02:40<00:01, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors:  99%|| 8.62G/8.67G [02:40<00:00, 53.9MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors: 100%|| 8.63G/8.67G [02:40<00:00, 53.4MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors: 100%|| 8.64G/8.67G [02:40<00:00, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors: 100%|| 8.65G/8.67G [02:40<00:00, 53.7MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors: 100%|| 8.66G/8.67G [02:41<00:00, 53.1MB/s]\u001b[A\n",
      "model-00001-of-000002.safetensors: 100%|| 8.67G/8.67G [02:41<00:00, 53.0MB/s]\u001b[A\n",
      "Downloading shards:  50%|            | 1/2 [02:41<02:41, 161.34s/it]\n",
      "model-00002-of-000002.safetensors:   0%|            | 0.00/7.39G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   0%|   | 10.5M/7.39G [00:00<02:18, 53.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   0%|   | 21.0M/7.39G [00:00<02:40, 45.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   0%|   | 31.5M/7.39G [00:00<02:36, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   1%|   | 41.9M/7.39G [00:00<02:35, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   1%|   | 52.4M/7.39G [00:01<02:33, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   1%|   | 62.9M/7.39G [00:01<02:31, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   1%|   | 73.4M/7.39G [00:01<02:31, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   1%|   | 83.9M/7.39G [00:01<02:29, 49.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   1%|   | 94.4M/7.39G [00:01<02:28, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   1%|    | 105M/7.39G [00:02<02:28, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   2%|    | 115M/7.39G [00:02<02:28, 49.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   2%|    | 126M/7.39G [00:02<02:27, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   2%|    | 136M/7.39G [00:02<02:28, 48.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   2%|    | 147M/7.39G [00:03<02:28, 48.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   2%|    | 157M/7.39G [00:03<02:26, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   2%|    | 168M/7.39G [00:03<02:28, 48.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   2%|    | 178M/7.39G [00:03<02:28, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   3%|    | 189M/7.39G [00:03<02:28, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   3%|    | 199M/7.39G [00:04<02:29, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   3%|    | 210M/7.39G [00:04<02:28, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   3%|    | 220M/7.39G [00:04<02:27, 48.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   3%|    | 231M/7.39G [00:04<02:26, 48.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   3%|   | 241M/7.39G [00:04<02:25, 49.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   3%|   | 252M/7.39G [00:05<02:26, 48.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   4%|   | 262M/7.39G [00:05<02:24, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   4%|   | 273M/7.39G [00:05<02:25, 48.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   4%|   | 283M/7.39G [00:05<02:25, 48.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   4%|   | 294M/7.39G [00:06<02:25, 48.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   4%|   | 304M/7.39G [00:06<02:25, 48.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   4%|   | 315M/7.39G [00:06<02:23, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   4%|   | 325M/7.39G [00:06<02:23, 49.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   5%|   | 336M/7.39G [00:06<02:23, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   5%|   | 346M/7.39G [00:07<02:24, 48.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   5%|   | 357M/7.39G [00:07<02:23, 48.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   5%|   | 367M/7.39G [00:07<02:22, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   5%|   | 377M/7.39G [00:07<02:25, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   5%|   | 388M/7.39G [00:07<02:25, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   5%|   | 398M/7.39G [00:08<02:24, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   6%|   | 409M/7.39G [00:08<02:22, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   6%|   | 419M/7.39G [00:08<02:21, 49.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   6%|   | 430M/7.39G [00:08<02:20, 49.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   6%|   | 440M/7.39G [00:09<02:20, 49.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   6%|   | 451M/7.39G [00:09<02:20, 49.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   6%|   | 461M/7.39G [00:09<02:20, 49.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   6%|   | 472M/7.39G [00:09<02:19, 49.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   7%|   | 482M/7.39G [00:09<02:18, 49.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   7%|   | 493M/7.39G [00:10<02:19, 49.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   7%|   | 503M/7.39G [00:10<02:19, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   7%|   | 514M/7.39G [00:10<02:19, 49.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   7%|   | 524M/7.39G [00:10<02:21, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   7%|   | 535M/7.39G [00:10<02:20, 48.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   7%|   | 545M/7.39G [00:11<02:22, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   8%|   | 556M/7.39G [00:11<02:22, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   8%|   | 566M/7.39G [00:11<02:21, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   8%|   | 577M/7.39G [00:11<02:21, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   8%|   | 587M/7.39G [00:12<02:21, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   8%|   | 598M/7.39G [00:12<02:21, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   8%|   | 608M/7.39G [00:12<02:19, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   8%|   | 619M/7.39G [00:12<02:19, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   9%|   | 629M/7.39G [00:12<02:20, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   9%|   | 640M/7.39G [00:13<02:19, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   9%|   | 650M/7.39G [00:13<02:20, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   9%|   | 661M/7.39G [00:13<02:20, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   9%|   | 671M/7.39G [00:13<02:21, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   9%|   | 682M/7.39G [00:14<02:20, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:   9%|   | 692M/7.39G [00:14<02:20, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  10%|   | 703M/7.39G [00:14<02:19, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  10%|   | 713M/7.39G [00:14<02:20, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  10%|   | 724M/7.39G [00:14<02:19, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  10%|   | 734M/7.39G [00:15<02:18, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  10%|   | 744M/7.39G [00:15<02:18, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  10%|   | 755M/7.39G [00:15<02:17, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  10%|   | 765M/7.39G [00:15<02:29, 44.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  10%|   | 776M/7.39G [00:16<02:46, 39.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  11%|   | 786M/7.39G [00:16<02:38, 41.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  11%|   | 797M/7.39G [00:16<02:32, 43.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  11%|   | 807M/7.39G [00:16<02:28, 44.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  11%|   | 818M/7.39G [00:17<02:25, 45.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  11%|   | 828M/7.39G [00:17<02:23, 45.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  11%|   | 839M/7.39G [00:17<02:21, 46.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  11%|   | 849M/7.39G [00:17<02:19, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  12%|   | 860M/7.39G [00:17<02:18, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  12%|   | 870M/7.39G [00:18<02:16, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  12%|   | 881M/7.39G [00:18<02:12, 49.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  12%|   | 891M/7.39G [00:18<02:13, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  12%|   | 902M/7.39G [00:18<02:14, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  12%|   | 912M/7.39G [00:18<02:14, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  12%|   | 923M/7.39G [00:19<02:15, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  13%|   | 933M/7.39G [00:19<02:15, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  13%|   | 944M/7.39G [00:19<02:15, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  13%|   | 954M/7.39G [00:19<02:14, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  13%|   | 965M/7.39G [00:20<02:14, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  13%|   | 975M/7.39G [00:20<02:12, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  13%|   | 986M/7.39G [00:20<02:12, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  13%|   | 996M/7.39G [00:20<02:11, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  14%|  | 1.01G/7.39G [00:20<02:12, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  14%|  | 1.02G/7.39G [00:21<02:12, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  14%|  | 1.03G/7.39G [00:21<02:13, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  14%|  | 1.04G/7.39G [00:21<02:14, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  14%|  | 1.05G/7.39G [00:21<02:13, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  14%|  | 1.06G/7.39G [00:22<02:12, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  14%|  | 1.07G/7.39G [00:22<02:14, 47.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  15%|  | 1.08G/7.39G [00:22<02:14, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  15%|  | 1.09G/7.39G [00:22<02:14, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  15%|  | 1.10G/7.39G [00:22<02:16, 46.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  15%|  | 1.11G/7.39G [00:23<02:14, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  15%|  | 1.12G/7.39G [00:23<02:12, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  15%|  | 1.13G/7.39G [00:23<02:12, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  15%|  | 1.14G/7.39G [00:23<02:11, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  16%|  | 1.15G/7.39G [00:24<02:13, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  16%|  | 1.16G/7.39G [00:24<02:12, 47.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  16%|  | 1.17G/7.39G [00:24<02:11, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  16%|  | 1.18G/7.39G [00:24<02:11, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  16%|  | 1.20G/7.39G [00:24<02:11, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  16%|  | 1.21G/7.39G [00:25<02:10, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  16%|  | 1.22G/7.39G [00:25<02:08, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  17%|  | 1.23G/7.39G [00:25<02:08, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  17%|  | 1.24G/7.39G [00:25<02:08, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  17%|  | 1.25G/7.39G [00:26<02:06, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  17%|  | 1.26G/7.39G [00:26<02:07, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  17%|  | 1.27G/7.39G [00:26<02:06, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  17%|  | 1.28G/7.39G [00:26<02:07, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  17%|  | 1.29G/7.39G [00:26<02:05, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  18%|  | 1.30G/7.39G [00:27<02:06, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  18%|  | 1.31G/7.39G [00:27<02:05, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  18%|  | 1.32G/7.39G [00:27<02:06, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  18%|  | 1.33G/7.39G [00:27<02:06, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  18%|  | 1.34G/7.39G [00:28<02:13, 45.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  18%|  | 1.35G/7.39G [00:28<02:11, 45.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  18%|  | 1.36G/7.39G [00:28<02:10, 46.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  19%|  | 1.37G/7.39G [00:28<02:09, 46.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  19%|  | 1.38G/7.39G [00:28<02:07, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  19%|  | 1.39G/7.39G [00:29<02:07, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  19%|  | 1.41G/7.39G [00:29<02:07, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  19%|  | 1.42G/7.39G [00:29<02:06, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  19%|  | 1.43G/7.39G [00:29<02:06, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  19%|  | 1.44G/7.39G [00:30<02:05, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  20%|  | 1.45G/7.39G [00:30<02:05, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  20%|  | 1.46G/7.39G [00:30<02:04, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  20%|  | 1.47G/7.39G [00:30<02:05, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  20%|  | 1.48G/7.39G [00:30<02:04, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  20%|  | 1.49G/7.39G [00:31<02:04, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  20%|  | 1.50G/7.39G [00:31<02:04, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  20%|  | 1.51G/7.39G [00:31<02:04, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  21%|  | 1.52G/7.39G [00:31<02:04, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  21%|  | 1.53G/7.39G [00:32<02:03, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  21%|  | 1.54G/7.39G [00:32<02:04, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  21%|  | 1.55G/7.39G [00:32<02:03, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  21%|  | 1.56G/7.39G [00:32<02:03, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  21%|  | 1.57G/7.39G [00:32<02:02, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  21%|  | 1.58G/7.39G [00:33<02:02, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  22%|  | 1.59G/7.39G [00:33<02:35, 37.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  22%|  | 1.60G/7.39G [00:33<02:26, 39.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  22%|  | 1.61G/7.39G [00:33<02:17, 42.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  22%|  | 1.63G/7.39G [00:34<02:12, 43.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  22%|  | 1.64G/7.39G [00:34<02:07, 45.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  22%|  | 1.65G/7.39G [00:34<02:04, 46.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  22%|  | 1.66G/7.39G [00:34<02:03, 46.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  23%|  | 1.67G/7.39G [00:35<02:02, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  23%|  | 1.68G/7.39G [00:35<02:01, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  23%|  | 1.69G/7.39G [00:35<02:00, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  23%|  | 1.70G/7.39G [00:35<02:01, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  23%|  | 1.71G/7.39G [00:35<02:01, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  23%|  | 1.72G/7.39G [00:36<02:00, 47.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  23%|  | 1.73G/7.39G [00:36<02:00, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  24%|  | 1.74G/7.39G [00:36<02:00, 47.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  24%|  | 1.75G/7.39G [00:36<02:00, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  24%|  | 1.76G/7.39G [00:37<01:59, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  24%|  | 1.77G/7.39G [00:37<01:57, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  24%|  | 1.78G/7.39G [00:37<01:58, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  24%|  | 1.79G/7.39G [00:37<01:56, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  24%|  | 1.80G/7.39G [00:37<01:56, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  25%|  | 1.81G/7.39G [00:38<01:57, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  25%|  | 1.82G/7.39G [00:38<01:55, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  25%|  | 1.84G/7.39G [00:38<01:56, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  25%|  | 1.85G/7.39G [00:38<01:57, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  25%|  | 1.86G/7.39G [00:39<01:57, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  25%|  | 1.87G/7.39G [00:39<01:55, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  25%|  | 1.88G/7.39G [00:39<01:55, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  26%|  | 1.89G/7.39G [00:39<01:55, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  26%|  | 1.90G/7.39G [00:39<01:56, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  26%|  | 1.91G/7.39G [00:40<01:54, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  26%|  | 1.92G/7.39G [00:40<01:54, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  26%|  | 1.93G/7.39G [00:40<01:55, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  26%|  | 1.94G/7.39G [00:40<01:54, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  26%|  | 1.95G/7.39G [00:41<01:54, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  27%|  | 1.96G/7.39G [00:41<01:53, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  27%|  | 1.97G/7.39G [00:41<01:53, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  27%|  | 1.98G/7.39G [00:41<01:53, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  27%|  | 1.99G/7.39G [00:41<01:52, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  27%|  | 2.00G/7.39G [00:42<01:52, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  27%|  | 2.01G/7.39G [00:42<01:52, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  27%|  | 2.02G/7.39G [00:42<01:52, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  28%|  | 2.03G/7.39G [00:42<01:50, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  28%|  | 2.04G/7.39G [00:43<01:50, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  28%|  | 2.06G/7.39G [00:43<01:50, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  28%|  | 2.07G/7.39G [00:43<01:50, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  28%|  | 2.08G/7.39G [00:43<01:48, 48.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  28%|  | 2.09G/7.39G [00:43<01:50, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  28%|  | 2.10G/7.39G [00:44<01:50, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  29%|  | 2.11G/7.39G [00:44<01:50, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  29%|  | 2.12G/7.39G [00:44<01:48, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  29%|  | 2.13G/7.39G [00:44<01:49, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  29%|  | 2.14G/7.39G [00:44<01:49, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  29%|  | 2.15G/7.39G [00:45<01:49, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  29%|  | 2.16G/7.39G [00:45<01:48, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  29%|  | 2.17G/7.39G [00:45<01:49, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  30%|  | 2.18G/7.39G [00:45<01:50, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  30%|  | 2.19G/7.39G [00:46<01:49, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  30%|  | 2.20G/7.39G [00:46<01:47, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  30%|  | 2.21G/7.39G [00:46<01:47, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  30%|  | 2.22G/7.39G [00:46<01:47, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  30%|  | 2.23G/7.39G [00:46<01:48, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  30%|  | 2.24G/7.39G [00:47<01:47, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  30%|  | 2.25G/7.39G [00:47<01:47, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  31%|  | 2.26G/7.39G [00:47<01:48, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  31%|  | 2.28G/7.39G [00:47<01:47, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  31%|  | 2.29G/7.39G [00:48<01:45, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  31%|  | 2.30G/7.39G [00:48<01:45, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  31%|  | 2.31G/7.39G [00:48<01:44, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  31%|  | 2.32G/7.39G [00:48<01:45, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  31%|  | 2.33G/7.39G [00:48<01:45, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  32%|  | 2.34G/7.39G [00:49<01:45, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  32%|  | 2.35G/7.39G [00:49<01:43, 48.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  32%|  | 2.36G/7.39G [00:49<01:43, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  32%|  | 2.37G/7.39G [00:49<01:44, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  32%|  | 2.38G/7.39G [00:49<01:43, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  32%|  | 2.39G/7.39G [00:50<01:43, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  32%|  | 2.40G/7.39G [00:50<01:43, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  33%|  | 2.41G/7.39G [00:50<01:43, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  33%|  | 2.42G/7.39G [00:50<01:43, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  33%|  | 2.43G/7.39G [00:51<01:42, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  33%|  | 2.44G/7.39G [00:51<01:42, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  33%|  | 2.45G/7.39G [00:51<01:42, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  33%|  | 2.46G/7.39G [00:51<01:41, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  33%|  | 2.47G/7.39G [00:51<01:41, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  34%|  | 2.49G/7.39G [00:52<01:41, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  34%|  | 2.50G/7.39G [00:52<01:40, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  34%|  | 2.51G/7.39G [00:52<01:41, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  34%|  | 2.52G/7.39G [00:52<01:41, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  34%|  | 2.53G/7.39G [00:53<01:42, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  34%|  | 2.54G/7.39G [00:53<01:42, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  34%|  | 2.55G/7.39G [00:53<02:09, 37.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  35%|  | 2.56G/7.39G [00:53<02:01, 39.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  35%|  | 2.57G/7.39G [00:54<01:54, 42.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  35%|  | 2.58G/7.39G [00:54<01:50, 43.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  35%|  | 2.59G/7.39G [00:54<01:46, 45.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  35%|  | 2.60G/7.39G [00:54<01:44, 45.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  35%|  | 2.61G/7.39G [00:54<01:42, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  35%|  | 2.62G/7.39G [00:55<01:42, 46.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  36%|  | 2.63G/7.39G [00:55<01:42, 46.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  36%|  | 2.64G/7.39G [00:55<01:42, 46.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  36%|  | 2.65G/7.39G [00:55<01:40, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  36%|  | 2.66G/7.39G [00:56<01:40, 47.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  36%|  | 2.67G/7.39G [00:56<01:39, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  36%|  | 2.68G/7.39G [00:56<01:40, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  36%|  | 2.69G/7.39G [00:56<01:39, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  37%|  | 2.71G/7.39G [00:56<01:38, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  37%|  | 2.72G/7.39G [00:57<01:37, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  37%|  | 2.73G/7.39G [00:57<01:36, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  37%|  | 2.74G/7.39G [00:57<01:37, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  37%|  | 2.75G/7.39G [00:57<01:36, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  37%|  | 2.76G/7.39G [00:58<01:36, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  37%|  | 2.77G/7.39G [00:58<01:36, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  38%| | 2.78G/7.39G [00:58<01:36, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  38%| | 2.79G/7.39G [00:58<01:35, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  38%| | 2.80G/7.39G [00:58<01:35, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  38%| | 2.81G/7.39G [00:59<01:36, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  38%| | 2.82G/7.39G [00:59<01:34, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  38%| | 2.83G/7.39G [00:59<01:34, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  38%| | 2.84G/7.39G [00:59<01:34, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  39%| | 2.85G/7.39G [01:00<01:34, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  39%| | 2.86G/7.39G [01:00<01:34, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  39%| | 2.87G/7.39G [01:00<01:33, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  39%| | 2.88G/7.39G [01:00<01:34, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  39%| | 2.89G/7.39G [01:00<01:34, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  39%| | 2.90G/7.39G [01:01<01:33, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  39%| | 2.92G/7.39G [01:01<01:33, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  40%| | 2.93G/7.39G [01:01<01:32, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  40%| | 2.94G/7.39G [01:01<01:32, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  40%| | 2.95G/7.39G [01:02<01:33, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  40%| | 2.96G/7.39G [01:02<01:32, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  40%| | 2.97G/7.39G [01:02<01:32, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  40%| | 2.98G/7.39G [01:02<01:32, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  40%| | 2.99G/7.39G [01:02<01:31, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  41%| | 3.00G/7.39G [01:03<01:30, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  41%| | 3.01G/7.39G [01:03<01:29, 48.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  41%| | 3.02G/7.39G [01:03<01:30, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  41%| | 3.03G/7.39G [01:03<01:29, 48.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  41%| | 3.04G/7.39G [01:03<01:29, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  41%| | 3.05G/7.39G [01:04<01:29, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  41%| | 3.06G/7.39G [01:04<01:29, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  42%| | 3.07G/7.39G [01:04<01:29, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  42%| | 3.08G/7.39G [01:04<01:28, 48.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  42%| | 3.09G/7.39G [01:05<01:27, 48.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  42%| | 3.10G/7.39G [01:05<01:27, 48.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  42%| | 3.11G/7.39G [01:05<01:28, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  42%| | 3.12G/7.39G [01:05<01:27, 48.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  42%| | 3.14G/7.39G [01:05<01:28, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  43%| | 3.15G/7.39G [01:06<01:28, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  43%| | 3.16G/7.39G [01:06<01:28, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  43%| | 3.17G/7.39G [01:06<01:28, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  43%| | 3.18G/7.39G [01:06<01:27, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  43%| | 3.19G/7.39G [01:07<01:27, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  43%| | 3.20G/7.39G [01:07<01:27, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  43%| | 3.21G/7.39G [01:07<01:27, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  44%| | 3.22G/7.39G [01:07<01:49, 38.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  44%| | 3.23G/7.39G [01:08<01:42, 40.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  44%| | 3.24G/7.39G [01:08<01:36, 42.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  44%| | 3.25G/7.39G [01:08<01:34, 44.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  44%| | 3.26G/7.39G [01:08<01:31, 45.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  44%| | 3.27G/7.39G [01:08<01:30, 45.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  44%| | 3.28G/7.39G [01:09<01:27, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  45%| | 3.29G/7.39G [01:09<01:25, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  45%| | 3.30G/7.39G [01:09<01:25, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  45%| | 3.31G/7.39G [01:09<01:24, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  45%| | 3.32G/7.39G [01:10<01:24, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  45%| | 3.33G/7.39G [01:10<01:24, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  45%| | 3.34G/7.39G [01:10<01:24, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  45%| | 3.36G/7.39G [01:10<01:24, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  46%| | 3.37G/7.39G [01:10<01:23, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  46%| | 3.38G/7.39G [01:11<01:23, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  46%| | 3.39G/7.39G [01:11<01:22, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  46%| | 3.40G/7.39G [01:11<01:24, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  46%| | 3.41G/7.39G [01:11<01:23, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  46%| | 3.42G/7.39G [01:11<01:23, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  46%| | 3.43G/7.39G [01:12<01:22, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  47%| | 3.44G/7.39G [01:12<01:24, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  47%| | 3.45G/7.39G [01:12<01:23, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  47%| | 3.46G/7.39G [01:12<01:23, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  47%| | 3.47G/7.39G [01:13<01:22, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  47%| | 3.48G/7.39G [01:13<01:22, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  47%| | 3.49G/7.39G [01:13<01:21, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  47%| | 3.50G/7.39G [01:13<01:21, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  48%| | 3.51G/7.39G [01:13<01:20, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  48%| | 3.52G/7.39G [01:14<01:20, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  48%| | 3.53G/7.39G [01:14<01:20, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  48%| | 3.54G/7.39G [01:14<01:20, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  48%| | 3.55G/7.39G [01:14<01:20, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  48%| | 3.57G/7.39G [01:15<01:20, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  48%| | 3.58G/7.39G [01:15<01:19, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  49%| | 3.59G/7.39G [01:15<01:18, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  49%| | 3.60G/7.39G [01:15<01:18, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  49%| | 3.61G/7.39G [01:15<01:18, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  49%| | 3.62G/7.39G [01:16<01:18, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  49%| | 3.63G/7.39G [01:16<01:19, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  49%| | 3.64G/7.39G [01:16<01:18, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  49%| | 3.65G/7.39G [01:16<01:17, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  50%| | 3.66G/7.39G [01:17<01:17, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  50%| | 3.67G/7.39G [01:17<01:17, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  50%| | 3.68G/7.39G [01:17<01:16, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  50%| | 3.69G/7.39G [01:17<01:16, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  50%| | 3.70G/7.39G [01:17<01:16, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  50%| | 3.71G/7.39G [01:18<01:16, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  50%| | 3.72G/7.39G [01:18<01:16, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  50%| | 3.73G/7.39G [01:18<01:16, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  51%| | 3.74G/7.39G [01:18<01:37, 37.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  51%| | 3.75G/7.39G [01:19<01:30, 40.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  51%| | 3.76G/7.39G [01:19<01:25, 42.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  51%| | 3.77G/7.39G [01:19<01:22, 43.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  51%| | 3.79G/7.39G [01:19<01:22, 44.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  51%| | 3.80G/7.39G [01:20<01:19, 45.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  51%| | 3.81G/7.39G [01:20<01:17, 46.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  52%| | 3.82G/7.39G [01:20<01:17, 46.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  52%| | 3.83G/7.39G [01:20<01:15, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  52%| | 3.84G/7.39G [01:20<01:14, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  52%| | 3.85G/7.39G [01:21<01:14, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  52%| | 3.86G/7.39G [01:21<01:14, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  52%| | 3.87G/7.39G [01:21<01:13, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  52%| | 3.88G/7.39G [01:21<01:12, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  53%| | 3.89G/7.39G [01:22<01:12, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  53%| | 3.90G/7.39G [01:22<01:13, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  53%| | 3.91G/7.39G [01:22<01:13, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  53%| | 3.92G/7.39G [01:22<01:12, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  53%| | 3.93G/7.39G [01:22<01:12, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  53%| | 3.94G/7.39G [01:23<01:12, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  53%| | 3.95G/7.39G [01:23<01:12, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  54%| | 3.96G/7.39G [01:23<01:11, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  54%| | 3.97G/7.39G [01:23<01:11, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  54%| | 3.98G/7.39G [01:24<01:11, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  54%| | 4.00G/7.39G [01:24<01:11, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  54%| | 4.01G/7.39G [01:24<01:11, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  54%| | 4.02G/7.39G [01:24<01:10, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  54%| | 4.03G/7.39G [01:24<01:10, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  55%| | 4.04G/7.39G [01:25<01:10, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  55%| | 4.05G/7.39G [01:25<01:10, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  55%| | 4.06G/7.39G [01:25<01:09, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  55%| | 4.07G/7.39G [01:25<01:08, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  55%| | 4.08G/7.39G [01:25<01:08, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  55%| | 4.09G/7.39G [01:26<01:09, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  55%| | 4.10G/7.39G [01:26<01:08, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  56%| | 4.11G/7.39G [01:26<01:08, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  56%| | 4.12G/7.39G [01:26<01:08, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  56%| | 4.13G/7.39G [01:27<01:08, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  56%| | 4.14G/7.39G [01:27<01:08, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  56%| | 4.15G/7.39G [01:27<01:07, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  56%| | 4.16G/7.39G [01:27<01:07, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  56%| | 4.17G/7.39G [01:27<01:06, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  57%| | 4.18G/7.39G [01:28<01:06, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  57%| | 4.19G/7.39G [01:28<01:06, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  57%| | 4.20G/7.39G [01:28<01:06, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  57%| | 4.22G/7.39G [01:28<01:07, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  57%| | 4.23G/7.39G [01:29<01:07, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  57%| | 4.24G/7.39G [01:29<01:07, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  57%| | 4.25G/7.39G [01:29<01:06, 47.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  58%| | 4.26G/7.39G [01:29<01:06, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  58%| | 4.27G/7.39G [01:29<01:05, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  58%| | 4.28G/7.39G [01:30<01:05, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  58%| | 4.29G/7.39G [01:30<01:05, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  58%| | 4.30G/7.39G [01:30<01:04, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  58%| | 4.31G/7.39G [01:30<01:04, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  58%| | 4.32G/7.39G [01:31<01:04, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  59%| | 4.33G/7.39G [01:31<01:03, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  59%| | 4.34G/7.39G [01:31<01:03, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  59%| | 4.35G/7.39G [01:31<01:03, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  59%| | 4.36G/7.39G [01:31<01:03, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  59%| | 4.37G/7.39G [01:32<01:03, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  59%| | 4.38G/7.39G [01:32<01:03, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  59%| | 4.39G/7.39G [01:32<01:02, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  60%| | 4.40G/7.39G [01:32<01:02, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  60%| | 4.41G/7.39G [01:33<01:02, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  60%| | 4.42G/7.39G [01:33<01:01, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  60%| | 4.44G/7.39G [01:33<01:02, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  60%| | 4.45G/7.39G [01:33<01:02, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  60%| | 4.46G/7.39G [01:33<01:02, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  60%| | 4.47G/7.39G [01:34<01:01, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  61%| | 4.48G/7.39G [01:34<01:01, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  61%| | 4.49G/7.39G [01:34<01:01, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  61%| | 4.50G/7.39G [01:34<01:01, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  61%| | 4.51G/7.39G [01:35<01:01, 46.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  61%| | 4.52G/7.39G [01:35<01:01, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  61%| | 4.53G/7.39G [01:35<01:01, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  61%| | 4.54G/7.39G [01:35<01:00, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  62%| | 4.55G/7.39G [01:35<00:59, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  62%| | 4.56G/7.39G [01:36<00:59, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  62%| | 4.57G/7.39G [01:36<00:59, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  62%| | 4.58G/7.39G [01:36<00:59, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  62%| | 4.59G/7.39G [01:36<00:58, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  62%| | 4.60G/7.39G [01:37<00:58, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  62%| | 4.61G/7.39G [01:37<00:57, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  63%| | 4.62G/7.39G [01:37<00:57, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  63%| | 4.63G/7.39G [01:37<00:57, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  63%| | 4.65G/7.39G [01:37<00:57, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  63%| | 4.66G/7.39G [01:38<00:57, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  63%| | 4.67G/7.39G [01:38<00:57, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  63%| | 4.68G/7.39G [01:38<00:56, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  63%| | 4.69G/7.39G [01:38<00:56, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  64%| | 4.70G/7.39G [01:39<00:56, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  64%| | 4.71G/7.39G [01:39<00:56, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  64%| | 4.72G/7.39G [01:39<00:55, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  64%| | 4.73G/7.39G [01:39<00:55, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  64%| | 4.74G/7.39G [01:39<00:55, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  64%| | 4.75G/7.39G [01:40<00:55, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  64%| | 4.76G/7.39G [01:40<01:10, 37.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  65%| | 4.77G/7.39G [01:40<01:04, 40.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  65%| | 4.78G/7.39G [01:40<01:01, 42.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  65%| | 4.79G/7.39G [01:41<00:59, 44.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  65%| | 4.80G/7.39G [01:41<00:57, 44.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  65%| | 4.81G/7.39G [01:41<00:56, 45.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  65%| | 4.82G/7.39G [01:41<00:55, 46.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  65%| | 4.83G/7.39G [01:42<00:55, 46.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  66%| | 4.84G/7.39G [01:42<00:54, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  66%| | 4.85G/7.39G [01:42<00:54, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  66%| | 4.87G/7.39G [01:42<00:53, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  66%| | 4.88G/7.39G [01:42<00:53, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  66%| | 4.89G/7.39G [01:43<00:53, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  66%| | 4.90G/7.39G [01:43<00:52, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  66%| | 4.91G/7.39G [01:43<00:52, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  67%| | 4.92G/7.39G [01:43<00:52, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  67%| | 4.93G/7.39G [01:44<00:51, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  67%| | 4.94G/7.39G [01:44<00:51, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  67%| | 4.95G/7.39G [01:44<00:51, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  67%| | 4.96G/7.39G [01:44<00:51, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  67%| | 4.97G/7.39G [01:44<00:51, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  67%| | 4.98G/7.39G [01:45<00:57, 42.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  68%| | 4.99G/7.39G [01:45<00:55, 43.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  68%| | 5.00G/7.39G [01:45<00:53, 44.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  68%| | 5.01G/7.39G [01:45<00:52, 45.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  68%| | 5.02G/7.39G [01:46<00:51, 46.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  68%| | 5.03G/7.39G [01:46<00:51, 46.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  68%| | 5.04G/7.39G [01:46<00:50, 46.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  68%| | 5.05G/7.39G [01:46<00:50, 46.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  69%| | 5.06G/7.39G [01:47<00:49, 47.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  69%| | 5.08G/7.39G [01:47<00:49, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  69%| | 5.09G/7.39G [01:47<00:48, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  69%| | 5.10G/7.39G [01:47<00:48, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  69%| | 5.11G/7.39G [01:47<00:48, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  69%| | 5.12G/7.39G [01:48<00:47, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  69%| | 5.13G/7.39G [01:48<00:46, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  70%| | 5.14G/7.39G [01:48<00:47, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  70%| | 5.15G/7.39G [01:48<00:47, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  70%| | 5.16G/7.39G [01:48<00:46, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  70%| | 5.17G/7.39G [01:49<00:46, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  70%| | 5.18G/7.39G [01:49<00:46, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  70%| | 5.19G/7.39G [01:49<00:46, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  70%| | 5.20G/7.39G [01:49<00:45, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  70%| | 5.21G/7.39G [01:50<00:45, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  71%| | 5.22G/7.39G [01:50<00:45, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  71%| | 5.23G/7.39G [01:50<00:44, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  71%|| 5.24G/7.39G [01:50<00:44, 48.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  71%|| 5.25G/7.39G [01:50<00:43, 48.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  71%|| 5.26G/7.39G [01:51<00:43, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  71%|| 5.27G/7.39G [01:51<00:43, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  71%|| 5.28G/7.39G [01:51<00:43, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  72%|| 5.30G/7.39G [01:51<00:43, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  72%|| 5.31G/7.39G [01:52<00:43, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  72%|| 5.32G/7.39G [01:52<00:43, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  72%|| 5.33G/7.39G [01:52<00:42, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  72%|| 5.34G/7.39G [01:52<00:43, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  72%|| 5.35G/7.39G [01:52<00:42, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  72%|| 5.36G/7.39G [01:53<00:42, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  73%|| 5.37G/7.39G [01:53<00:42, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  73%|| 5.38G/7.39G [01:53<00:42, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  73%|| 5.39G/7.39G [01:53<00:42, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  73%|| 5.40G/7.39G [01:54<00:41, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  73%|| 5.41G/7.39G [01:54<00:41, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  73%|| 5.42G/7.39G [01:54<00:41, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  73%|| 5.43G/7.39G [01:54<00:40, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  74%|| 5.44G/7.39G [01:54<00:40, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  74%|| 5.45G/7.39G [01:55<00:47, 40.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  74%|| 5.46G/7.39G [01:55<00:47, 40.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  74%|| 5.47G/7.39G [01:55<00:44, 42.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  74%|| 5.48G/7.39G [01:55<00:43, 44.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  74%|| 5.49G/7.39G [01:56<00:42, 45.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  74%|| 5.51G/7.39G [01:56<00:41, 46.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  75%|| 5.52G/7.39G [01:56<00:40, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  75%|| 5.53G/7.39G [01:56<00:39, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  75%|| 5.54G/7.39G [01:57<00:39, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  75%|| 5.55G/7.39G [01:57<00:38, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  75%|| 5.56G/7.39G [01:57<00:38, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  75%|| 5.57G/7.39G [01:57<00:37, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  75%|| 5.58G/7.39G [01:57<00:36, 49.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  76%|| 5.59G/7.39G [01:58<00:36, 49.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  76%|| 5.60G/7.39G [01:58<00:36, 49.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  76%|| 5.61G/7.39G [01:58<00:36, 49.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  76%|| 5.62G/7.39G [01:58<00:36, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  76%|| 5.63G/7.39G [01:58<00:36, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  76%|| 5.64G/7.39G [01:59<00:36, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  76%|| 5.65G/7.39G [01:59<00:36, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  77%|| 5.66G/7.39G [01:59<00:36, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  77%|| 5.67G/7.39G [01:59<00:35, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  77%|| 5.68G/7.39G [02:00<00:35, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  77%|| 5.69G/7.39G [02:00<00:35, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  77%|| 5.70G/7.39G [02:00<00:36, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  77%|| 5.71G/7.39G [02:00<00:35, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  77%|| 5.73G/7.39G [02:00<00:35, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  78%|| 5.74G/7.39G [02:01<00:34, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  78%|| 5.75G/7.39G [02:01<00:34, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  78%|| 5.76G/7.39G [02:01<00:34, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  78%|| 5.77G/7.39G [02:01<00:33, 48.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  78%|| 5.78G/7.39G [02:02<00:33, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  78%|| 5.79G/7.39G [02:02<00:33, 48.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  78%|| 5.80G/7.39G [02:02<00:32, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  79%|| 5.81G/7.39G [02:02<00:32, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  79%|| 5.82G/7.39G [02:02<00:32, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  79%|| 5.83G/7.39G [02:03<00:32, 48.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  79%|| 5.84G/7.39G [02:03<00:31, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  79%|| 5.85G/7.39G [02:03<00:31, 48.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  79%|| 5.86G/7.39G [02:03<00:31, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  79%|| 5.87G/7.39G [02:03<00:31, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  80%|| 5.88G/7.39G [02:04<00:31, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  80%|| 5.89G/7.39G [02:04<00:31, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  80%|| 5.90G/7.39G [02:04<00:31, 47.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  80%|| 5.91G/7.39G [02:04<00:30, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  80%|| 5.92G/7.39G [02:05<00:30, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  80%|| 5.93G/7.39G [02:05<00:31, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  80%|| 5.95G/7.39G [02:05<00:31, 46.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  81%|| 5.96G/7.39G [02:05<00:30, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  81%|| 5.97G/7.39G [02:05<00:30, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  81%|| 5.98G/7.39G [02:06<00:30, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  81%|| 5.99G/7.39G [02:06<00:30, 46.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  81%|| 6.00G/7.39G [02:06<00:29, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  81%|| 6.01G/7.39G [02:06<00:29, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  81%|| 6.02G/7.39G [02:07<00:29, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  82%|| 6.03G/7.39G [02:07<00:28, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  82%|| 6.04G/7.39G [02:07<00:28, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  82%|| 6.05G/7.39G [02:07<00:28, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  82%|| 6.06G/7.39G [02:07<00:27, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  82%|| 6.07G/7.39G [02:08<00:27, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  82%|| 6.08G/7.39G [02:08<00:28, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  82%|| 6.09G/7.39G [02:08<00:27, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  83%|| 6.10G/7.39G [02:08<00:27, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  83%|| 6.11G/7.39G [02:09<00:34, 37.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  83%|| 6.12G/7.39G [02:09<00:31, 40.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  83%|| 6.13G/7.39G [02:09<00:30, 41.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  83%|| 6.14G/7.39G [02:09<00:29, 43.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  83%|| 6.16G/7.39G [02:10<00:28, 44.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  83%|| 6.17G/7.39G [02:10<00:27, 44.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  84%|| 6.18G/7.39G [02:10<00:26, 45.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  84%|| 6.19G/7.39G [02:10<00:25, 46.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  84%|| 6.20G/7.39G [02:11<00:25, 46.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  84%|| 6.21G/7.39G [02:11<00:25, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  84%|| 6.22G/7.39G [02:11<00:24, 47.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  84%|| 6.23G/7.39G [02:11<00:24, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  84%|| 6.24G/7.39G [02:11<00:24, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  85%|| 6.25G/7.39G [02:12<00:24, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  85%|| 6.26G/7.39G [02:12<00:23, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  85%|| 6.27G/7.39G [02:12<00:23, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  85%|| 6.28G/7.39G [02:12<00:23, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  85%|| 6.29G/7.39G [02:13<00:23, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  85%|| 6.30G/7.39G [02:13<00:23, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  85%|| 6.31G/7.39G [02:13<00:22, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  86%|| 6.32G/7.39G [02:13<00:22, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  86%|| 6.33G/7.39G [02:13<00:22, 47.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  86%|| 6.34G/7.39G [02:14<00:22, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  86%|| 6.35G/7.39G [02:14<00:21, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  86%|| 6.36G/7.39G [02:14<00:21, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  86%|| 6.38G/7.39G [02:14<00:21, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  86%|| 6.39G/7.39G [02:15<00:21, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  87%|| 6.40G/7.39G [02:15<00:20, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  87%|| 6.41G/7.39G [02:15<00:20, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  87%|| 6.42G/7.39G [02:15<00:20, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  87%|| 6.43G/7.39G [02:15<00:20, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  87%|| 6.44G/7.39G [02:16<00:19, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  87%|| 6.45G/7.39G [02:16<00:19, 48.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  87%|| 6.46G/7.39G [02:16<00:18, 49.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  88%|| 6.47G/7.39G [02:16<00:19, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  88%|| 6.48G/7.39G [02:17<00:18, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  88%|| 6.49G/7.39G [02:17<00:18, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  88%|| 6.50G/7.39G [02:17<00:18, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  88%|| 6.51G/7.39G [02:17<00:18, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  88%|| 6.52G/7.39G [02:17<00:18, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  88%|| 6.53G/7.39G [02:18<00:17, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  89%|| 6.54G/7.39G [02:18<00:17, 48.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  89%|| 6.55G/7.39G [02:18<00:17, 48.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  89%|| 6.56G/7.39G [02:18<00:17, 47.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  89%|| 6.57G/7.39G [02:18<00:17, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  89%|| 6.59G/7.39G [02:19<00:16, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  89%|| 6.60G/7.39G [02:19<00:16, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  89%|| 6.61G/7.39G [02:19<00:16, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  90%|| 6.62G/7.39G [02:19<00:16, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  90%|| 6.63G/7.39G [02:20<00:20, 37.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  90%|| 6.64G/7.39G [02:20<00:18, 40.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  90%|| 6.65G/7.39G [02:20<00:17, 41.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  90%|| 6.66G/7.39G [02:20<00:17, 42.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  90%|| 6.67G/7.39G [02:21<00:16, 44.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  90%|| 6.68G/7.39G [02:21<00:15, 45.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  90%|| 6.69G/7.39G [02:21<00:15, 45.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  91%|| 6.70G/7.39G [02:21<00:15, 45.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  91%|| 6.71G/7.39G [02:22<00:14, 46.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  91%|| 6.72G/7.39G [02:22<00:14, 46.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  91%|| 6.73G/7.39G [02:22<00:14, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  91%|| 6.74G/7.39G [02:22<00:14, 46.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  91%|| 6.75G/7.39G [02:22<00:13, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  91%|| 6.76G/7.39G [02:23<00:14, 44.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  92%|| 6.77G/7.39G [02:23<00:13, 44.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  92%|| 6.78G/7.39G [02:23<00:13, 45.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  92%|| 6.79G/7.39G [02:23<00:12, 46.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  92%|| 6.81G/7.39G [02:24<00:12, 46.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  92%|| 6.82G/7.39G [02:24<00:12, 46.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  92%|| 6.83G/7.39G [02:24<00:12, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  92%|| 6.84G/7.39G [02:24<00:11, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  93%|| 6.85G/7.39G [02:25<00:11, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  93%|| 6.86G/7.39G [02:25<00:11, 46.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  93%|| 6.87G/7.39G [02:25<00:11, 46.9MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  93%|| 6.88G/7.39G [02:25<00:10, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  93%|| 6.89G/7.39G [02:25<00:10, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  93%|| 6.90G/7.39G [02:26<00:10, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  93%|| 6.91G/7.39G [02:26<00:10, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  94%|| 6.92G/7.39G [02:26<00:09, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  94%|| 6.93G/7.39G [02:26<00:09, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  94%|| 6.94G/7.39G [02:27<00:09, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  94%|| 6.95G/7.39G [02:27<00:09, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  94%|| 6.96G/7.39G [02:27<00:09, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  94%|| 6.97G/7.39G [02:27<00:08, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  94%|| 6.98G/7.39G [02:27<00:08, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  95%|| 6.99G/7.39G [02:28<00:08, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  95%|| 7.00G/7.39G [02:28<00:08, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  95%|| 7.01G/7.39G [02:28<00:07, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  95%|| 7.03G/7.39G [02:28<00:07, 48.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  95%|| 7.04G/7.39G [02:28<00:07, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  95%|| 7.05G/7.39G [02:29<00:07, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  95%|| 7.06G/7.39G [02:29<00:07, 47.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  96%|| 7.07G/7.39G [02:29<00:06, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  96%|| 7.08G/7.39G [02:29<00:06, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  96%|| 7.09G/7.39G [02:30<00:06, 47.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  96%|| 7.10G/7.39G [02:30<00:06, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  96%|| 7.11G/7.39G [02:30<00:07, 38.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  96%|| 7.12G/7.39G [02:30<00:06, 40.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  96%|| 7.13G/7.39G [02:31<00:06, 42.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  97%|| 7.14G/7.39G [02:31<00:05, 43.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  97%|| 7.15G/7.39G [02:31<00:05, 44.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  97%|| 7.16G/7.39G [02:31<00:05, 46.0MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  97%|| 7.17G/7.39G [02:32<00:04, 46.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  97%|| 7.18G/7.39G [02:32<00:04, 46.4MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  97%|| 7.19G/7.39G [02:32<00:04, 46.6MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  97%|| 7.20G/7.39G [02:32<00:04, 46.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  98%|| 7.21G/7.39G [02:32<00:03, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  98%|| 7.22G/7.39G [02:33<00:03, 47.1MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  98%|| 7.24G/7.39G [02:33<00:03, 46.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  98%|| 7.25G/7.39G [02:33<00:03, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  98%|| 7.26G/7.39G [02:33<00:02, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  98%|| 7.27G/7.39G [02:34<00:02, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  98%|| 7.28G/7.39G [02:34<00:02, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  99%|| 7.29G/7.39G [02:34<00:02, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  99%|| 7.30G/7.39G [02:34<00:01, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  99%|| 7.31G/7.39G [02:34<00:01, 47.5MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  99%|| 7.32G/7.39G [02:35<00:01, 47.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  99%|| 7.33G/7.39G [02:35<00:01, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  99%|| 7.34G/7.39G [02:35<00:01, 47.8MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors:  99%|| 7.35G/7.39G [02:35<00:00, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors: 100%|| 7.36G/7.39G [02:36<00:00, 47.7MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors: 100%|| 7.37G/7.39G [02:36<00:00, 48.2MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors: 100%|| 7.38G/7.39G [02:36<00:00, 47.3MB/s]\u001b[A\n",
      "model-00002-of-000002.safetensors: 100%|| 7.39G/7.39G [02:36<00:00, 47.1MB/s]\u001b[A\n",
      "Downloading shards: 100%|| 2/2 [05:18<00:00, 159.29s/it]\n",
      "Downloading shards: 100%|| 2/2 [05:18<00:00, 159.43s/it]\n",
      "[INFO|modeling_utils.py:1633] 2025-03-08 20:31:17,678 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1140] 2025-03-08 20:31:17,691 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:03<00:00,  1.68s/it]\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:03<00:00,  1.80s/it]\n",
      "[INFO|modeling_utils.py:4970] 2025-03-08 20:31:21,498 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4978] 2025-03-08 20:31:21,498 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Llama-8B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "generation_config.json: 100%|| 181/181 [00:00<00:00, 1.34MB/s]\n",
      "[INFO|configuration_utils.py:1095] 2025-03-08 20:31:22,229 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-03-08 20:31:22,231 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "[INFO|2025-03-08 20:31:22] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-03-08 20:31:22] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-03-08 20:31:22] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-03-08 20:31:22] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-03-08 20:31:22] llamafactory.model.model_utils.misc:157 >> Found linear modules: v_proj,up_proj,k_proj,q_proj,o_proj,gate_proj,down_proj\n",
      "[INFO|2025-03-08 20:31:22] llamafactory.model.loader:157 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
      "[INFO|trainer.py:746] 2025-03-08 20:31:22,741 >> Using auto half precision backend\n",
      "[WARNING|trainer.py:781] 2025-03-08 20:31:22,742 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[INFO|trainer.py:2405] 2025-03-08 20:31:23,159 >> ***** Running training *****\n",
      "[INFO|trainer.py:2406] 2025-03-08 20:31:23,159 >>   Num examples = 679\n",
      "[INFO|trainer.py:2407] 2025-03-08 20:31:23,159 >>   Num Epochs = 30\n",
      "[INFO|trainer.py:2408] 2025-03-08 20:31:23,159 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2411] 2025-03-08 20:31:23,159 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2412] 2025-03-08 20:31:23,159 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2413] 2025-03-08 20:31:23,159 >>   Total optimization steps = 630\n",
      "[INFO|trainer.py:2414] 2025-03-08 20:31:23,163 >>   Number of trainable parameters = 20,971,520\n",
      "  1%|                                          | 5/630 [00:09<18:58,  1.82s/it][INFO|2025-03-08 20:31:32] llamafactory.train.callbacks:157 >> {'loss': 2.4750, 'learning_rate': 4.9992e-05, 'epoch': 0.24, 'throughput': 1123.65}\n",
      "{'loss': 2.475, 'grad_norm': 2.6637344360351562, 'learning_rate': 4.999222955002041e-05, 'epoch': 0.24, 'num_input_tokens_seen': 10992}\n",
      "  2%|                                         | 10/630 [00:18<18:07,  1.75s/it][INFO|2025-03-08 20:31:41] llamafactory.train.callbacks:157 >> {'loss': 1.8449, 'learning_rate': 4.9969e-05, 'epoch': 0.47, 'throughput': 1196.53}\n",
      "{'loss': 1.8449, 'grad_norm': 2.3780174255371094, 'learning_rate': 4.996892303047306e-05, 'epoch': 0.47, 'num_input_tokens_seen': 22144}\n",
      "  2%|                                         | 15/630 [00:27<17:47,  1.74s/it][INFO|2025-03-08 20:31:50] llamafactory.train.callbacks:157 >> {'loss': 1.3997, 'learning_rate': 4.9930e-05, 'epoch': 0.71, 'throughput': 1230.83}\n",
      "{'loss': 1.3997, 'grad_norm': 1.8097103834152222, 'learning_rate': 4.9930094929529506e-05, 'epoch': 0.71, 'num_input_tokens_seen': 33424}\n",
      "  3%|                                        | 20/630 [00:35<17:38,  1.74s/it][INFO|2025-03-08 20:31:59] llamafactory.train.callbacks:157 >> {'loss': 1.0898, 'learning_rate': 4.9876e-05, 'epoch': 0.94, 'throughput': 1237.29}\n",
      "{'loss': 1.0898, 'grad_norm': 1.3260287046432495, 'learning_rate': 4.987576938413504e-05, 'epoch': 0.94, 'num_input_tokens_seen': 44336}\n",
      "  4%|                                        | 25/630 [00:43<16:04,  1.59s/it][INFO|2025-03-08 20:32:06] llamafactory.train.callbacks:157 >> {'loss': 0.9389, 'learning_rate': 4.9806e-05, 'epoch': 1.14, 'throughput': 1247.58}\n",
      "{'loss': 0.9389, 'grad_norm': 1.0427314043045044, 'learning_rate': 4.9805980165004304e-05, 'epoch': 1.14, 'num_input_tokens_seen': 53856}\n",
      "  5%|                                        | 30/630 [00:51<17:01,  1.70s/it][INFO|2025-03-08 20:32:14] llamafactory.train.callbacks:157 >> {'loss': 0.8547, 'learning_rate': 4.9721e-05, 'epoch': 1.38, 'throughput': 1253.16}\n",
      "{'loss': 0.8547, 'grad_norm': 0.9907586574554443, 'learning_rate': 4.972077065562821e-05, 'epoch': 1.38, 'num_input_tokens_seen': 64912}\n",
      "  6%|                                       | 35/630 [01:00<17:09,  1.73s/it][INFO|2025-03-08 20:32:23] llamafactory.train.callbacks:157 >> {'loss': 0.7819, 'learning_rate': 4.9620e-05, 'epoch': 1.61, 'throughput': 1258.24}\n",
      "{'loss': 0.7819, 'grad_norm': 0.9953262805938721, 'learning_rate': 4.962019382530521e-05, 'epoch': 1.61, 'num_input_tokens_seen': 76096}\n",
      "  6%|                                       | 40/630 [01:09<17:05,  1.74s/it][INFO|2025-03-08 20:32:32] llamafactory.train.callbacks:157 >> {'loss': 0.7571, 'learning_rate': 4.9504e-05, 'epoch': 1.85, 'throughput': 1260.99}\n",
      "{'loss': 0.7571, 'grad_norm': 1.2090213298797607, 'learning_rate': 4.9504312196213596e-05, 'epoch': 1.85, 'num_input_tokens_seen': 87248}\n",
      "  7%|                                       | 45/630 [01:16<14:10,  1.45s/it][INFO|2025-03-08 20:32:39] llamafactory.train.callbacks:157 >> {'loss': 0.6582, 'learning_rate': 4.9373e-05, 'epoch': 2.05, 'throughput': 1261.36}\n",
      "{'loss': 0.6582, 'grad_norm': 1.024832844734192, 'learning_rate': 4.937319780454559e-05, 'epoch': 2.05, 'num_input_tokens_seen': 96528}\n",
      "  8%|                                      | 50/630 [01:25<16:11,  1.68s/it][INFO|2025-03-08 20:32:48] llamafactory.train.callbacks:157 >> {'loss': 0.6554, 'learning_rate': 4.9227e-05, 'epoch': 2.28, 'throughput': 1265.17}\n",
      "{'loss': 0.6554, 'grad_norm': 1.0792315006256104, 'learning_rate': 4.922693215572695e-05, 'epoch': 2.28, 'num_input_tokens_seen': 107744}\n",
      "  9%|                                      | 55/630 [01:33<16:25,  1.71s/it][INFO|2025-03-08 20:32:56] llamafactory.train.callbacks:157 >> {'loss': 0.5564, 'learning_rate': 4.9066e-05, 'epoch': 2.52, 'throughput': 1265.30}\n",
      "{'loss': 0.5564, 'grad_norm': 1.2389589548110962, 'learning_rate': 4.90656061737503e-05, 'epoch': 2.52, 'num_input_tokens_seen': 118672}\n",
      " 10%|                                      | 60/630 [01:42<16:26,  1.73s/it][INFO|2025-03-08 20:33:05] llamafactory.train.callbacks:157 >> {'loss': 0.5221, 'learning_rate': 4.8889e-05, 'epoch': 2.75, 'throughput': 1265.53}\n",
      "{'loss': 0.5221, 'grad_norm': 1.1605894565582275, 'learning_rate': 4.888932014465352e-05, 'epoch': 2.75, 'num_input_tokens_seen': 129696}\n",
      " 10%|                                     | 65/630 [01:51<16:14,  1.72s/it][INFO|2025-03-08 20:33:14] llamafactory.train.callbacks:157 >> {'loss': 0.5042, 'learning_rate': 4.8698e-05, 'epoch': 2.99, 'throughput': 1266.43}\n",
      "{'loss': 0.5042, 'grad_norm': 1.220228672027588, 'learning_rate': 4.86981836541783e-05, 'epoch': 2.99, 'num_input_tokens_seen': 140736}\n",
      " 11%|                                     | 70/630 [01:58<15:14,  1.63s/it][INFO|2025-03-08 20:33:21] llamafactory.train.callbacks:157 >> {'loss': 0.4269, 'learning_rate': 4.8492e-05, 'epoch': 3.19, 'throughput': 1267.59}\n",
      "{'loss': 0.4269, 'grad_norm': 1.2412527799606323, 'learning_rate': 4.849231551964771e-05, 'epoch': 3.19, 'num_input_tokens_seen': 150208}\n",
      " 12%|                                     | 75/630 [02:07<15:48,  1.71s/it][INFO|2025-03-08 20:33:30] llamafactory.train.callbacks:157 >> {'loss': 0.4136, 'learning_rate': 4.8272e-05, 'epoch': 3.42, 'throughput': 1269.78}\n",
      "{'loss': 0.4136, 'grad_norm': 1.154723048210144, 'learning_rate': 4.827184371610511e-05, 'epoch': 3.42, 'num_input_tokens_seen': 161456}\n",
      " 13%|                                    | 80/630 [02:15<15:43,  1.72s/it][INFO|2025-03-08 20:33:38] llamafactory.train.callbacks:157 >> {'loss': 0.3778, 'learning_rate': 4.8037e-05, 'epoch': 3.66, 'throughput': 1270.87}\n",
      "{'loss': 0.3778, 'grad_norm': 1.3820950984954834, 'learning_rate': 4.803690529676019e-05, 'epoch': 3.66, 'num_input_tokens_seen': 172544}\n",
      " 13%|                                    | 85/630 [02:24<15:37,  1.72s/it][INFO|2025-03-08 20:33:47] llamafactory.train.callbacks:157 >> {'loss': 0.3403, 'learning_rate': 4.7788e-05, 'epoch': 3.89, 'throughput': 1271.26}\n",
      "{'loss': 0.3403, 'grad_norm': 1.376813292503357, 'learning_rate': 4.778764630779183e-05, 'epoch': 3.89, 'num_input_tokens_seen': 183552}\n",
      " 14%|                                    | 90/630 [02:31<13:52,  1.54s/it][INFO|2025-03-08 20:33:54] llamafactory.train.callbacks:157 >> {'loss': 0.3085, 'learning_rate': 4.7524e-05, 'epoch': 4.09, 'throughput': 1272.11}\n",
      "{'loss': 0.3085, 'grad_norm': 1.3830397129058838, 'learning_rate': 4.752422169756048e-05, 'epoch': 4.09, 'num_input_tokens_seen': 193072}\n",
      " 15%|                                   | 95/630 [02:40<15:08,  1.70s/it][INFO|2025-03-08 20:34:03] llamafactory.train.callbacks:157 >> {'loss': 0.2231, 'learning_rate': 4.7247e-05, 'epoch': 4.33, 'throughput': 1273.01}\n",
      "{'loss': 0.2231, 'grad_norm': 1.4603729248046875, 'learning_rate': 4.724679522028672e-05, 'epoch': 4.33, 'num_input_tokens_seen': 204256}\n",
      " 16%|                                  | 100/630 [02:49<15:10,  1.72s/it][INFO|2025-03-08 20:34:12] llamafactory.train.callbacks:157 >> {'loss': 0.2591, 'learning_rate': 4.6956e-05, 'epoch': 4.56, 'throughput': 1273.55}\n",
      "{'loss': 0.2591, 'grad_norm': 2.089587688446045, 'learning_rate': 4.6955539334255716e-05, 'epoch': 4.56, 'num_input_tokens_seen': 215312}\n",
      " 16%|                                  | 100/630 [02:49<15:10,  1.72s/it][INFO|trainer.py:3942] 2025-03-08 20:34:12,233 >> Saving model checkpoint to saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-100\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:34:12,725 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:34:12,726 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-08 20:34:12,853 >> tokenizer config file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-08 20:34:12,853 >> Special tokens file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-100/special_tokens_map.json\n",
      " 17%|                                  | 105/630 [02:58<15:44,  1.80s/it][INFO|2025-03-08 20:34:21] llamafactory.train.callbacks:157 >> {'loss': 0.1977, 'learning_rate': 4.6651e-05, 'epoch': 4.80, 'throughput': 1266.37}\n",
      "{'loss': 0.1977, 'grad_norm': 2.368988513946533, 'learning_rate': 4.665063509461097e-05, 'epoch': 4.8, 'num_input_tokens_seen': 226288}\n",
      " 17%|                                 | 110/630 [03:06<11:47,  1.36s/it][INFO|2025-03-08 20:34:29] llamafactory.train.callbacks:157 >> {'loss': 0.2242, 'learning_rate': 4.6332e-05, 'epoch': 5.00, 'throughput': 1267.27}\n",
      "{'loss': 0.2242, 'grad_norm': 4.299772262573242, 'learning_rate': 4.6332272040803895e-05, 'epoch': 5.0, 'num_input_tokens_seen': 235840}\n",
      " 18%|                                 | 115/630 [03:14<14:18,  1.67s/it][INFO|2025-03-08 20:34:37] llamafactory.train.callbacks:157 >> {'loss': 0.1273, 'learning_rate': 4.6001e-05, 'epoch': 5.24, 'throughput': 1268.06}\n",
      "{'loss': 0.1273, 'grad_norm': 1.3841619491577148, 'learning_rate': 4.600064807876929e-05, 'epoch': 5.24, 'num_input_tokens_seen': 246960}\n",
      " 19%|                                 | 120/630 [03:23<14:34,  1.71s/it][INFO|2025-03-08 20:34:46] llamafactory.train.callbacks:157 >> {'loss': 0.1039, 'learning_rate': 4.5656e-05, 'epoch': 5.47, 'throughput': 1268.96}\n",
      "{'loss': 0.1039, 'grad_norm': 1.70705246925354, 'learning_rate': 4.5655969357899874e-05, 'epoch': 5.47, 'num_input_tokens_seen': 258080}\n",
      " 20%|                                | 125/630 [03:32<14:28,  1.72s/it][INFO|2025-03-08 20:34:55] llamafactory.train.callbacks:157 >> {'loss': 0.1102, 'learning_rate': 4.5298e-05, 'epoch': 5.71, 'throughput': 1270.11}\n",
      "{'loss': 0.1102, 'grad_norm': 1.6765825748443604, 'learning_rate': 4.529845014289642e-05, 'epoch': 5.71, 'num_input_tokens_seen': 269264}\n",
      " 21%|                                | 130/630 [03:40<14:19,  1.72s/it][INFO|2025-03-08 20:35:03] llamafactory.train.callbacks:157 >> {'loss': 0.0898, 'learning_rate': 4.4928e-05, 'epoch': 5.94, 'throughput': 1270.22}\n",
      "{'loss': 0.0898, 'grad_norm': 1.6544173955917358, 'learning_rate': 4.4928312680573064e-05, 'epoch': 5.94, 'num_input_tokens_seen': 280240}\n",
      " 21%|                                | 135/630 [03:47<13:08,  1.59s/it][INFO|2025-03-08 20:35:11] llamafactory.train.callbacks:157 >> {'loss': 0.0591, 'learning_rate': 4.4546e-05, 'epoch': 6.14, 'throughput': 1270.50}\n",
      "{'loss': 0.0591, 'grad_norm': 1.0574060678482056, 'learning_rate': 4.454578706170075e-05, 'epoch': 6.14, 'num_input_tokens_seen': 289664}\n",
      " 22%|                                | 140/630 [03:56<13:56,  1.71s/it][INFO|2025-03-08 20:35:19] llamafactory.train.callbacks:157 >> {'loss': 0.0419, 'learning_rate': 4.4151e-05, 'epoch': 6.38, 'throughput': 1270.16}\n",
      "{'loss': 0.0419, 'grad_norm': 1.4408985376358032, 'learning_rate': 4.415111107797445e-05, 'epoch': 6.38, 'num_input_tokens_seen': 300576}\n",
      " 23%|                               | 145/630 [04:05<13:57,  1.73s/it][INFO|2025-03-08 20:35:28] llamafactory.train.callbacks:157 >> {'loss': 0.0312, 'learning_rate': 4.3745e-05, 'epoch': 6.61, 'throughput': 1271.76}\n",
      "{'loss': 0.0312, 'grad_norm': 1.7244898080825806, 'learning_rate': 4.374453007419336e-05, 'epoch': 6.61, 'num_input_tokens_seen': 311952}\n",
      " 24%|                               | 150/630 [04:13<13:46,  1.72s/it][INFO|2025-03-08 20:35:37] llamafactory.train.callbacks:157 >> {'loss': 0.0278, 'learning_rate': 4.3326e-05, 'epoch': 6.85, 'throughput': 1272.09}\n",
      "{'loss': 0.0278, 'grad_norm': 1.6768300533294678, 'learning_rate': 4.332629679574566e-05, 'epoch': 6.85, 'num_input_tokens_seen': 323008}\n",
      " 25%|                               | 155/630 [04:21<11:32,  1.46s/it][INFO|2025-03-08 20:35:44] llamafactory.train.callbacks:157 >> {'loss': 0.0460, 'learning_rate': 4.2897e-05, 'epoch': 7.05, 'throughput': 1272.28}\n",
      "{'loss': 0.046, 'grad_norm': 1.3096930980682373, 'learning_rate': 4.2896671231492966e-05, 'epoch': 7.05, 'num_input_tokens_seen': 332432}\n",
      " 25%|                              | 160/630 [04:29<13:13,  1.69s/it][INFO|2025-03-08 20:35:53] llamafactory.train.callbacks:157 >> {'loss': 0.0218, 'learning_rate': 4.2456e-05, 'epoch': 7.28, 'throughput': 1272.86}\n",
      "{'loss': 0.0218, 'grad_norm': 1.5798274278640747, 'learning_rate': 4.245592045215182e-05, 'epoch': 7.28, 'num_input_tokens_seen': 343616}\n",
      " 26%|                              | 165/630 [04:38<13:19,  1.72s/it][INFO|2025-03-08 20:36:01] llamafactory.train.callbacks:157 >> {'loss': 0.0258, 'learning_rate': 4.2004e-05, 'epoch': 7.52, 'throughput': 1272.95}\n",
      "{'loss': 0.0258, 'grad_norm': 1.666565179824829, 'learning_rate': 4.2004318444272985e-05, 'epoch': 7.52, 'num_input_tokens_seen': 354640}\n",
      " 27%|                              | 170/630 [04:47<13:15,  1.73s/it][INFO|2025-03-08 20:36:10] llamafactory.train.callbacks:157 >> {'loss': 0.0234, 'learning_rate': 4.1542e-05, 'epoch': 7.75, 'throughput': 1272.93}\n",
      "{'loss': 0.0234, 'grad_norm': 1.5033131837844849, 'learning_rate': 4.154214593992149e-05, 'epoch': 7.75, 'num_input_tokens_seen': 365648}\n",
      " 28%|                             | 175/630 [04:55<13:03,  1.72s/it][INFO|2025-03-08 20:36:19] llamafactory.train.callbacks:157 >> {'loss': 0.0232, 'learning_rate': 4.1070e-05, 'epoch': 7.99, 'throughput': 1273.60}\n",
      "{'loss': 0.0232, 'grad_norm': 1.0743577480316162, 'learning_rate': 4.1069690242163484e-05, 'epoch': 7.99, 'num_input_tokens_seen': 376784}\n",
      " 29%|                             | 180/630 [05:03<12:15,  1.63s/it][INFO|2025-03-08 20:36:26] llamafactory.train.callbacks:157 >> {'loss': 0.0149, 'learning_rate': 4.0587e-05, 'epoch': 8.19, 'throughput': 1273.60}\n",
      "{'loss': 0.0149, 'grad_norm': 0.5421279072761536, 'learning_rate': 4.058724504646834e-05, 'epoch': 8.19, 'num_input_tokens_seen': 386176}\n",
      " 29%|                             | 185/630 [05:11<12:40,  1.71s/it][INFO|2025-03-08 20:36:35] llamafactory.train.callbacks:157 >> {'loss': 0.0170, 'learning_rate': 4.0095e-05, 'epoch': 8.42, 'throughput': 1273.37}\n",
      "{'loss': 0.017, 'grad_norm': 1.239676594734192, 'learning_rate': 4.009511025813694e-05, 'epoch': 8.42, 'num_input_tokens_seen': 397104}\n",
      " 30%|                            | 190/630 [05:20<12:38,  1.72s/it][INFO|2025-03-08 20:36:43] llamafactory.train.callbacks:157 >> {'loss': 0.0195, 'learning_rate': 3.9594e-05, 'epoch': 8.66, 'throughput': 1273.34}\n",
      "{'loss': 0.0195, 'grad_norm': 1.1544581651687622, 'learning_rate': 3.959359180586975e-05, 'epoch': 8.66, 'num_input_tokens_seen': 408112}\n",
      " 31%|                            | 195/630 [05:29<12:36,  1.74s/it][INFO|2025-03-08 20:36:52] llamafactory.train.callbacks:157 >> {'loss': 0.0202, 'learning_rate': 3.9083e-05, 'epoch': 8.89, 'throughput': 1273.54}\n",
      "{'loss': 0.0202, 'grad_norm': 0.9573498368263245, 'learning_rate': 3.908300145159055e-05, 'epoch': 8.89, 'num_input_tokens_seen': 419264}\n",
      " 32%|                            | 200/630 [05:36<11:03,  1.54s/it][INFO|2025-03-08 20:36:59] llamafactory.train.callbacks:157 >> {'loss': 0.0108, 'learning_rate': 3.8564e-05, 'epoch': 9.09, 'throughput': 1273.64}\n",
      "{'loss': 0.0108, 'grad_norm': 0.7413063645362854, 'learning_rate': 3.856365659664399e-05, 'epoch': 9.09, 'num_input_tokens_seen': 428704}\n",
      " 32%|                            | 200/630 [05:36<11:03,  1.54s/it][INFO|trainer.py:3942] 2025-03-08 20:36:59,765 >> Saving model checkpoint to saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-200\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:37:00,250 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:37:00,251 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-08 20:37:00,368 >> tokenizer config file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-08 20:37:00,368 >> Special tokens file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-200/special_tokens_map.json\n",
      " 33%|                           | 205/630 [05:46<12:30,  1.77s/it][INFO|2025-03-08 20:37:09] llamafactory.train.callbacks:157 >> {'loss': 0.0106, 'learning_rate': 3.8036e-05, 'epoch': 9.33, 'throughput': 1270.72}\n",
      "{'loss': 0.0106, 'grad_norm': 0.27612730860710144, 'learning_rate': 3.803588008448745e-05, 'epoch': 9.33, 'num_input_tokens_seen': 439872}\n",
      " 33%|                           | 210/630 [05:54<12:11,  1.74s/it][INFO|2025-03-08 20:37:17] llamafactory.train.callbacks:157 >> {'loss': 0.0125, 'learning_rate': 3.7500e-05, 'epoch': 9.56, 'throughput': 1270.44}\n",
      "{'loss': 0.0125, 'grad_norm': 2.3768255710601807, 'learning_rate': 3.7500000000000003e-05, 'epoch': 9.56, 'num_input_tokens_seen': 450784}\n",
      " 34%|                           | 215/630 [06:03<12:13,  1.77s/it][INFO|2025-03-08 20:37:26] llamafactory.train.callbacks:157 >> {'loss': 0.0082, 'learning_rate': 3.6956e-05, 'epoch': 9.80, 'throughput': 1270.09}\n",
      "{'loss': 0.0082, 'grad_norm': 0.388851523399353, 'learning_rate': 3.695634946553296e-05, 'epoch': 9.8, 'num_input_tokens_seen': 461872}\n",
      " 35%|                          | 220/630 [06:11<09:13,  1.35s/it][INFO|2025-03-08 20:37:34] llamafactory.train.callbacks:157 >> {'loss': 0.0060, 'learning_rate': 3.6405e-05, 'epoch': 10.00, 'throughput': 1270.59}\n",
      "{'loss': 0.006, 'grad_norm': 0.36168360710144043, 'learning_rate': 3.6405266433829075e-05, 'epoch': 10.0, 'num_input_tokens_seen': 471440}\n",
      " 36%|                          | 225/630 [06:19<11:16,  1.67s/it][INFO|2025-03-08 20:37:42] llamafactory.train.callbacks:157 >> {'loss': 0.0048, 'learning_rate': 3.5847e-05, 'epoch': 10.24, 'throughput': 1270.49}\n",
      "{'loss': 0.0048, 'grad_norm': 0.08878426998853683, 'learning_rate': 3.5847093477938956e-05, 'epoch': 10.24, 'num_input_tokens_seen': 482448}\n",
      " 37%|                          | 230/630 [06:28<11:26,  1.72s/it][INFO|2025-03-08 20:37:51] llamafactory.train.callbacks:157 >> {'loss': 0.0062, 'learning_rate': 3.5282e-05, 'epoch': 10.47, 'throughput': 1270.89}\n",
      "{'loss': 0.0062, 'grad_norm': 0.5010160207748413, 'learning_rate': 3.5282177578265296e-05, 'epoch': 10.47, 'num_input_tokens_seen': 493600}\n",
      " 37%|                         | 235/630 [06:37<11:25,  1.73s/it][INFO|2025-03-08 20:38:00] llamafactory.train.callbacks:157 >> {'loss': 0.0047, 'learning_rate': 3.4711e-05, 'epoch': 10.71, 'throughput': 1270.75}\n",
      "{'loss': 0.0047, 'grad_norm': 1.3019115924835205, 'learning_rate': 3.471086990686737e-05, 'epoch': 10.71, 'num_input_tokens_seen': 504608}\n",
      " 38%|                         | 240/630 [06:45<11:14,  1.73s/it][INFO|2025-03-08 20:38:08] llamafactory.train.callbacks:157 >> {'loss': 0.0067, 'learning_rate': 3.4134e-05, 'epoch': 10.94, 'throughput': 1270.89}\n",
      "{'loss': 0.0067, 'grad_norm': 1.1898729801177979, 'learning_rate': 3.413352560915988e-05, 'epoch': 10.94, 'num_input_tokens_seen': 515680}\n",
      " 39%|                         | 245/630 [06:53<10:36,  1.65s/it][INFO|2025-03-08 20:38:16] llamafactory.train.callbacks:157 >> {'loss': 0.0081, 'learning_rate': 3.3551e-05, 'epoch': 11.14, 'throughput': 1270.78}\n",
      "{'loss': 0.0081, 'grad_norm': 0.4410153329372406, 'learning_rate': 3.355050358314172e-05, 'epoch': 11.14, 'num_input_tokens_seen': 525264}\n",
      " 40%|                        | 250/630 [07:01<10:52,  1.72s/it][INFO|2025-03-08 20:38:25] llamafactory.train.callbacks:157 >> {'loss': 0.0065, 'learning_rate': 3.2962e-05, 'epoch': 11.38, 'throughput': 1270.90}\n",
      "{'loss': 0.0065, 'grad_norm': 0.14897437393665314, 'learning_rate': 3.2962166256292113e-05, 'epoch': 11.38, 'num_input_tokens_seen': 536320}\n",
      " 40%|                        | 255/630 [07:10<10:46,  1.72s/it][INFO|2025-03-08 20:38:33] llamafactory.train.callbacks:157 >> {'loss': 0.0039, 'learning_rate': 3.2369e-05, 'epoch': 11.61, 'throughput': 1270.95}\n",
      "{'loss': 0.0039, 'grad_norm': 0.4796214997768402, 'learning_rate': 3.2368879360272606e-05, 'epoch': 11.61, 'num_input_tokens_seen': 547328}\n",
      " 41%|                        | 260/630 [07:19<10:41,  1.74s/it][INFO|2025-03-08 20:38:42] llamafactory.train.callbacks:157 >> {'loss': 0.0038, 'learning_rate': 3.1771e-05, 'epoch': 11.85, 'throughput': 1271.43}\n",
      "{'loss': 0.0038, 'grad_norm': 0.23520857095718384, 'learning_rate': 3.177101170357513e-05, 'epoch': 11.85, 'num_input_tokens_seen': 558576}\n",
      " 42%|                       | 265/630 [07:26<08:52,  1.46s/it][INFO|2025-03-08 20:38:49] llamafactory.train.callbacks:157 >> {'loss': 0.0056, 'learning_rate': 3.1169e-05, 'epoch': 12.05, 'throughput': 1271.53}\n",
      "{'loss': 0.0056, 'grad_norm': 0.10652504861354828, 'learning_rate': 3.116893494225734e-05, 'epoch': 12.05, 'num_input_tokens_seen': 567984}\n",
      " 43%|                       | 270/630 [07:35<10:08,  1.69s/it][INFO|2025-03-08 20:38:58] llamafactory.train.callbacks:157 >> {'loss': 0.0042, 'learning_rate': 3.0563e-05, 'epoch': 12.28, 'throughput': 1271.66}\n",
      "{'loss': 0.0042, 'grad_norm': 0.1726974993944168, 'learning_rate': 3.056302334890786e-05, 'epoch': 12.28, 'num_input_tokens_seen': 579104}\n",
      " 44%|                       | 275/630 [07:44<10:14,  1.73s/it][INFO|2025-03-08 20:39:07] llamafactory.train.callbacks:157 >> {'loss': 0.0073, 'learning_rate': 2.9954e-05, 'epoch': 12.52, 'throughput': 1271.63}\n",
      "{'loss': 0.0073, 'grad_norm': 0.7425929307937622, 'learning_rate': 2.9953653579984942e-05, 'epoch': 12.52, 'num_input_tokens_seen': 590160}\n",
      " 44%|                      | 280/630 [07:52<10:08,  1.74s/it][INFO|2025-03-08 20:39:15] llamafactory.train.callbacks:157 >> {'loss': 0.0047, 'learning_rate': 2.9341e-05, 'epoch': 12.75, 'throughput': 1271.59}\n",
      "{'loss': 0.0047, 'grad_norm': 0.0974057987332344, 'learning_rate': 2.9341204441673266e-05, 'epoch': 12.75, 'num_input_tokens_seen': 601248}\n",
      " 45%|                      | 285/630 [08:01<09:57,  1.73s/it][INFO|2025-03-08 20:39:24] llamafactory.train.callbacks:157 >> {'loss': 0.0042, 'learning_rate': 2.8726e-05, 'epoch': 12.99, 'throughput': 1271.62}\n",
      "{'loss': 0.0042, 'grad_norm': 0.1775149405002594, 'learning_rate': 2.872605665440436e-05, 'epoch': 12.99, 'num_input_tokens_seen': 612272}\n",
      " 46%|                      | 290/630 [08:08<09:15,  1.63s/it][INFO|2025-03-08 20:39:32] llamafactory.train.callbacks:157 >> {'loss': 0.0032, 'learning_rate': 2.8109e-05, 'epoch': 13.19, 'throughput': 1272.26}\n",
      "{'loss': 0.0032, 'grad_norm': 0.051279500126838684, 'learning_rate': 2.8108592616187133e-05, 'epoch': 13.19, 'num_input_tokens_seen': 621968}\n",
      " 47%|                     | 295/630 [08:17<09:30,  1.70s/it][INFO|2025-03-08 20:39:40] llamafactory.train.callbacks:157 >> {'loss': 0.0024, 'learning_rate': 2.7489e-05, 'epoch': 13.42, 'throughput': 1272.01}\n",
      "{'loss': 0.0024, 'grad_norm': 0.14169082045555115, 'learning_rate': 2.748919616489542e-05, 'epoch': 13.42, 'num_input_tokens_seen': 632816}\n",
      " 48%|                     | 300/630 [08:26<09:25,  1.71s/it][INFO|2025-03-08 20:39:49] llamafactory.train.callbacks:157 >> {'loss': 0.0052, 'learning_rate': 2.6868e-05, 'epoch': 13.66, 'throughput': 1272.62}\n",
      "{'loss': 0.0052, 'grad_norm': 0.8999799489974976, 'learning_rate': 2.686825233966061e-05, 'epoch': 13.66, 'num_input_tokens_seen': 644064}\n",
      " 48%|                     | 300/630 [08:26<09:25,  1.71s/it][INFO|trainer.py:3942] 2025-03-08 20:39:49,262 >> Saving model checkpoint to saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-300\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:39:50,152 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:39:50,153 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-08 20:39:50,269 >> tokenizer config file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-08 20:39:50,269 >> Special tokens file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-300/special_tokens_map.json\n",
      " 48%|                     | 305/630 [08:35<09:46,  1.80s/it][INFO|2025-03-08 20:39:59] llamafactory.train.callbacks:157 >> {'loss': 0.0044, 'learning_rate': 2.6246e-05, 'epoch': 13.89, 'throughput': 1269.59}\n",
      "{'loss': 0.0044, 'grad_norm': 0.5364619493484497, 'learning_rate': 2.624614714151743e-05, 'epoch': 13.89, 'num_input_tokens_seen': 655056}\n",
      " 49%|                    | 310/630 [08:43<08:14,  1.55s/it][INFO|2025-03-08 20:40:06] llamafactory.train.callbacks:157 >> {'loss': 0.0041, 'learning_rate': 2.5623e-05, 'epoch': 14.09, 'throughput': 1269.93}\n",
      "{'loss': 0.0041, 'grad_norm': 0.04178781434893608, 'learning_rate': 2.5623267293451826e-05, 'epoch': 14.09, 'num_input_tokens_seen': 664544}\n",
      " 50%|                    | 315/630 [08:51<08:52,  1.69s/it][INFO|2025-03-08 20:40:15] llamafactory.train.callbacks:157 >> {'loss': 0.0019, 'learning_rate': 2.5000e-05, 'epoch': 14.33, 'throughput': 1269.83}\n",
      "{'loss': 0.0019, 'grad_norm': 0.11028875410556793, 'learning_rate': 2.5e-05, 'epoch': 14.33, 'num_input_tokens_seen': 675440}\n",
      " 51%|                    | 320/630 [09:00<08:51,  1.71s/it][INFO|2025-03-08 20:40:23] llamafactory.train.callbacks:157 >> {'loss': 0.0023, 'learning_rate': 2.4377e-05, 'epoch': 14.56, 'throughput': 1270.21}\n",
      "{'loss': 0.0023, 'grad_norm': 0.09418242424726486, 'learning_rate': 2.4376732706548183e-05, 'epoch': 14.56, 'num_input_tokens_seen': 686560}\n",
      " 52%|                   | 325/630 [09:09<08:41,  1.71s/it][INFO|2025-03-08 20:40:32] llamafactory.train.callbacks:157 >> {'loss': 0.0032, 'learning_rate': 2.3754e-05, 'epoch': 14.80, 'throughput': 1270.84}\n",
      "{'loss': 0.0032, 'grad_norm': 0.329816609621048, 'learning_rate': 2.375385285848257e-05, 'epoch': 14.8, 'num_input_tokens_seen': 697776}\n",
      " 52%|                   | 330/630 [09:16<06:43,  1.35s/it][INFO|2025-03-08 20:40:39] llamafactory.train.callbacks:157 >> {'loss': 0.0029, 'learning_rate': 2.3132e-05, 'epoch': 15.00, 'throughput': 1271.01}\n",
      "{'loss': 0.0029, 'grad_norm': 0.20883709192276, 'learning_rate': 2.3131747660339394e-05, 'epoch': 15.0, 'num_input_tokens_seen': 707264}\n",
      " 53%|                   | 335/630 [09:25<08:07,  1.65s/it][INFO|2025-03-08 20:40:48] llamafactory.train.callbacks:157 >> {'loss': 0.0021, 'learning_rate': 2.2511e-05, 'epoch': 15.24, 'throughput': 1271.03}\n",
      "{'loss': 0.0021, 'grad_norm': 0.17239868640899658, 'learning_rate': 2.251080383510459e-05, 'epoch': 15.24, 'num_input_tokens_seen': 718208}\n",
      " 54%|                  | 340/630 [09:33<08:14,  1.71s/it][INFO|2025-03-08 20:40:56] llamafactory.train.callbacks:157 >> {'loss': 0.0030, 'learning_rate': 2.1891e-05, 'epoch': 15.47, 'throughput': 1271.42}\n",
      "{'loss': 0.003, 'grad_norm': 0.02953590266406536, 'learning_rate': 2.189140738381288e-05, 'epoch': 15.47, 'num_input_tokens_seen': 729360}\n",
      " 55%|                  | 345/630 [09:42<08:09,  1.72s/it][INFO|2025-03-08 20:41:05] llamafactory.train.callbacks:157 >> {'loss': 0.0033, 'learning_rate': 2.1274e-05, 'epoch': 15.71, 'throughput': 1271.74}\n",
      "{'loss': 0.0033, 'grad_norm': 0.052054136991500854, 'learning_rate': 2.1273943345595637e-05, 'epoch': 15.71, 'num_input_tokens_seen': 740496}\n",
      " 56%|                  | 350/630 [09:50<08:00,  1.72s/it][INFO|2025-03-08 20:41:14] llamafactory.train.callbacks:157 >> {'loss': 0.0024, 'learning_rate': 2.0659e-05, 'epoch': 15.94, 'throughput': 1272.03}\n",
      "{'loss': 0.0024, 'grad_norm': 0.13147161900997162, 'learning_rate': 2.0658795558326743e-05, 'epoch': 15.94, 'num_input_tokens_seen': 751600}\n",
      " 56%|                  | 355/630 [09:58<07:16,  1.59s/it][INFO|2025-03-08 20:41:21] llamafactory.train.callbacks:157 >> {'loss': 0.0026, 'learning_rate': 2.0046e-05, 'epoch': 16.14, 'throughput': 1271.98}\n",
      "{'loss': 0.0026, 'grad_norm': 0.15247400104999542, 'learning_rate': 2.0046346420015067e-05, 'epoch': 16.14, 'num_input_tokens_seen': 760896}\n",
      " 57%|                 | 360/630 [10:06<07:36,  1.69s/it][INFO|2025-03-08 20:41:29] llamafactory.train.callbacks:157 >> {'loss': 0.0026, 'learning_rate': 1.9437e-05, 'epoch': 16.38, 'throughput': 1272.36}\n",
      "{'loss': 0.0026, 'grad_norm': 0.02506321668624878, 'learning_rate': 1.9436976651092144e-05, 'epoch': 16.38, 'num_input_tokens_seen': 772048}\n",
      " 58%|                 | 365/630 [10:15<07:32,  1.71s/it][INFO|2025-03-08 20:41:38] llamafactory.train.callbacks:157 >> {'loss': 0.0028, 'learning_rate': 1.8831e-05, 'epoch': 16.61, 'throughput': 1272.71}\n",
      "{'loss': 0.0028, 'grad_norm': 0.27298083901405334, 'learning_rate': 1.8831065057742657e-05, 'epoch': 16.61, 'num_input_tokens_seen': 783184}\n",
      " 59%|                 | 370/630 [10:23<07:24,  1.71s/it][INFO|2025-03-08 20:41:47] llamafactory.train.callbacks:157 >> {'loss': 0.0015, 'learning_rate': 1.8229e-05, 'epoch': 16.85, 'throughput': 1273.10}\n",
      "{'loss': 0.0015, 'grad_norm': 0.14990279078483582, 'learning_rate': 1.8228988296424877e-05, 'epoch': 16.85, 'num_input_tokens_seen': 794336}\n",
      " 60%|                | 375/630 [10:31<06:08,  1.45s/it][INFO|2025-03-08 20:41:54] llamafactory.train.callbacks:157 >> {'loss': 0.0017, 'learning_rate': 1.7631e-05, 'epoch': 17.05, 'throughput': 1273.15}\n",
      "{'loss': 0.0017, 'grad_norm': 0.05996917560696602, 'learning_rate': 1.7631120639727393e-05, 'epoch': 17.05, 'num_input_tokens_seen': 803680}\n",
      " 60%|                | 380/630 [10:39<06:57,  1.67s/it][INFO|2025-03-08 20:42:02] llamafactory.train.callbacks:157 >> {'loss': 0.0025, 'learning_rate': 1.7038e-05, 'epoch': 17.28, 'throughput': 1273.65}\n",
      "{'loss': 0.0025, 'grad_norm': 0.06810235977172852, 'learning_rate': 1.7037833743707892e-05, 'epoch': 17.28, 'num_input_tokens_seen': 814896}\n",
      " 61%|                | 385/630 [10:48<06:57,  1.71s/it][INFO|2025-03-08 20:42:11] llamafactory.train.callbacks:157 >> {'loss': 0.0012, 'learning_rate': 1.6449e-05, 'epoch': 17.52, 'throughput': 1274.31}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0200089979916811, 'learning_rate': 1.6449496416858284e-05, 'epoch': 17.52, 'num_input_tokens_seen': 826240}\n",
      " 62%|               | 390/630 [10:56<06:51,  1.71s/it][INFO|2025-03-08 20:42:20] llamafactory.train.callbacks:157 >> {'loss': 0.0017, 'learning_rate': 1.5866e-05, 'epoch': 17.75, 'throughput': 1274.09}\n",
      "{'loss': 0.0017, 'grad_norm': 0.17551763355731964, 'learning_rate': 1.5866474390840125e-05, 'epoch': 17.75, 'num_input_tokens_seen': 837040}\n",
      " 63%|               | 395/630 [11:05<06:42,  1.71s/it][INFO|2025-03-08 20:42:28] llamafactory.train.callbacks:157 >> {'loss': 0.0014, 'learning_rate': 1.5289e-05, 'epoch': 17.99, 'throughput': 1274.48}\n",
      "{'loss': 0.0014, 'grad_norm': 0.08478059619665146, 'learning_rate': 1.5289130093132632e-05, 'epoch': 17.99, 'num_input_tokens_seen': 848208}\n",
      " 63%|               | 400/630 [11:12<06:13,  1.62s/it][INFO|2025-03-08 20:42:36] llamafactory.train.callbacks:157 >> {'loss': 0.0015, 'learning_rate': 1.4718e-05, 'epoch': 18.19, 'throughput': 1274.59}\n",
      "{'loss': 0.0015, 'grad_norm': 0.09761931747198105, 'learning_rate': 1.4717822421734718e-05, 'epoch': 18.19, 'num_input_tokens_seen': 857632}\n",
      " 63%|               | 400/630 [11:12<06:13,  1.62s/it][INFO|trainer.py:3942] 2025-03-08 20:42:36,037 >> Saving model checkpoint to saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-400\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:42:36,506 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:42:36,508 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-08 20:42:36,633 >> tokenizer config file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-08 20:42:36,633 >> Special tokens file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-400/special_tokens_map.json\n",
      " 64%|              | 405/630 [11:22<06:37,  1.77s/it][INFO|2025-03-08 20:42:45] llamafactory.train.callbacks:157 >> {'loss': 0.0017, 'learning_rate': 1.4153e-05, 'epoch': 18.42, 'throughput': 1273.27}\n",
      "{'loss': 0.0017, 'grad_norm': 0.06061822175979614, 'learning_rate': 1.4152906522061048e-05, 'epoch': 18.42, 'num_input_tokens_seen': 868864}\n",
      " 65%|              | 410/630 [11:30<06:20,  1.73s/it][INFO|2025-03-08 20:42:54] llamafactory.train.callbacks:157 >> {'loss': 0.0014, 'learning_rate': 1.3595e-05, 'epoch': 18.66, 'throughput': 1273.52}\n",
      "{'loss': 0.0014, 'grad_norm': 0.023283952847123146, 'learning_rate': 1.3594733566170926e-05, 'epoch': 18.66, 'num_input_tokens_seen': 880000}\n",
      " 66%|              | 415/630 [11:39<06:10,  1.72s/it][INFO|2025-03-08 20:43:02] llamafactory.train.callbacks:157 >> {'loss': 0.0013, 'learning_rate': 1.3044e-05, 'epoch': 18.89, 'throughput': 1273.48}\n",
      "{'loss': 0.0013, 'grad_norm': 0.13078150153160095, 'learning_rate': 1.3043650534467053e-05, 'epoch': 18.89, 'num_input_tokens_seen': 891008}\n",
      " 67%|             | 420/630 [11:46<05:21,  1.53s/it][INFO|2025-03-08 20:43:10] llamafactory.train.callbacks:157 >> {'loss': 0.0018, 'learning_rate': 1.2500e-05, 'epoch': 19.09, 'throughput': 1273.66}\n",
      "{'loss': 0.0018, 'grad_norm': 0.016516495496034622, 'learning_rate': 1.2500000000000006e-05, 'epoch': 19.09, 'num_input_tokens_seen': 900448}\n",
      " 67%|             | 425/630 [11:55<05:45,  1.69s/it][INFO|2025-03-08 20:43:18] llamafactory.train.callbacks:157 >> {'loss': 0.0009, 'learning_rate': 1.1964e-05, 'epoch': 19.33, 'throughput': 1274.19}\n",
      "{'loss': 0.0009, 'grad_norm': 0.07581782341003418, 'learning_rate': 1.196411991551255e-05, 'epoch': 19.33, 'num_input_tokens_seen': 911760}\n",
      " 68%|             | 430/630 [12:04<05:42,  1.71s/it][INFO|2025-03-08 20:43:27] llamafactory.train.callbacks:157 >> {'loss': 0.0014, 'learning_rate': 1.1436e-05, 'epoch': 19.56, 'throughput': 1274.49}\n",
      "{'loss': 0.0014, 'grad_norm': 0.1069764569401741, 'learning_rate': 1.1436343403356017e-05, 'epoch': 19.56, 'num_input_tokens_seen': 922944}\n",
      " 69%|            | 435/630 [12:12<05:35,  1.72s/it][INFO|2025-03-08 20:43:35] llamafactory.train.callbacks:157 >> {'loss': 0.0021, 'learning_rate': 1.0917e-05, 'epoch': 19.80, 'throughput': 1274.53}\n",
      "{'loss': 0.0021, 'grad_norm': 0.26494741439819336, 'learning_rate': 1.0916998548409449e-05, 'epoch': 19.8, 'num_input_tokens_seen': 933984}\n",
      " 70%|            | 440/630 [12:20<04:13,  1.34s/it][INFO|2025-03-08 20:43:43] llamafactory.train.callbacks:157 >> {'loss': 0.0017, 'learning_rate': 1.0406e-05, 'epoch': 20.00, 'throughput': 1274.43}\n",
      "{'loss': 0.0017, 'grad_norm': 0.03752429783344269, 'learning_rate': 1.0406408194130259e-05, 'epoch': 20.0, 'num_input_tokens_seen': 943248}\n",
      " 71%|            | 445/630 [12:28<05:04,  1.65s/it][INFO|2025-03-08 20:43:51] llamafactory.train.callbacks:157 >> {'loss': 0.0019, 'learning_rate': 9.9049e-06, 'epoch': 20.24, 'throughput': 1274.77}\n",
      "{'loss': 0.0019, 'grad_norm': 0.15287873148918152, 'learning_rate': 9.90488974186306e-06, 'epoch': 20.24, 'num_input_tokens_seen': 954416}\n",
      " 71%|           | 450/630 [12:37<05:11,  1.73s/it][INFO|2025-03-08 20:44:00] llamafactory.train.callbacks:157 >> {'loss': 0.0013, 'learning_rate': 9.4128e-06, 'epoch': 20.47, 'throughput': 1274.55}\n",
      "{'loss': 0.0013, 'grad_norm': 0.1124785766005516, 'learning_rate': 9.412754953531663e-06, 'epoch': 20.47, 'num_input_tokens_seen': 965520}\n",
      " 72%|           | 455/630 [12:46<05:01,  1.72s/it][INFO|2025-03-08 20:44:09] llamafactory.train.callbacks:157 >> {'loss': 0.0014, 'learning_rate': 8.9303e-06, 'epoch': 20.71, 'throughput': 1274.69}\n",
      "{'loss': 0.0014, 'grad_norm': 0.019292015582323074, 'learning_rate': 8.930309757836517e-06, 'epoch': 20.71, 'num_input_tokens_seen': 976624}\n",
      " 73%|           | 460/630 [12:54<04:50,  1.71s/it][INFO|2025-03-08 20:44:17] llamafactory.train.callbacks:157 >> {'loss': 0.0016, 'learning_rate': 8.4579e-06, 'epoch': 20.94, 'throughput': 1274.80}\n",
      "{'loss': 0.0016, 'grad_norm': 0.09579281508922577, 'learning_rate': 8.45785406007852e-06, 'epoch': 20.94, 'num_input_tokens_seen': 987600}\n",
      " 74%|          | 465/630 [13:02<04:33,  1.66s/it][INFO|2025-03-08 20:44:25] llamafactory.train.callbacks:157 >> {'loss': 0.0010, 'learning_rate': 7.9957e-06, 'epoch': 21.14, 'throughput': 1274.00}\n",
      "{'loss': 0.001, 'grad_norm': 0.13174644112586975, 'learning_rate': 7.99568155572701e-06, 'epoch': 21.14, 'num_input_tokens_seen': 996880}\n",
      " 75%|          | 470/630 [13:11<04:33,  1.71s/it][INFO|2025-03-08 20:44:34] llamafactory.train.callbacks:157 >> {'loss': 0.0015, 'learning_rate': 7.5441e-06, 'epoch': 21.38, 'throughput': 1274.19}\n",
      "{'loss': 0.0015, 'grad_norm': 0.0977935642004013, 'learning_rate': 7.5440795478481815e-06, 'epoch': 21.38, 'num_input_tokens_seen': 1008000}\n",
      " 75%|          | 475/630 [13:19<04:25,  1.71s/it][INFO|2025-03-08 20:44:42] llamafactory.train.callbacks:157 >> {'loss': 0.0014, 'learning_rate': 7.1033e-06, 'epoch': 21.61, 'throughput': 1274.39}\n",
      "{'loss': 0.0014, 'grad_norm': 0.07388658821582794, 'learning_rate': 7.103328768507039e-06, 'epoch': 21.61, 'num_input_tokens_seen': 1019072}\n",
      " 76%|         | 480/630 [13:28<04:17,  1.72s/it][INFO|2025-03-08 20:44:51] llamafactory.train.callbacks:157 >> {'loss': 0.0019, 'learning_rate': 6.6737e-06, 'epoch': 21.85, 'throughput': 1274.48}\n",
      "{'loss': 0.0019, 'grad_norm': 0.15483370423316956, 'learning_rate': 6.673703204254347e-06, 'epoch': 21.85, 'num_input_tokens_seen': 1030096}\n",
      " 77%|         | 485/630 [13:35<03:29,  1.44s/it][INFO|2025-03-08 20:44:58] llamafactory.train.callbacks:157 >> {'loss': 0.0010, 'learning_rate': 6.2555e-06, 'epoch': 22.05, 'throughput': 1274.92}\n",
      "{'loss': 0.001, 'grad_norm': 0.15039624273777008, 'learning_rate': 6.255469925806643e-06, 'epoch': 22.05, 'num_input_tokens_seen': 1039728}\n",
      " 78%|         | 490/630 [13:44<03:53,  1.67s/it][INFO|2025-03-08 20:45:07] llamafactory.train.callbacks:157 >> {'loss': 0.0009, 'learning_rate': 5.8489e-06, 'epoch': 22.28, 'throughput': 1275.15}\n",
      "{'loss': 0.0009, 'grad_norm': 0.019171500578522682, 'learning_rate': 5.848888922025553e-06, 'epoch': 22.28, 'num_input_tokens_seen': 1050880}\n",
      " 79%|        | 495/630 [13:52<03:49,  1.70s/it][INFO|2025-03-08 20:45:15] llamafactory.train.callbacks:157 >> {'loss': 0.0013, 'learning_rate': 5.4542e-06, 'epoch': 22.52, 'throughput': 1275.46}\n",
      "{'loss': 0.0013, 'grad_norm': 0.08933424204587936, 'learning_rate': 5.454212938299255e-06, 'epoch': 22.52, 'num_input_tokens_seen': 1062048}\n",
      " 79%|        | 500/630 [14:01<03:44,  1.73s/it][INFO|2025-03-08 20:45:24] llamafactory.train.callbacks:157 >> {'loss': 0.0017, 'learning_rate': 5.0717e-06, 'epoch': 22.75, 'throughput': 1275.33}\n",
      "{'loss': 0.0017, 'grad_norm': 0.22073900699615479, 'learning_rate': 5.071687319426946e-06, 'epoch': 22.75, 'num_input_tokens_seen': 1073120}\n",
      " 79%|        | 500/630 [14:01<03:44,  1.73s/it][INFO|trainer.py:3942] 2025-03-08 20:45:24,613 >> Saving model checkpoint to saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-500\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:45:25,482 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:45:25,484 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-08 20:45:25,609 >> tokenizer config file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-08 20:45:25,609 >> Special tokens file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-500/special_tokens_map.json\n",
      " 80%|        | 505/630 [14:11<03:45,  1.80s/it][INFO|2025-03-08 20:45:34] llamafactory.train.callbacks:157 >> {'loss': 0.0012, 'learning_rate': 4.7015e-06, 'epoch': 22.99, 'throughput': 1273.63}\n",
      "{'loss': 0.0012, 'grad_norm': 0.015157852321863174, 'learning_rate': 4.701549857103588e-06, 'epoch': 22.99, 'num_input_tokens_seen': 1084224}\n",
      " 81%|       | 510/630 [14:18<03:15,  1.63s/it][INFO|2025-03-08 20:45:41] llamafactory.train.callbacks:157 >> {'loss': 0.0013, 'learning_rate': 4.3440e-06, 'epoch': 23.19, 'throughput': 1273.75}\n",
      "{'loss': 0.0013, 'grad_norm': 0.09972506016492844, 'learning_rate': 4.344030642100133e-06, 'epoch': 23.19, 'num_input_tokens_seen': 1093616}\n",
      " 82%|       | 515/630 [14:27<03:14,  1.69s/it][INFO|2025-03-08 20:45:50] llamafactory.train.callbacks:157 >> {'loss': 0.0019, 'learning_rate': 3.9994e-06, 'epoch': 23.42, 'throughput': 1273.83}\n",
      "{'loss': 0.0019, 'grad_norm': 0.09632644802331924, 'learning_rate': 3.9993519212307154e-06, 'epoch': 23.42, 'num_input_tokens_seen': 1104560}\n",
      " 83%|       | 520/630 [14:35<03:07,  1.71s/it][INFO|2025-03-08 20:45:58] llamafactory.train.callbacks:157 >> {'loss': 0.0011, 'learning_rate': 3.6677e-06, 'epoch': 23.66, 'throughput': 1274.07}\n",
      "{'loss': 0.0011, 'grad_norm': 0.09618669003248215, 'learning_rate': 3.66772795919611e-06, 'epoch': 23.66, 'num_input_tokens_seen': 1115680}\n",
      " 83%|      | 525/630 [14:44<03:00,  1.72s/it][INFO|2025-03-08 20:46:07] llamafactory.train.callbacks:157 >> {'loss': 0.0014, 'learning_rate': 3.3494e-06, 'epoch': 23.89, 'throughput': 1274.31}\n",
      "{'loss': 0.0014, 'grad_norm': 0.07645117491483688, 'learning_rate': 3.3493649053890326e-06, 'epoch': 23.89, 'num_input_tokens_seen': 1126832}\n",
      " 84%|      | 530/630 [14:51<02:32,  1.52s/it][INFO|2025-03-08 20:46:14] llamafactory.train.callbacks:157 >> {'loss': 0.0012, 'learning_rate': 3.0445e-06, 'epoch': 24.09, 'throughput': 1274.26}\n",
      "{'loss': 0.0012, 'grad_norm': 0.11822141706943512, 'learning_rate': 3.044460665744284e-06, 'epoch': 24.09, 'num_input_tokens_seen': 1136048}\n",
      " 85%|      | 535/630 [15:00<02:39,  1.68s/it][INFO|2025-03-08 20:46:23] llamafactory.train.callbacks:157 >> {'loss': 0.0017, 'learning_rate': 2.7532e-06, 'epoch': 24.33, 'throughput': 1274.38}\n",
      "{'loss': 0.0017, 'grad_norm': 0.08317866921424866, 'learning_rate': 2.7532047797132867e-06, 'epoch': 24.33, 'num_input_tokens_seen': 1147104}\n",
      " 86%|     | 540/630 [15:08<02:33,  1.70s/it][INFO|2025-03-08 20:46:31] llamafactory.train.callbacks:157 >> {'loss': 0.0015, 'learning_rate': 2.4758e-06, 'epoch': 24.56, 'throughput': 1274.53}\n",
      "{'loss': 0.0015, 'grad_norm': 0.02888348139822483, 'learning_rate': 2.475778302439524e-06, 'epoch': 24.56, 'num_input_tokens_seen': 1158160}\n",
      " 87%|     | 545/630 [15:17<02:24,  1.70s/it][INFO|2025-03-08 20:46:40] llamafactory.train.callbacks:157 >> {'loss': 0.0007, 'learning_rate': 2.2124e-06, 'epoch': 24.80, 'throughput': 1274.71}\n",
      "{'loss': 0.0007, 'grad_norm': 0.01364864595234394, 'learning_rate': 2.212353692208172e-06, 'epoch': 24.8, 'num_input_tokens_seen': 1169216}\n",
      " 87%|     | 550/630 [15:24<01:45,  1.32s/it][INFO|2025-03-08 20:46:47] llamafactory.train.callbacks:157 >> {'loss': 0.0013, 'learning_rate': 1.9631e-06, 'epoch': 25.00, 'throughput': 1275.06}\n",
      "{'loss': 0.0013, 'grad_norm': 0.027770454064011574, 'learning_rate': 1.9630947032398067e-06, 'epoch': 25.0, 'num_input_tokens_seen': 1178768}\n",
      " 88%|     | 555/630 [15:33<02:03,  1.64s/it][INFO|2025-03-08 20:46:56] llamafactory.train.callbacks:157 >> {'loss': 0.0014, 'learning_rate': 1.7282e-06, 'epoch': 25.24, 'throughput': 1275.11}\n",
      "{'loss': 0.0014, 'grad_norm': 0.07493856549263, 'learning_rate': 1.7281562838948966e-06, 'epoch': 25.24, 'num_input_tokens_seen': 1189744}\n",
      " 89%|    | 560/630 [15:41<01:58,  1.69s/it][INFO|2025-03-08 20:47:04] llamafactory.train.callbacks:157 >> {'loss': 0.0014, 'learning_rate': 1.5077e-06, 'epoch': 25.47, 'throughput': 1275.57}\n",
      "{'loss': 0.0014, 'grad_norm': 0.09306124597787857, 'learning_rate': 1.5076844803522922e-06, 'epoch': 25.47, 'num_input_tokens_seen': 1201024}\n",
      " 90%|    | 565/630 [15:50<01:50,  1.71s/it][INFO|2025-03-08 20:47:13] llamafactory.train.callbacks:157 >> {'loss': 0.0012, 'learning_rate': 1.3018e-06, 'epoch': 25.71, 'throughput': 1275.83}\n",
      "{'loss': 0.0012, 'grad_norm': 0.022750353440642357, 'learning_rate': 1.3018163458217076e-06, 'epoch': 25.71, 'num_input_tokens_seen': 1212176}\n",
      " 90%|    | 570/630 [15:58<01:42,  1.71s/it][INFO|2025-03-08 20:47:21] llamafactory.train.callbacks:157 >> {'loss': 0.0013, 'learning_rate': 1.1107e-06, 'epoch': 25.94, 'throughput': 1275.82}\n",
      "{'loss': 0.0013, 'grad_norm': 0.1477058231830597, 'learning_rate': 1.1106798553464804e-06, 'epoch': 25.94, 'num_input_tokens_seen': 1223072}\n",
      " 91%|   | 575/630 [16:05<01:26,  1.58s/it][INFO|2025-03-08 20:47:29] llamafactory.train.callbacks:157 >> {'loss': 0.0013, 'learning_rate': 9.3439e-07, 'epoch': 26.14, 'throughput': 1275.83}\n",
      "{'loss': 0.0013, 'grad_norm': 0.10070653259754181, 'learning_rate': 9.343938262496993e-07, 'epoch': 26.14, 'num_input_tokens_seen': 1232384}\n",
      " 92%|   | 580/630 [16:14<01:24,  1.68s/it][INFO|2025-03-08 20:47:37] llamafactory.train.callbacks:157 >> {'loss': 0.0012, 'learning_rate': 7.7307e-07, 'epoch': 26.38, 'throughput': 1276.02}\n",
      "{'loss': 0.0012, 'grad_norm': 0.054480813443660736, 'learning_rate': 7.730678442730538e-07, 'epoch': 26.38, 'num_input_tokens_seen': 1243488}\n",
      " 93%|   | 585/630 [16:23<01:16,  1.71s/it][INFO|2025-03-08 20:47:46] llamafactory.train.callbacks:157 >> {'loss': 0.0014, 'learning_rate': 6.2680e-07, 'epoch': 26.61, 'throughput': 1276.05}\n",
      "{'loss': 0.0014, 'grad_norm': 0.13926231861114502, 'learning_rate': 6.268021954544096e-07, 'epoch': 26.61, 'num_input_tokens_seen': 1254464}\n",
      " 94%|  | 590/630 [16:31<01:08,  1.70s/it][INFO|2025-03-08 20:47:54] llamafactory.train.callbacks:157 >> {'loss': 0.0012, 'learning_rate': 4.9569e-07, 'epoch': 26.85, 'throughput': 1276.46}\n",
      "{'loss': 0.0012, 'grad_norm': 0.16035203635692596, 'learning_rate': 4.956878037864043e-07, 'epoch': 26.85, 'num_input_tokens_seen': 1265712}\n",
      " 94%|  | 595/630 [16:38<00:50,  1.44s/it][INFO|2025-03-08 20:48:02] llamafactory.train.callbacks:157 >> {'loss': 0.0011, 'learning_rate': 3.7981e-07, 'epoch': 27.05, 'throughput': 1276.59}\n",
      "{'loss': 0.0011, 'grad_norm': 0.01588532142341137, 'learning_rate': 3.7980617469479953e-07, 'epoch': 27.05, 'num_input_tokens_seen': 1275184}\n",
      " 95%|  | 600/630 [16:47<00:49,  1.66s/it][INFO|2025-03-08 20:48:10] llamafactory.train.callbacks:157 >> {'loss': 0.0011, 'learning_rate': 2.7923e-07, 'epoch': 27.28, 'throughput': 1276.84}\n",
      "{'loss': 0.0011, 'grad_norm': 0.08856447786092758, 'learning_rate': 2.7922934437178695e-07, 'epoch': 27.28, 'num_input_tokens_seen': 1286352}\n",
      " 95%|  | 600/630 [16:47<00:49,  1.66s/it][INFO|trainer.py:3942] 2025-03-08 20:48:10,621 >> Saving model checkpoint to saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-600\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:48:11,176 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:48:11,177 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-08 20:48:11,297 >> tokenizer config file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-08 20:48:11,297 >> Special tokens file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-600/special_tokens_map.json\n",
      " 96%| | 605/630 [16:56<00:44,  1.77s/it][INFO|2025-03-08 20:48:20] llamafactory.train.callbacks:157 >> {'loss': 0.0015, 'learning_rate': 1.9402e-07, 'epoch': 27.52, 'throughput': 1275.76}\n",
      "{'loss': 0.0015, 'grad_norm': 0.16395550966262817, 'learning_rate': 1.9401983499569842e-07, 'epoch': 27.52, 'num_input_tokens_seen': 1297376}\n",
      " 97%| | 610/630 [17:05<00:34,  1.72s/it][INFO|2025-03-08 20:48:28] llamafactory.train.callbacks:157 >> {'loss': 0.0015, 'learning_rate': 1.2423e-07, 'epoch': 27.75, 'throughput': 1275.97}\n",
      "{'loss': 0.0015, 'grad_norm': 0.025977496057748795, 'learning_rate': 1.2423061586496477e-07, 'epoch': 27.75, 'num_input_tokens_seen': 1308496}\n",
      " 98%| | 615/630 [17:13<00:25,  1.70s/it][INFO|2025-03-08 20:48:37] llamafactory.train.callbacks:157 >> {'loss': 0.0012, 'learning_rate': 6.9905e-08, 'epoch': 27.99, 'throughput': 1276.16}\n",
      "{'loss': 0.0012, 'grad_norm': 0.17940537631511688, 'learning_rate': 6.990507047049676e-08, 'epoch': 27.99, 'num_input_tokens_seen': 1319552}\n",
      " 98%|| 620/630 [17:21<00:16,  1.62s/it][INFO|2025-03-08 20:48:44] llamafactory.train.callbacks:157 >> {'loss': 0.0016, 'learning_rate': 3.1077e-08, 'epoch': 28.19, 'throughput': 1276.22}\n",
      "{'loss': 0.0016, 'grad_norm': 0.21937575936317444, 'learning_rate': 3.107696952694139e-08, 'epoch': 28.19, 'num_input_tokens_seen': 1328944}\n",
      " 99%|| 625/630 [17:29<00:08,  1.69s/it][INFO|2025-03-08 20:48:53] llamafactory.train.callbacks:157 >> {'loss': 0.0010, 'learning_rate': 7.7704e-09, 'epoch': 28.42, 'throughput': 1276.51}\n",
      "{'loss': 0.001, 'grad_norm': 0.10906460136175156, 'learning_rate': 7.770449979593864e-09, 'epoch': 28.42, 'num_input_tokens_seen': 1340192}\n",
      "100%|| 630/630 [17:38<00:00,  1.70s/it][INFO|2025-03-08 20:49:01] llamafactory.train.callbacks:157 >> {'loss': 0.0010, 'learning_rate': 0.0000e+00, 'epoch': 28.66, 'throughput': 1276.81}\n",
      "{'loss': 0.001, 'grad_norm': 0.01156515721231699, 'learning_rate': 0.0, 'epoch': 28.66, 'num_input_tokens_seen': 1351408}\n",
      "100%|| 630/630 [17:38<00:00,  1.70s/it][INFO|trainer.py:3942] 2025-03-08 20:49:01,592 >> Saving model checkpoint to saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-630\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:49:02,492 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:49:02,493 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-08 20:49:02,593 >> tokenizer config file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-630/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-08 20:49:02,593 >> Special tokens file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-630/special_tokens_map.json\n",
      "[INFO|trainer.py:2657] 2025-03-08 20:49:02,916 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1059.7537, 'train_samples_per_second': 19.221, 'train_steps_per_second': 0.594, 'train_loss': 0.13375964585676908, 'epoch': 28.66, 'num_input_tokens_seen': 1351408}\n",
      "100%|| 630/630 [17:39<00:00,  1.68s/it]\n",
      "[INFO|trainer.py:3942] 2025-03-08 20:49:02,919 >> Saving model checkpoint to saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:49:03,384 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:49:03,385 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-08 20:49:03,476 >> tokenizer config file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-08 20:49:03,476 >> Special tokens file saved in saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =    28.6588\n",
      "  num_input_tokens_seen    =    1351408\n",
      "  total_flos               = 56832412GF\n",
      "  train_loss               =     0.1338\n",
      "  train_runtime            = 0:17:39.75\n",
      "  train_samples_per_second =     19.221\n",
      "  train_steps_per_second   =      0.594\n",
      "Figure saved at: saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/training_loss.png\n",
      "[WARNING|2025-03-08 20:49:03] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-03-08 20:49:03] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:449] 2025-03-08 20:49:03,707 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:54:52,252 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:54:52,256 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:52,499 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:52,499 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:52,499 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:52,499 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:52,499 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:52,499 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 20:54:53,237 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:54:54,971 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:54:54,973 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:55,188 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:55,188 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:55,188 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:55,188 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:55,188 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-08 20:54:55,188 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-08 20:54:55,809 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:699] 2025-03-08 20:54:56,112 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-08 20:54:56,113 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-03-08 20:54:56] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|2025-03-08 20:54:56] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3982] 2025-03-08 20:54:56,213 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1633] 2025-03-08 20:54:56,215 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1140] 2025-03-08 20:54:56,217 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:03<00:00,  1.67s/it]\n",
      "[INFO|modeling_utils.py:4970] 2025-03-08 20:54:59,715 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4978] 2025-03-08 20:54:59,715 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Llama-8B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-03-08 20:55:00,056 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/6a6f4aa4197940add57724a7707d069478df56b1/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-03-08 20:55:00,057 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "[INFO|2025-03-08 20:55:00] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-03-08 20:55:00] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39\n",
      "[INFO|2025-03-08 20:55:00] llamafactory.model.loader:157 >> all params: 8,051,232,768\n",
      "[WARNING|2025-03-08 20:55:00] llamafactory.chat.hf_engine:168 >> There is no current event loop, creating a new one.\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7860 <> https://dc7490390d9c1cfc39.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2df23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f92922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file not found. Please check the path.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Define the path to the saved model\n",
    "model_path = \"llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Model found at: {model_path}\")\n",
    "    # Create a downloadable link in Jupyter Notebook\n",
    "    display(FileLink(model_path))  # Click the link to download <button class=\"citation-flag\" data-index=\"8\">\n",
    "else:\n",
    "    print(\"Model file not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc4d5bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.49.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers safetensors torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dfd4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01753d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.49.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 4/4 [00:04<00:00,  1.11s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-630/adapter_model.safetensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py:205\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-630/adapter_model.safetensors'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_name)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load and merge LoRA adapter\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()  \u001b[38;5;66;03m# Merge LoRA weights into the base model <button class=\"citation-flag\" data-index=\"1\">\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Save merged model (creates pytorch_model.bin and config.json)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:453\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    452\u001b[0m     config \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 453\u001b[0m         \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubfolder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_auth_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     ]\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[1;32m    463\u001b[0m     config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py:211\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    206\u001b[0m             model_id,\n\u001b[1;32m    207\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    208\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-630/adapter_model.safetensors'"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers peft safetensors torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Define paths\n",
    "base_model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\"  # Base model on Hugging Face\n",
    "adapter_path = \"llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/checkpoint-630/adapter_model.safetensors\"\n",
    "output_dir = \"merged_model\"  # Folder to save the merged model\n",
    "\n",
    "# Load base model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",  # Automatically uses GPU if available\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load and merge LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model = model.merge_and_unload()  # Merge LoRA weights into the base model <button class=\"citation-flag\" data-index=\"1\">\n",
    "\n",
    "# Save merged model (creates pytorch_model.bin and config.json)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdb03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8009d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 4/4 [00:03<00:00,  1.05it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py:205\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_name)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load and merge LoRA adapter (now using the parent directory) <button class=\"citation-flag\" data-index=\"3\">\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Requires adapter_config.json in this folder\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Save merged model\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:453\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    452\u001b[0m     config \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 453\u001b[0m         \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubfolder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_auth_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     ]\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[1;32m    463\u001b[0m     config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py:211\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    206\u001b[0m             model_id,\n\u001b[1;32m    207\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    208\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Define paths\n",
    "base_model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\"  # Base model <button class=\"citation-flag\" data-index=\"4\">\n",
    "adapter_path = \"llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/\"  # Parent directory with config <button class=\"citation-flag\" data-index=\"1\">\n",
    "output_dir = \"merged_model\"\n",
    "\n",
    "# Load base model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load and merge LoRA adapter (now using the parent directory) <button class=\"citation-flag\" data-index=\"3\">\n",
    "model = PeftModel.from_pretrained(model, adapter_path)  # Requires adapter_config.json in this folder\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model merged successfully and saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c28a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdc6ce48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.49.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ecf1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae7ca166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2967a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git: 'lfs' is not a git command. See 'git --help'.\n",
      "\n",
      "The most similar command is\n",
      "\tlog\n"
     ]
    }
   ],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "075e61ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: choco: command not found\n"
     ]
    }
   ],
   "source": [
    "!choco install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d48fb7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: sudo: command not found\n",
      "/bin/bash: line 1: sudo: command not found\n",
      "git: 'lfs' is not a git command. See 'git --help'.\n",
      "\n",
      "The most similar command is\n",
      "\tlog\n",
      "git and git-lfs have been installed and initialized.\n"
     ]
    }
   ],
   "source": [
    "# Install git and git-lfs\n",
    "try:\n",
    "    # For Ubuntu/Debian\n",
    "    !sudo apt-get update\n",
    "    !sudo apt-get install -y git git-lfs\n",
    "except Exception as e:\n",
    "    print(f\"Error during installation: {e}\")\n",
    "\n",
    "# Initialize git-lfs\n",
    "!git lfs install\n",
    "\n",
    "print(\"git and git-lfs have been installed and initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94a42b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/git\n"
     ]
    }
   ],
   "source": [
    "!which git\n",
    "!which git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5807ad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/manumishra/llm_finetuned_dsa into local empty directory.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Repository' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m     23\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[0;32m---> 24\u001b[0m         \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m(file_path)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Commit and push the changes to the Hugging Face Hub\u001b[39;00m\n\u001b[1;32m     27\u001b[0m repo\u001b[38;5;241m.\u001b[39mcommit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd model files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Repository' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import HfApi, Repository\n",
    "\n",
    "# Define the path to your model directory\n",
    "model_dir = \"llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39\"\n",
    "\n",
    "# Define the repository name on Hugging Face Hub\n",
    "repo_id = \"manumishra/llm_finetuned_dsa\"\n",
    "\n",
    "# Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Create a new repository or use an existing one\n",
    "api.create_repo(repo_id=repo_id, exist_ok=True)\n",
    "\n",
    "# Initialize a local repository\n",
    "repo = Repository(local_dir=model_dir, clone_from=repo_id)\n",
    "\n",
    "# Add all files in the model directory to the repository\n",
    "for root, dirs, files in os.walk(model_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        repo.add(file_path)\n",
    "\n",
    "# Commit and push the changes to the Hugging Face Hub\n",
    "repo.commit(\"Add model files\")\n",
    "repo.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c64555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "910ed514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files have been uploaded to the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import upload_folder\n",
    "\n",
    "# Define the path to your model directory\n",
    "model_dir = \"llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora\"\n",
    "\n",
    "# Define the repository name on Hugging Face Hub\n",
    "repo_id = \"manumishra/llm_finetuned_dsa\"\n",
    "\n",
    "# Upload the folder to the Hugging Face Hub\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=model_dir,\n",
    "    commit_message=\"Add model files\",\n",
    "    ignore_patterns=[\"*.log\", \"*.txt\"],  # Add any patterns you want to ignore\n",
    ")\n",
    "\n",
    "print(\"Model files have been uploaded to the Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1e120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "662dd5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files and folders have been uploaded to the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import upload_folder\n",
    "\n",
    "# Define the path to your model directory\n",
    "model_dir = \"llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/\"\n",
    "\n",
    "# Define the repository name on Hugging Face Hub\n",
    "repo_id = \"manumishra/llm_finetuned_dsa\"\n",
    "\n",
    "# Upload the folder to the Hugging Face Hub\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=model_dir,\n",
    "    commit_message=\"Add all model files and folders\",\n",
    ")\n",
    "\n",
    "print(\"All files and folders have been uploaded to the Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8439f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1df6e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files and folders have been uploaded to the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import upload_folder\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39/\",\n",
    "    repo_id=\"manumishra/llm_finetuned_dsa\",\n",
    "    path_in_repo=\"train_2025-03-08-20-16-39\",\n",
    "    allow_patterns=\"*.txt\", # Upload all local text files\n",
    ")\n",
    "\n",
    "print(\"All files and folders have been uploaded to the Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee0d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "413580dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid `path_in_repo` in CommitOperation: cannot update files under a '.git/' folder (path: '.git/HEAD').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[1;32m     20\u001b[0m         relative_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrelpath(file_path, model_dir)  \u001b[38;5;66;03m# Get the relative path for the file\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m         \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUpload \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrelative_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll files have been successfully pushed to the Hugging Face Hub!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py:1551\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py:4430\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid repo type, must be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstants\u001b[38;5;241m.\u001b[39mREPO_TYPES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4427\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4428\u001b[0m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4429\u001b[0m )\n\u001b[0;32m-> 4430\u001b[0m operation \u001b[38;5;241m=\u001b[39m \u001b[43mCommitOperationAdd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4435\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_commit(\n\u001b[1;32m   4436\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   4437\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4444\u001b[0m     parent_commit\u001b[38;5;241m=\u001b[39mparent_commit,\n\u001b[1;32m   4445\u001b[0m )\n\u001b[1;32m   4447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m<string>:5\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, path_in_repo, path_or_fileobj)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/_commit_api.py:168\u001b[0m, in \u001b[0;36mCommitOperationAdd.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validates `path_or_fileobj` and compute `upload_info`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_in_repo \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_path_in_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# Validate `path_or_fileobj` value\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_or_fileobj, Path):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/_commit_api.py:294\u001b[0m, in \u001b[0;36m_validate_path_in_repo\u001b[0;34m(path_in_repo)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m forbidden \u001b[38;5;129;01min\u001b[39;00m FORBIDDEN_FOLDERS:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(part \u001b[38;5;241m==\u001b[39m forbidden \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m path_in_repo\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    295\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `path_in_repo` in CommitOperation: cannot update files under a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforbidden\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m folder (path:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path_in_repo\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid `path_in_repo` in CommitOperation: cannot update files under a '.git/' folder (path: '.git/HEAD')."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "# Define the path to your model directory\n",
    "model_dir = \"llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39\"\n",
    "\n",
    "# Define the repository name on Hugging Face Hub\n",
    "repo_id = \"manumishra/llm_finetuned_dsa\"  # Replace with your username and repo name\n",
    "\n",
    "# Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Upload each file in the directory to the Hugging Face Hub\n",
    "for root, dirs, files in os.walk(model_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(file_path, model_dir)  # Get the relative path for the file\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=file_path,\n",
    "            path_in_repo=relative_path,\n",
    "            repo_id=repo_id,\n",
    "            commit_message=f\"Upload {relative_path}\",\n",
    "        )\n",
    "\n",
    "print(\"All files have been successfully pushed to the Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7170bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0c5b051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: README.md\n",
      "All valid files have been successfully pushed to the Hugging Face Hub!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# Step 1: Authenticate with Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "# Step 2: Define the path to your fine-tuned model directory\n",
    "model_dir = \"llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39\"\n",
    "\n",
    "# Step 3: Define the repository name on Hugging Face Hub\n",
    "repo_id = \"manumishra/llm_finetuned_dsa\"\n",
    "\n",
    "# Step 4: Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Step 5: Upload each file in the directory to the Hugging Face Hub\n",
    "for root, dirs, files in os.walk(model_dir):\n",
    "    # Skip hidden/system directories (e.g., .git)\n",
    "    dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
    "\n",
    "    for file in files:\n",
    "        # Skip hidden/system files (e.g., .DS_Store, .gitignore)\n",
    "        if file.startswith(\".\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(file_path, model_dir)  # Get the relative path for the file\n",
    "\n",
    "        try:\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=file_path,\n",
    "                path_in_repo=relative_path,\n",
    "                repo_id=repo_id,\n",
    "                commit_message=f\"Upload {relative_path}\",\n",
    "            )\n",
    "            print(f\"Uploaded: {relative_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {relative_path}: {e}\")\n",
    "\n",
    "print(\"All valid files have been successfully pushed to the Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794cdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ec145e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model or tokenizer: Unrecognized model in llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: README.md\n",
      "All valid files have been successfully pushed to the Hugging Face Hub!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# Step 1: Authenticate with Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "# Step 2: Define the path to your fine-tuned model directory\n",
    "model_dir = \"llm_finetune/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-03-08-20-16-39\"\n",
    "\n",
    "# Step 3: Define the repository name on Hugging Face Hub\n",
    "repo_id = \"manumishra/llm_finetuned_dsa\"\n",
    "\n",
    "# Step 4: Load the model and tokenizer\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    # Step 5: Push the model and tokenizer to the Hugging Face Hub\n",
    "    model.push_to_hub(repo_id, commit_message=\"Upload model\")\n",
    "    tokenizer.push_to_hub(repo_id, commit_message=\"Upload tokenizer\")\n",
    "    print(\"Model and tokenizer have been successfully pushed to the Hugging Face Hub!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model or tokenizer: {e}\")\n",
    "\n",
    "# Step 6: Initialize the Hugging Face API for additional file uploads\n",
    "api = HfApi()\n",
    "\n",
    "# Step 7: Upload any remaining files in the directory\n",
    "for root, dirs, files in os.walk(model_dir):\n",
    "    # Skip hidden/system directories (e.g., .git)\n",
    "    dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
    "\n",
    "    for file in files:\n",
    "        # Skip hidden/system files (e.g., .DS_Store, .gitignore)\n",
    "        if file.startswith(\".\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(file_path, model_dir)  # Get the relative path for the file\n",
    "\n",
    "        try:\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=file_path,\n",
    "                path_in_repo=relative_path,\n",
    "                repo_id=repo_id,\n",
    "                commit_message=f\"Upload {relative_path}\",\n",
    "            )\n",
    "            print(f\"Uploaded: {relative_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {relative_path}: {e}\")\n",
    "\n",
    "print(\"All valid files have been successfully pushed to the Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eebfaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d5411f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llm_finetuned_dsa'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (6/6), 2.09 KiB | 2.09 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/manumishra/llm_finetuned_dsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97257e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff  README_zh.md  \u001b[0m\u001b[01;34mdocker\u001b[0m/             pyproject.toml    \u001b[01;34msrc\u001b[0m/\n",
      "LICENSE       \u001b[01;34massets\u001b[0m/       \u001b[01;34mevaluation\u001b[0m/         requirements.txt  \u001b[01;34mtests\u001b[0m/\n",
      "MANIFEST.in   \u001b[01;34mcache\u001b[0m/        \u001b[01;34mexamples\u001b[0m/           \u001b[01;34msaves\u001b[0m/\n",
      "Makefile      \u001b[01;34mconfig\u001b[0m/       \u001b[01;34mllm_finetune\u001b[0m/       \u001b[01;34mscripts\u001b[0m/\n",
      "README.md     \u001b[01;34mdata\u001b[0m/         \u001b[01;34mllm_finetuned_dsa\u001b[0m/  setup.py\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3283e48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd llm_finetuned_dsa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7022c2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75083d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  \u001b[0m\u001b[01;34mtrain_2025-03-08-20-16-39\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b31c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6c229eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Provided path: '.' is not a file on the local file system",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint 630/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your desired folder name\u001b[39;00m\n\u001b[1;32m     15\u001b[0m placeholder_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.gitkeep\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Placeholder file to simulate folder creation\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Empty content for the placeholder file\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mplaceholder_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCreate folder: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfolder_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFolder \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been created in the repository \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py:1551\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py:4430\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid repo type, must be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstants\u001b[38;5;241m.\u001b[39mREPO_TYPES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4427\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4428\u001b[0m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4429\u001b[0m )\n\u001b[0;32m-> 4430\u001b[0m operation \u001b[38;5;241m=\u001b[39m \u001b[43mCommitOperationAdd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4435\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_commit(\n\u001b[1;32m   4436\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   4437\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4444\u001b[0m     parent_commit\u001b[38;5;241m=\u001b[39mparent_commit,\n\u001b[1;32m   4445\u001b[0m )\n\u001b[1;32m   4447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m<string>:5\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, path_in_repo, path_or_fileobj)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/_commit_api.py:176\u001b[0m, in \u001b[0;36mCommitOperationAdd.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m     path_or_fileobj \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_or_fileobj))\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path_or_fileobj):\n\u001b[0;32m--> 176\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided path: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_fileobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a file on the local file system\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_or_fileobj, (io\u001b[38;5;241m.\u001b[39mBufferedIOBase, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# ^^ Inspired from: https://stackoverflow.com/questions/44584829/how-to-determine-if-file-is-opened-in-binary-or-text-mode\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_or_fileobj must be either an instance of str, bytes or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m io.BufferedIOBase. If you passed a file-like object, make sure it is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in binary mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Provided path: '.' is not a file on the local file system"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, HfApi\n",
    "import os\n",
    "\n",
    "# Step 1: Authenticate with Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "# Step 2: Define the repository name\n",
    "repo_id = \"manumishra/llm_finetuned_dsa\"\n",
    "\n",
    "# Step 3: Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Step 4: Create a new folder\n",
    "folder_path = \"checkpoint 630/\"  # Replace with your desired folder name\n",
    "placeholder_file = \".gitkeep\"  # Placeholder file to simulate folder creation\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"\",  # Empty content for the placeholder file\n",
    "    path_in_repo=folder_path + placeholder_file,\n",
    "    repo_id=repo_id,\n",
    "    commit_message=f\"Create folder: {folder_path}\",\n",
    ")\n",
    "\n",
    "print(f\"Folder '{folder_path}' has been created in the repository '{repo_id}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da7c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60588237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6545bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ef69dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_TOKEN=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "18306e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://729060c8c477584040.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7860 <> https://729060c8c477584040.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8817558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c8fb87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "27c97bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_AUTH_TOKEN=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bca1c063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 679\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "# Load a private or custom dataset\n",
    "dataset = load_dataset(\"manumishra/dsa_llm_new\")\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8065eafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://de2aa9dc815c7dff44.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "[INFO|2025-03-17 15:12:39] llamafactory.cli:143 >> Initializing 2 distributed tasks at: 127.0.0.1:25074\n",
      "W0317 15:12:40.984000 15637 torch/distributed/run.py:792] \n",
      "W0317 15:12:40.984000 15637 torch/distributed/run.py:792] *****************************************\n",
      "W0317 15:12:40.984000 15637 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0317 15:12:40.984000 15637 torch/distributed/run.py:792] *****************************************\n",
      "[INFO|2025-03-17 15:12:44] llamafactory.hparams.parser:383 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[WARNING|2025-03-17 15:12:45] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[WARNING|2025-03-17 15:12:45] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-03-17 15:12:45] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:46,274 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:46,275 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:46,275 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:46,275 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:46,275 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:46,275 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|processing_utils.py:816] 2025-03-17 15:12:48,907 >> loading configuration file processor_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/processor_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:49,203 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:49,204 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:49,204 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:49,204 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:49,204 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/dbd91bbaf64a0e591f4340ce8b66fd1dba9ab6bd/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-17 15:12:49,204 >> loading file chat_template.jinja from cache at None\n",
      "[rank1]:[W317 15:12:49.335228268 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[INFO|2025-03-17 15:12:50] llamafactory.data.template:143 >> Add <end_of_turn> to stop words.\n",
      "[INFO|2025-03-17 15:12:50] llamafactory.data.loader:143 >> Loading dataset manumishra/dsa_llm_new...\n",
      "[rank0]:[W317 15:12:55.605902640 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "Running tokenizer on dataset (num_proc=16):   0%| | 0/679 [00:09<?, ? examples/s\n",
      "[rank0]: multiprocess.pool.RemoteTraceback: \n",
      "[rank0]: \"\"\"\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 125, in worker\n",
      "[rank0]:     result = (True, func(*args, **kwds))\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n",
      "[rank0]:     for i, result in enumerate(func(**kwargs)):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3519, in _map_single\n",
      "[rank0]:     for i, batch in iter_outputs(shard_iterable):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3469, in iter_outputs\n",
      "[rank0]:     yield i, apply_function(example, i, offset=offset)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3392, in apply_function\n",
      "[rank0]:     processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/data/processor/supervised.py\", line 99, in preprocess_dataset\n",
      "[rank0]:     input_ids, labels = self._encode_data_example(\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/data/processor/supervised.py\", line 43, in _encode_data_example\n",
      "[rank0]:     messages = self.template.mm_plugin.process_messages(prompt + response, images, videos, audios, self.processor)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/data/mm_plugin.py\", line 409, in process_messages\n",
      "[rank0]:     self._validate_input(processor, images, videos, audios)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/data/mm_plugin.py\", line 163, in _validate_input\n",
      "[rank0]:     raise ValueError(\"Processor was not found, please check and update your processor config.\")\n",
      "[rank0]: ValueError: Processor was not found, please check and update your processor config.\n",
      "[rank0]: \"\"\"\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 103, in run_exp\n",
      "[rank0]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 68, in _training_function\n",
      "[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n",
      "[rank0]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/data/loader.py\", line 303, in get_dataset\n",
      "[rank0]:     dataset = _get_preprocessed_dataset(\n",
      "[rank0]:   File \"/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/data/loader.py\", line 249, in _get_preprocessed_dataset\n",
      "[rank0]:     dataset = dataset.map(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 562, in wrapper\n",
      "[rank0]:     out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3171, in map\n",
      "[rank0]:     for rank, done, content in iflatmap_unordered(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 718, in iflatmap_unordered\n",
      "[rank0]:     [async_result.get(timeout=0.05) for async_result in async_results]\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 718, in <listcomp>\n",
      "[rank0]:     [async_result.get(timeout=0.05) for async_result in async_results]\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 774, in get\n",
      "[rank0]:     raise self._value\n",
      "[rank0]: ValueError: Processor was not found, please check and update your processor config.\n",
      "Running tokenizer on dataset (num_proc=16):   0%| | 0/679 [00:00<?, ? examples/s[rank0]:[W317 15:16:08.645581996 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0317 15:16:09.857000 15637 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 15703 closing signal SIGTERM\n",
      "E0317 15:16:10.123000 15637 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 15702) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 918, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/workspace/llm_finetune/LLaMA-Factory/llm_finetuned_dsa/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-03-17_15:16:09\n",
      "  host      : 7ae64d46f1c7\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 15702)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7860 <> https://de2aa9dc815c7dff44.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b657664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b1e3e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/llamafactory-cli\", line 5, in <module>\n",
      "    from llamafactory.cli import main\n",
      "ModuleNotFoundError: No module named 'llamafactory'\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
